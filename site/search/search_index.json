{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p> VisionAI Apps for Workplace Safety. Pretrained &amp; Ready to deploy.  </p> <p> <p>Documentation: https://docs.visionify.ai</p>"},{"location":"#overview","title":"Overview","text":"<p>VisionAI offers a collection of pre-trained apps tailored for workplace safety use cases. Developed by Visionify as part of the Workplace Safety suite, VisionAI is ready for production deployment and accessible through web-based GUI.</p>"},{"location":"#what-is-visionai","title":"What is VisionAI?","text":"<ul> <li>A Platform to run AI scenarios for CCTV cameras.</li> <li>Choose from the list of scenarios here for workplace safety &amp; building security.</li> </ul>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<ul> <li>Migrate CCTV feeds to the cloud. Manage cameras, choose AI scenarios and configuring alerting from cloud.</li> </ul> <p>Key features of VisionAI include:</p> <ul> <li> <p>No hardware installation: Works with any IP/security cameras using RTSP streams. No need to install any new cameras, sensors, or other hardware.</p> </li> <li> <p>User-friendly: Easy-to-use web interface for managing cameras and associated apps, catering to both technical and non-technical users.</p> </li> <li> <p>Production-ready: Apps are trained on diverse, carefully curated datasets from industrial and academic sources, ensuring out-of-the-box functionality.</p> </li> <li> <p>Customizable: Allows app customization and model fine-tuning with a flexible architecture based on the NVIDIA Triton server. Refer to customization documentation for more details.</p> </li> <li> <p>Integrations: VisionAI currently integrates with Azure Event hubs, Redis PubSub for reports, alerts and notifications. We have roadmap plans to add support for other message brokers as well.</p> </li> </ul>"},{"location":"#visionai-apps","title":"VisionAI Apps","text":"<p>VisionAI offers a variety of workplace health and safety scenarios, with continuous development of new use cases. View the complete list of VisionAI Apps here. If you require a specific scenario not listed here, feel free to contact us.</p> <p>Our primary focus is on workplace health and safety models, but we are expanding our scope to include Quality Inspection, Food Safety/Debris Detection, and more. These additional scenarios are available to customers on a case-by-case basis.</p>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#prerequisite","title":"Prerequisite","text":""},{"location":"#operating-systemos","title":"Operating System(OS)","text":"<p>Ubuntu 22.04</p>"},{"location":"#docker","title":"Docker","text":"<ul> <li> <p>Install Docker Engine and Docker tools         Open a terminal window and run the following commands to install Docker Engine, Docker CLI, Docker Compose, and Docker Buildx plugin:     <pre><code>sudo chmod a+r /etc/apt/keyrings/docker.gpg\nsudo apt-get update\nsudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin\n</code></pre></p> </li> <li> <p>Grant permissions to Docker     Run the following command to grant permissions to Docker:     <pre><code>sudo chmod 666 /var/run/docker.sock\n</code></pre>     This will avoid the error that may occur during fetching the server API version.</p> </li> </ul>"},{"location":"#disc-space","title":"Disc Space","text":"<p>VisionAI application uses Docker containers to run the apps and the Docker images are large in size. Minimum of 100GB of free space is requied on the host machine.</p>"},{"location":"#ram-requirement","title":"RAM requirement","text":"<p>VisionAI application requires minimum 16GB RAM to run the apps.</p>"},{"location":"#gpu","title":"GPU","text":"<p>VisionAI is a Video-based AI platform that uses GPU for inference. It is recommended to use a GPU with at least 8GB of memory for optimal performance. VisionAI supports NVIDIA GPUs only - Following are a few recommended options:</p> <ul> <li>NVIDIA GeForce RTX 2060/RTX 2060 Ti</li> <li>NVIDIA GeForce RTX 3050/RTX 3050 Ti</li> <li>NVIDIA P40</li> <li>NVIDIA A100</li> </ul>"},{"location":"#internet-connectivity","title":"Internet Connectivity","text":"<p>During initial download and setup portion of VisionAI application, we would need good internet connectivity in order to download the required dependencies and Docker containers. Once the setup is complete, VisionAI can be used offline.</p>"},{"location":"#license","title":"License","text":"<p>You can purchase license by contacing us: sales@visionify.ai</p> <p>For any queries related to VisionAI toolkit usage: support@visionify.ai</p>"},{"location":"#installation","title":"Installation","text":"<ul> <li>Upgrade pip to the latest version <pre><code>$ pip install --upgrade pip\n</code></pre></li> <li>Install VisionAI through <code>PyPI</code>:</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\nSuccessfully installed visionai\n</code></pre> <ul> <li>Update to the latest version, if already installed:</li> </ul> <pre><code>$ pip install --upgrade --force-reinstall visionai\n</code></pre> <ul> <li>Initialize VisionAI to download and install dependencies (Docker, Pytorch, NVIDIA Triton, etc.):</li> </ul> <pre><code>$ visionai init\n</code></pre> <ul> <li>Upon successful initialization, you should be able to see the following services running:</li> </ul> Service Port Purpose <code>Web UI</code> <code>http://localhost:3001</code> VisionAI Web-app <code>Web API</code> <code>http://localhost:3002</code> VisionAI API service <code>Triton HTTP</code> <code>http://localhost:8000</code> Triton Model server (http) <code>Triton GRPC</code> <code>grpc://localhost:8001</code> Triton Model server (grpc) <code>Triton Metrics</code> <code>http://localhost:8002</code> Triton Model metrics server (prometheus) <code>Redis</code> <code>redis://localhost:6379</code> Redis server, currently supports PUBSUB"},{"location":"#visionai-web-application","title":"VisionAI Web Application","text":"<ul> <li> <p>VisionAI supports a web-based option for managing cameras, scenarios. </p> </li> <li> <p>You can manage cameras, scenarios, see events etc., directly on the web-app. The web-app is running your own local compute instance. All the data is saved in your machine, and it is persistent as long as VisionAI application is not uninstalled.</p> </li> </ul>"},{"location":"#visionai-web-app","title":"VisionAI Web-app","text":"<p>VisionAI web-app is a software application that runs in a web browser. It is designed to provide a user-friendly interface and functionality that can be accessed from any device, without the need for installation on the device.</p> <p>Open http://localhost:3001 in the browser. Use your default username/password as master/master. After this, you will be asked to create a new admin user. Please use a strong password and create an admin user.</p> <p></p>"},{"location":"#cameras","title":"Cameras","text":"<p>Once you are signed in, you will see a blank dashboard page. Let\u2019s add an IP camera to the system. In order to do this, go to \u201cCameras\u201d tab on the left menu and Click on the + button.</p> <p></p> <p>A new pop-up window will appear to add cameras. You can enter the camera name, description, and RTSP URI for the camera. The RTSP URI can be obtained from the Camera or NVR documentation. You can ignore the other fields as they are optional. Click on \u201cAdd\u201d button.</p> <p></p> <p>Once you have added the camera, it should appear on the Cameras window and should show the initial streaming for the camera. Add any additional cameras in a similar fashion. Once all cameras have been added, the front-screen should look like this:</p> <p></p>"},{"location":"#scenarios","title":"Scenarios","text":"<p>We can enable Vision AI scenarios for each of these cameras. In order to do this, go to Scenarios tab on the left menu to browse through the available scenarios.</p> <p>.</p> <p>This shows details about the Scenario, the model version used, events supported and model accuracy, recall and F1 score metrics. You can now click on the \u201cGet this\u201d button again to apply the scenario to cameras.</p> <p>.</p> <p>In the next page, Select the Cameras for which you want to apply this scenario.</p> <p></p> <p>Click \"Save and Next\". In the next page, you can specify email and text message notification settings. Provide the email address and phone number you want to be notified at and click \"Save and Next button\"</p> <p></p> <p>On the next page, it will ask you to confirm your settings. Click Submit. VisionAI will now download these models and start running them for these scenarios.</p>"},{"location":"#redis","title":"Redis","text":"<ul> <li>VisionAI supports out-of-box integration with Redis, Prometheus, Grafana and Azure Event Hub. Once the web-app is started, you can view the Grafana dashboard at: http://localhost:3003. The default username and password is <code>admin</code>/<code>admin</code>.</li> </ul> <pre><code>Redis server is at: redis://localhost:6379\n</code></pre>"},{"location":"#next-steps","title":"Next steps","text":"<p>Congratulations! You have successfully configured and used VisionAI toolkit. You can also browse through our scenarios section to understand different use-cases that are supported currently. If you have a need for a scenario, do not hesitate to submit a request here.</p>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"roadmap/#scenarios-roadmap","title":"Scenarios Roadmap","text":"<ol> <li>Hazard Warning<ol> <li> Smoke and Fire Detection</li> <li> No smoking/no vaping</li> <li> Spills &amp; leaks detection</li> <li> Gask leak detection</li> <li> Missing fire extinguisher</li> <li> Blocked exit monitoring</li> <li> Equipment temperature monitoring</li> <li> Equipment rust and corrosion</li> </ol> </li> <li>Employee Health &amp; Safety<ol> <li> PPE Detection</li> <li> Working at heights</li> <li> Environment monitoring</li> <li> Slip, trip and fall detection</li> <li> Posture &amp; Ergonomics</li> <li> Empty pallets</li> <li> Spills &amp; Leaks detection (Liquids)</li> <li> Hand sanitizer/hand-wash</li> <li> Worker fatigue detection</li> <li> Worker skin tempreature monitoring</li> <li> Confined spaces monitoring</li> </ol> </li> <li>Company Compliance Policies<ol> <li> Max occupancy</li> <li> Restricted areas/times</li> <li> Dwell time</li> <li> Social distancing</li> <li> Station occupancy</li> <li> Occupancy metrics</li> <li> Authorized personnel</li> <li> Tailgating</li> <li> Perimeter control</li> <li> No food or drinks</li> <li> No phone, text, pictures</li> <li> No Smoking zones</li> <li> No children/visitors</li> <li> Waste Management</li> <li> Energy Conservation</li> <li> Restricted Areas</li> </ol> </li> <li>Equipment Monitoring<ol> <li> Equipment temperature monitoring</li> <li> Equipment rust and corrosion</li> <li> Equipment vibration monitoring</li> <li> Equipment noise monitoring</li> <li> Read analog dials</li> <li> Tools check-in/out</li> <li> Spill &amp; leak</li> </ol> </li> <li>Environment Monitoring<ol> <li> Temperature monitoring</li> <li> Humidity monitoring</li> <li> Air quality monitoring</li> <li> Noise monitoring</li> <li> Pressure monitoring</li> <li> Volatile organic compounds (VOCs)</li> <li> Carbon monoxide (CO)</li> <li> Ambient light</li> <li> Dust monitoring</li> <li> Water quality monitoring</li> <li> Energy usage monitoring</li> <li> Waste management</li> <li> Water usage monitoring</li> <li> Water level monitoring</li> <li> Radiation monitoring</li> </ol> </li> <li>Suspicious Activity Detection<ol> <li> Loitering</li> <li> Suspicious packages</li> <li> Aggressive behavior</li> <li> Vandalism &amp; property destruction</li> <li> Firearms &amp; knives</li> <li> Sexual harassments</li> <li> Solicitation</li> <li> Theft</li> <li> Shipping activity</li> <li> Intrusion detection</li> </ol> </li> <li>Vehicle Activity Detection<ol> <li> Vehicle usage</li> <li> Vehicle policies</li> <li> Forklift zone breach</li> <li> Vehicle license plate</li> <li> Vehicle speed</li> <li> Vehicle cargo</li> </ol> </li> <li>Privacy<ol> <li> Blur faces</li> <li> Blur signs/text</li> <li> Blur screens</li> <li> Blur license plates</li> <li> Obstructed camera view</li> </ol> </li> </ol>"},{"location":"todo/","title":"Todo","text":"<ul> <li>Figure out how to integrate API/web-app into same codebase. (or through Docker somehow).</li> <li>Move edgeApi to fastapi, enable documentation.</li> <li>Move edge-inference scripts here.</li> <li>Add inference dependencies to the package.</li> <li>Support for pipelines: TODO</li> <li>Add support for docsqa.jina.ai after documentation is complete. This would provide an automated bot for the end-user.</li> <li>Work through these items for the PyPi package improvement (link): <pre><code>Basic info present?         0\nSource repository present?  0\nReadme present?             0\nLicense present?            1\nHas multiple versions?      1\nFollows SemVer?             1\nRecent release?             1\nNot brand new?              0\n1.0.0 or greater?           0\nDependent Packages          0\nDependent Repositories      0\nStars                       0\nContributors                0\nLibraries.io subscribers    0\n</code></pre></li> </ul>"},{"location":"company/about/","title":"VisionAI","text":"<p>Documentation for VisionAI toolkit.</p>"},{"location":"company/about/#overview","title":"Overview","text":"<p>VisionAI provides a set of command line utilities for you to manage different Vision AI scenarios that have been pre-developed and pre-tested. VisionAI focuses on workplace health and safety models - and majority of the models you see here have been developed with that in mind.</p> <p>These are production-ready model trained from open-source and academic datasets. We are continuously working on new scenarios - and our current scenario repo consists of over 60 scenarios that are listed here. They are developed with the intent of being easy-to-use for business. The framework also supports a whole bunch of custom scenarios.</p>"},{"location":"company/about/#install-visionai","title":"Install VisionAI","text":"<p>Install VisionAI application through <code>PyPI</code>. There are other options available for install - including a Docker container option. These are detailed in installation section.</p> <pre><code>$ pip install visionai\n---&gt; 100%\n\nSuccessfully installed visionai\n\n\u2728 You are all set to use visionai toolkit \u2728\n</code></pre>"},{"location":"company/about/#deploy-to-azure","title":"Deploy to Azure","text":"<p>Deploy a fully configured and tested solution directly from Azure Marketplace. VisionAI runs computer vision models, most of which run orders of magnitude faster if executed on a GPU machine. Our Azure Marketplace offer VisionAI Community Edition is available through Azure Marketplace here. The community edition deploys a fully configured Virtual Machine with the recommended hardware and software options. Get more details here.</p> <p></p> <ul> <li>TODO: Point to ARM template that needs to be deployed (using these instructions and here is an example JSON file).</li> </ul>"},{"location":"company/about/#list-available-scenarios","title":"List available Scenarios","text":"<p>VisionAI is organized in terms of scenarios. Consider each scenario as being a business use-case, that is solved by a combination of Machine Learning models and an inference algorithm. For example Warn me when max occupancy of this area exceeds 80 people is a business scenario, where as the People detection is an ML model.</p> <p>VisionAI supports 60 scenarios currently and more are being added continuously. Our current focus is on Workplace Safety scenarios. Please contact us if a scenario you need is not present in our repo and we will look into it.</p> <pre><code>$ visionai scenarios list\n\n------------------------------------------------\nPrivacy Suite\nblur-faces\nblur-text\n\nFire safety\nearly-smoke-and-fire-detection\nsmoking-and-vaping-detection\n\nPersonnel safety\nppe-detection\npfas-system-detection\nrailings-detection\n\nSuspicious activity\nshipping-activity-detection\nagressive-behaivior\n\n\nCompliance Policies\nmax-occupancy\n\nEquipment\nrust-and-corrosion-detection\n\nIR Camera\ntemperature-monitoring\n------------------------------------------------\n\n\u2728 More scenarios are added regularly \u2728\n</code></pre>"},{"location":"company/about/#get-details-for-a-scenario","title":"Get details for a Scenario","text":"<p>You can get details about a scenario using <code>visionai scenario details</code> command. Specify the scenario you want additional details for. The details of a scenario include the dataset size, model accuracy metrics,</p> <pre><code>$ visionai scenario --name early-smoke-and-fire-detection details\n\n------------------------------------------------\nCategory: Fire safety\nScenario: early-smoke-and-fire-detection\nThis scenario has been trained on open-source datasets consisting of 126,293 images. The datasets images are primarily outdoors (70%), but do contain a good number of indoor images (30%). There is a ~50-50% mix of day vs night images. You can find more details about this scenario at visionify.ai/early-smoke-and-fire-detection.\n\n\nModel: smoke-and-fire-detection-1.0.1.pt\nModel size: 127MB\nModel type: Object Detection\nFramework: PyTorch\n\nModel performance:\nDataset size: 126,293 images\nAccuracy: 94.1%\nRecall: 93%\nF1-Score: 93.5%\n\nEvents:\nsmoke-detected  | Immediate\nfire-detected   | Immediate\n\nEvent examples:\n{\n    \"scenario\": \"smoke-and-fire-detection\",\n    \"event_name\": \"smoke-detected\",\n    \"event_details\": {\n        \"camera\": \"camera-01\",\n        \"date\": \"2023-01-04 11:05:02\",\n        \"confidence\": 0.92\n    }\n}\n------------------------------------------------\n</code></pre>"},{"location":"company/about/#run-a-scenario","title":"Run a Scenario","text":"<p>Use <code>visionai run</code> command to run a scenario. In its simplest sense, you can run a single scenario on your web-cam. In a more complex use-case, you can specify a pipeline of scenarios, configure notification logic for each scenario, timings for each scenario etc.</p> <pre><code>$ visionai run --scenario early-smoke-and-fire-detection --camera OFFICE-01\n\nStarting early-smoke-and-fire-detection\n...\n</code></pre>"},{"location":"company/about/#get-help-on-commands","title":"Get help on commands","text":"<p>You can get more help on any command by adding --help at the end of the command. For example, if you want to get details about pipeline commands, you can run the following commands.</p> <pre><code>$ visionai pipeline --help\n\n Usage: visionai pipeline [OPTIONS] COMMAND [ARGS]...\n\n Manage pipelines\n Pipeline is a sequence of preprocess routines and\n scenarios to be run on a given set of cameras. Each\n pipeline can be configured to run specific scenarios -\n each scenario with their own customizations for event\n notifications. This module provides robust methods for\n managing pipelines, showing their details, adding/remove\n cameras from pipelines and running a pipeline.\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 --help          Show this message and exit.              \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 add-camera      Add a camera to a pipeline               \u2502\n\u2502 add-preprocess  Add a preprocess routine to a pipeline   \u2502\n\u2502 add-scenario    Add a scenario to a pipeline             \u2502\n\u2502 create          Create a named pipeline                  \u2502\n\u2502 remove-camera   Remove a camera from a pipeline          \u2502\n\u2502 reset           Reset the pipeline to original state.    \u2502\n\u2502 run             Run a pipeline of scenarios on given     \u2502\n\u2502                 cameras                                  \u2502\n\u2502 show            Show details of a pipeline               \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n\n$ visionai pipeline add-scenario --help\n\n Usage: visionai pipeline add-scenario [OPTIONS]\n\n Add a scenario to a pipeline\n The order of the scenarios does not matter. All added\n scenarios are run in different threads. All scenarios are\n run after pre-processing stage is done.\n ``` Ex: visionai pipeline --name test_pipe add-scenario\n --name smoke-and-fire visionai pipeline --name test_pipe\n add-scenario --name ppe-detection visionai pipeline --name\n test_pipe run ```\n @arg pipeline - specify a named pipeline @arg scenario -\n specify name of the scenario to run\n @return None\n\n\u256d\u2500 Options \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 *  --pipeline        TEXT  pipeline name [default: None] \u2502\n\u2502                            [required]                    \u2502\n\u2502 *  --scenario        TEXT  scenario to add               \u2502\n\u2502                            [default: None]               \u2502\n\u2502                            [required]                    \u2502\n\u2502    --help                  Show this message and exit.   \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"company/about/#next-steps","title":"Next steps","text":"<p>Congratulations! You have successfully run the first scenario. Now go through tutorials to learn about how to run multiple scnearios, how to configure each scenario for the events you need, how to set up the dependencies etc.</p> <p>Or you can also go through our scenarios page to explore the different scenarios available and their model details. If you have a need for a scenario to be implemented, do not hesitate to submit a request.</p>"},{"location":"company/careers/","title":"VisionAI","text":"<p>Documentation for VisionAI toolkit.</p>"},{"location":"company/careers/#overview","title":"Overview","text":"<p>VisionAI provides a set of command line utilities for you to manage different Vision AI scenarios that have been pre-developed and pre-tested. VisionAI focuses on workplace health and safety models - and majority of the models you see here have been developed with that in mind.</p> <p>These are production-ready model trained from open-source and academic datasets. We are continuously working on new scenarios - and our current scenario repo consists of over 60 scenarios that are listed here. They are developed with the intent of being easy-to-use for business. The framework also supports a whole bunch of custom scenarios.</p>"},{"location":"company/careers/#next-steps","title":"Next steps","text":"<p>Congratulations! You have successfully run the first scenario. Now go through Tutorials to learn about how to run multiple scnearios, how to configure each scenario for the events you need, how to set up the dependencies etc.</p> <p>Or you can also go through our scenarios page to explore the different scenarios available and their model details. If you have a need for a scenario to be implemented, do not hesitate to submit a request.</p>"},{"location":"company/contact/","title":"VisionAI Contact Information","text":"<p>You can contact us at:</p> <p>For general queries: info@visionify.ai</p> <p>For queries related to sales or pricing plans: sales@visionify.ai</p> <p>For queries related to scenarios or models or customization: development@visionify.ai</p> <p>For queries related to VisionAI toolkit usage: support@visionify.ai</p> <p>Note: Please put VisionAI in the subject!</p> <p>More information is available at: https://visionify.ai/</p> <p>Our github contains the source for all of our scenarios: https://github.com/visionify/visionai</p> <p>If you want to set up a meeting with us visit https://www.visionify.ai/contact-us and follow the instructions for scheduling a call.</p>"},{"location":"company/install/","title":"VisionAI Web-app","text":"<p>VisionAI web-app is a software application that runs in a web browser. It is designed to provide a user-friendly interface and functionality that can be accessed from any device with an intpernet connection, without the need for installation on the device.</p> <p>The web-app primarily consists of parts including Dashboard, Cameras and Scenarios as shown below:</p> <p> The main screen that appears when you open web-app is:</p> <p></p> <p>Let us see each of these parts in detail.</p> <ul> <li>Dashboard: Visionai Webapp's dashboard is a graphical user interface that displays important information about latest events in the right pane of the screen. It shows metrics such as total camera added, active camera, alerts generated on daily basis and available scenarios. It also presents a graph showing camera versus events triggered. This Dashboard's data analytics will help managers, analysts, or decision-makers quickly assess the performance and make informed decisions.</li> </ul> <p> - Cameras:  Cameras screen shows detailed information about added camera in the app. </p> <p> It also has an option to add any new camera by providing its details such as:</p> <p> - Scenario: Scenario page provides information about different scenarios available and added camera instance can be connect with these scenarios.</p> <p></p> <p>To get particular scenario in use, click on get this. It will open the following window:</p> <p></p> <p>Here, we can find detailed information about these scenarios such as their accuracy metrics, events generated and user manual for guided instructions.</p> <p></p>"},{"location":"company/privacy-policy/","title":"Privacy Policy","text":"<p>Last updated: February 20, 2023</p> <p>This Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your information when You use the Service and tells You about Your privacy rights and how the law protects You.</p> <p>We use Your Personal data to provide and improve the Service. By using the Service, You agree to the collection and use of information in accordance with this Privacy Policy. This Privacy Policy has been created with the help of the TermsFeed Privacy Policy Generator.</p>"},{"location":"company/privacy-policy/#interpretation-and-definitions","title":"Interpretation and Definitions","text":""},{"location":"company/privacy-policy/#interpretation","title":"Interpretation","text":"<p>The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.</p>"},{"location":"company/privacy-policy/#definitions","title":"Definitions","text":"<p>For the purposes of this Privacy Policy:</p> <ul> <li>Account means a unique account created for You to access our Service or parts of our Service.</li> <li> <p>Affiliate means an entity that controls, is controlled by or is under common control with a party, where \"control\" means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority.</p> </li> <li> <p>Company (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this Agreement) refers to DestinationJ Software Technologies LLC, 1499 W 120<sup>th</sup> Ave, Ste 110, Westminster CO 80234.</p> </li> <li> <p>Cookies are small files that are placed on Your computer, mobile device or any other device by a website, containing the details of Your browsing history on that website among its many uses.</p> </li> <li> <p>Country refers to: Colorado,  United States</p> </li> <li> <p>Device means any device that can access the Service such as a computer, a cellphone or a digital tablet.</p> </li> <li> <p>Personal Data is any information that relates to an identified or identifiable individual.</p> </li> <li> <p>Service refers to the Website.</p> </li> <li> <p>Service Provider means any natural or legal person who processes the data on behalf of the Company. It refers to third-party companies or individuals employed by the Company to facilitate the Service, to provide the Service on behalf of the Company, to perform services related to the Service or to assist the Company in analyzing how the Service is used.</p> </li> <li> <p>Usage Data refers to data collected automatically, either generated by the use of the Service or from the Service infrastructure itself (for example, the duration of a page visit).</p> </li> <li> <p>Website refers to Visionify: Workplace Safety through Vision AI, accessible from https://www.visionify.ai</p> </li> <li> <p>You means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.</p> </li> </ul>"},{"location":"company/privacy-policy/#collecting-and-using-your-personal-data","title":"Collecting and Using Your Personal Data","text":""},{"location":"company/privacy-policy/#types-of-data-collected","title":"Types of Data Collected","text":""},{"location":"company/privacy-policy/#personal-data","title":"Personal Data","text":"<p>While using Our Service, We may ask You to provide Us with certain personally identifiable information that can be used to contact or identify You. Personally identifiable information may include, but is not limited to:</p> <ul> <li>Email address</li> <li>First name and last name</li> <li>Phone number</li> <li> <p>Address, State, Province, ZIP/Postal code, City</p> </li> <li> <p>Usage Data</p> </li> </ul>"},{"location":"company/privacy-policy/#usage-data","title":"Usage Data","text":"<p>Usage Data is collected automatically when using the Service.</p> <p>Usage Data may include information such as Your Device's Internet Protocol address (e.g. IP address), browser type, browser version, the pages of our Service that You visit, the time and date of Your visit, the time spent on those pages, unique device identifiers and other diagnostic data.</p> <p>When You access the Service by or through a mobile device, We may collect certain information automatically, including, but not limited to, the type of mobile device You use, Your mobile device unique ID, the IP address of Your mobile device, Your mobile operating system, the type of mobile Internet browser You use, unique device identifiers and other diagnostic data.</p> <p>We may also collect information that Your browser sends whenever You visit our Service or when You access the Service by or through a mobile device.</p>"},{"location":"company/privacy-policy/#tracking-technologies-and-cookies","title":"Tracking Technologies and Cookies","text":"<p>We use Cookies and similar tracking technologies to track the activity on Our Service and store certain information. Tracking technologies used are beacons, tags, and scripts to collect and track information and to improve and analyze Our Service. The technologies We use may include:</p> <ul> <li>Cookies or Browser Cookies. A cookie is a small file placed on Your Device. You can instruct Your browser to refuse all Cookies or to indicate when a Cookie is being sent. However, if You do not accept Cookies, You may not be able to use some parts of our Service. Unless you have adjusted Your browser setting so that it will refuse Cookies, our Service may use Cookies.</li> <li>Web Beacons. Certain sections of our Service and our emails may contain small electronic files known as web beacons (also referred to as clear gifs, pixel tags, and single-pixel gifs) that permit the Company, for example, to count users who have visited those pages or opened an email and for other related website statistics (for example, recording the popularity of a certain section and verifying system and server integrity).</li> </ul> <p>Cookies can be \"Persistent\" or \"Session\" Cookies. Persistent Cookies remain on Your personal computer or mobile device when You go offline, while Session Cookies are deleted as soon as You close Your web browser. You can learn more about cookies on TermsFeed website article.</p> <p>We use both Session and Persistent Cookies for the purposes set out below:</p> <ul> <li> <p>Necessary / Essential Cookies</p> <p>Type: Session Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies are essential to provide You with services available through the Website and to enable You to use some of its features. They help to authenticate users and prevent fraudulent use of user accounts. Without these Cookies, the services that You have asked for cannot be provided, and We only use these Cookies to provide You with those services. - Cookies Policy / Notice Acceptance Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies identify if users have accepted the use of cookies on the Website. - Functionality Cookies</p> <p>Type: Persistent Cookies</p> <p>Administered by: Us</p> <p>Purpose: These Cookies allow us to remember choices You make when You use the Website, such as remembering your login details or language preference. The purpose of these Cookies is to provide You with a more personal experience and to avoid You having to re-enter your preferences every time You use the Website.</p> </li> </ul> <p>For more information about the cookies we use and your choices regarding cookies, please visit our Cookies Policy or the Cookies section of our Privacy Policy.</p>"},{"location":"company/privacy-policy/#use-of-your-personal-data","title":"Use of Your Personal Data","text":"<p>The Company may use Personal Data for the following purposes:</p> <ul> <li>To provide and maintain our Service, including to monitor the usage of our Service.</li> <li>To manage Your Account: to manage Your registration as a user of the Service. The Personal Data You provide can give You access to different functionalities of the Service that are available to You as a registered user.</li> <li>For the performance of a contract: the development, compliance and undertaking of the purchase contract for the products, items or services You have purchased or of any other contract with Us through the Service.</li> <li>To contact You: To contact You by email, telephone calls, SMS, or other equivalent forms of electronic communication, such as a mobile application's push notifications regarding updates or informative communications related to the functionalities, products or contracted services, including the security updates, when necessary or reasonable for their implementation.</li> <li>To provide You with news, special offers and general information about other goods, services and events which we offer that are similar to those that you have already purchased or enquired about unless You have opted not to receive such information.</li> <li> <p>To manage Your requests: To attend and manage Your requests to Us.</p> </li> <li> <p>For business transfers: We may use Your information to evaluate or conduct a merger, divestiture, restructuring, reorganization, dissolution, or other sale or transfer of some or all of Our assets, whether as a going concern or as part of bankruptcy, liquidation, or similar proceeding, in which Personal Data held by Us about our Service users is among the assets transferred.</p> </li> <li>For other purposes: We may use Your information for other purposes, such as data analysis, identifying usage trends, determining the effectiveness of our promotional campaigns and to evaluate and improve our Service, products, services, marketing and your experience. </li> </ul> <p>We may share Your personal information in the following situations:</p> <ul> <li>With Service Providers: We may share Your personal information with Service Providers to monitor and analyze the use of our Service,  to contact You.</li> <li>For business transfers: We may share or transfer Your personal information in connection with, or during negotiations of, any merger, sale of Company assets, financing, or acquisition of all or a portion of Our business to another company.</li> <li>With Affiliates: We may share Your information with Our affiliates, in which case we will require those affiliates to honor this Privacy Policy. Affiliates include Our parent company and any other subsidiaries, joint venture partners or other companies that We control or that are under common control with Us.</li> <li>With business partners: We may share Your information with Our business partners to offer You certain products, services or promotions.</li> <li>With other users: when You share personal information or otherwise interact in the public areas with other users, such information may be viewed by all users and may be publicly distributed outside. </li> <li>With Your consent: We may disclose Your personal information for any other purpose with Your consent.</li> </ul>"},{"location":"company/privacy-policy/#retention-of-your-personal-data","title":"Retention of Your Personal Data","text":"<p>The Company will retain Your Personal Data only for as long as is necessary for the purposes set out in this Privacy Policy. We will retain and use Your Personal Data to the extent necessary to comply with our legal obligations (for example, if we are required to retain your data to comply with applicable laws), resolve disputes, and enforce our legal agreements and policies.</p> <p>The Company will also retain Usage Data for internal analysis purposes. Usage Data is generally retained for a shorter period of time, except when this data is used to strengthen the security or to improve the functionality of Our Service, or We are legally obligated to retain this data for longer time periods.</p>"},{"location":"company/privacy-policy/#transfer-of-your-personal-data","title":"Transfer of Your Personal Data","text":"<p>Your information, including Personal Data, is processed at the Company's operating offices and in any other places where the parties involved in the processing are located. It means that this information may be transferred to \u2014 and maintained on \u2014 computers located outside of Your state, province, country or other governmental jurisdiction where the data protection laws may differ than those from Your jurisdiction.</p> <p>Your consent to this Privacy Policy followed by Your submission of such information represents Your agreement to that transfer.</p> <p>The Company will take all steps reasonably necessary to ensure that Your data is treated securely and in accordance with this Privacy Policy and no transfer of Your Personal Data will take place to an organization or a country unless there are adequate controls in place including the security of Your data and other personal information.</p>"},{"location":"company/privacy-policy/#delete-your-personal-data","title":"Delete Your Personal Data","text":"<p>You have the right to delete or request that We assist in deleting the Personal Data that We have collected about You.</p> <p>Our Service may give You the ability to delete certain information about You from within the Service.</p> <p>You may update, amend, or delete Your information at any time by signing in to Your Account, if you have one, and visiting the account settings section that allows you to manage Your personal information. You may also contact Us to request access to, correct, or delete any personal information that You have provided to Us.</p> <p>Please note, however, that We may need to retain certain information when we have a legal obligation or lawful basis to do so.</p>"},{"location":"company/privacy-policy/#disclosure-of-your-personal-data","title":"Disclosure of Your Personal Data","text":""},{"location":"company/privacy-policy/#business-transactions","title":"Business Transactions","text":"<p>If the Company is involved in a merger, acquisition or asset sale, Your Personal Data may be transferred. We will provide notice before Your Personal Data is transferred and becomes subject to a different Privacy Policy.</p>"},{"location":"company/privacy-policy/#law-enforcement","title":"Law enforcement","text":"<p>Under certain circumstances, the Company may be required to disclose Your Personal Data if required to do so by law or in response to valid requests by public authorities (e.g. a court or a government agency).</p>"},{"location":"company/privacy-policy/#other-legal-requirements","title":"Other legal requirements","text":"<p>The Company may disclose Your Personal Data in the good faith belief that such action is necessary to:</p> <ul> <li>Comply with a legal obligation</li> <li>Protect and defend the rights or property of the Company</li> <li>Prevent or investigate possible wrongdoing in connection with the Service</li> <li>Protect the personal safety of Users of the Service or the public</li> <li>Protect against legal liability</li> </ul>"},{"location":"company/privacy-policy/#security-of-your-personal-data","title":"Security of Your Personal Data","text":"<p>The security of Your Personal Data is important to Us, but remember that no method of transmission over the Internet, or method of electronic storage is 100% secure. While We strive to use commercially acceptable means to protect Your Personal Data, We cannot guarantee its absolute security.</p>"},{"location":"company/privacy-policy/#childrens-privacy","title":"Children's Privacy","text":"<p>Our Service does not address anyone under the age of 13. We do not knowingly collect personally identifiable information from anyone under the age of 13. If You are a parent or guardian and You are aware that Your child has provided Us with Personal Data, please contact Us. If We become aware that We have collected Personal Data from anyone under the age of 13 without verification of parental consent, We take steps to remove that information from Our servers.</p> <p>If We need to rely on consent as a legal basis for processing Your information and Your country requires consent from a parent, We may require Your parent's consent before We collect and use that information.</p>"},{"location":"company/privacy-policy/#links-to-other-websites","title":"Links to Other Websites","text":"<p>Our Service may contain links to other websites that are not operated by Us. If You click on a third party link, You will be directed to that third party's site. We strongly advise You to review the Privacy Policy of every site You visit.</p> <p>We have no control over and assume no responsibility for the content, privacy policies or practices of any third party sites or services.</p>"},{"location":"company/privacy-policy/#changes-to-this-privacy-policy","title":"Changes to this Privacy Policy","text":"<p>We may update Our Privacy Policy from time to time. We will notify You of any changes by posting the new Privacy Policy on this page.</p> <p>We will let You know via email and/or a prominent notice on Our Service, prior to the change becoming effective and update the \"Last updated\" date at the top of this Privacy Policy.</p> <p>You are advised to review this Privacy Policy periodically for any changes. Changes to this Privacy Policy are effective when they are posted on this page.</p>"},{"location":"company/privacy-policy/#contact-us","title":"Contact Us","text":"<p>If you have any questions about this Privacy Policy, You can contact us:</p> <ul> <li> <p>By email: info@visionify.ai</p> </li> <li> <p>By visiting this page on our website: https://www.visionify.ai/contact-us</p> </li> </ul>"},{"location":"company/team/","title":"Changelog","text":""},{"location":"company/team/#visionai-changelog","title":"VisionAI Changelog","text":""},{"location":"company/team/#017-january-22-2023","title":"0.1.7 January 22, 2023","text":"<ul> <li>Implemented camera add/delete functionality</li> </ul>"},{"location":"company/team/#016-january-20-2023","title":"0.1.6 January 20, 2023","text":"<ul> <li>Implemented initial set of commands in different files (dummy implementation)</li> <li>Testing commands individually or through the main application</li> </ul>"},{"location":"company/team/#013-january-16-2023","title":"0.1.3 January 16, 2023","text":"<ul> <li>Basic overview and usage documentation is updated.</li> <li>Started using a termy JS script to show terminal animations nicely</li> </ul>"},{"location":"company/team/#012-january-14-20123","title":"0.1.2 January 14, 20123","text":"<ul> <li>Made MkDocs documents based on Typer format</li> <li>Registered CNAME to point to https://docs.visionify.ai</li> </ul>"},{"location":"company/team/#011-january-11-2023","title":"0.1.1 January 11, 2023","text":"<ul> <li>Updated Azure DevOps CI/CD to automatically publish package on each merge</li> <li>Initial set of commands for visionai application</li> <li>Made <code>visionai</code> as a callable CLI application through poetry</li> </ul>"},{"location":"company/team/#010-january-10-2023","title":"0.1.0 January 10, 2023","text":"<ul> <li>Initial release: <code>pip install visionai</code></li> <li>Pushed package to <code>PyPI</code> repository</li> </ul>"},{"location":"company/terms-and-conditions/","title":"Terms and Conditions","text":"<p>Last updated: February 20, 2023</p> <p>Please read these terms and conditions carefully before using Our Service.</p>"},{"location":"company/terms-and-conditions/#interpretation-and-definitions","title":"Interpretation and Definitions","text":""},{"location":"company/terms-and-conditions/#interpretation","title":"Interpretation","text":"<p>The words of which the initial letter is capitalized have meanings defined under the following conditions. The following definitions shall have the same meaning regardless of whether they appear in singular or in plural.</p>"},{"location":"company/terms-and-conditions/#definitions","title":"Definitions","text":"<p>For the purposes of these Terms and Conditions:</p> <ul> <li> <p>Affiliate means an entity that controls, is controlled by or is under common control with a party, where \"control\" means ownership of 50% or more of the shares, equity interest or other securities entitled to vote for election of directors or other managing authority.</p> </li> <li> <p>Country refers to: Colorado,  United States</p> </li> <li> <p>Company (referred to as either \"the Company\", \"We\", \"Us\" or \"Our\" in this Agreement) refers to DestinationJ Software Technologies LLC, 1499 W 120<sup>th</sup> Ave, Ste 110, Westminster CO 80234.</p> </li> <li> <p>Device means any device that can access the Service such as a computer, a cellphone or a digital tablet.</p> </li> <li> <p>Service refers to the Website.</p> </li> <li> <p>Terms and Conditions (also referred as \"Terms\") mean these Terms and Conditions that form the entire agreement between You and the Company regarding the use of the Service. This Terms and Conditions agreement has been created with the help of the TermsFeed Terms and Conditions Generator.</p> </li> <li>Third-party Social Media Service means any services or content (including data, information, products or services) provided by a third-party that may be displayed, included or made available by the Service.</li> <li>Website refers to Visionify: Workplace Safety through Vision AI, accessible from https://www.visionify.ai</li> <li>You means the individual accessing or using the Service, or the company, or other legal entity on behalf of which such individual is accessing or using the Service, as applicable.</li> </ul>"},{"location":"company/terms-and-conditions/#acknowledgment","title":"Acknowledgment","text":"<p>These are the Terms and Conditions governing the use of this Service and the agreement that operates between You and the Company. These Terms and Conditions set out the rights and obligations of all users regarding the use of the Service.</p> <p>Your access to and use of the Service is conditioned on Your acceptance of and compliance with these Terms and Conditions. These Terms and Conditions apply to all visitors, users and others who access or use the Service.</p> <p>By accessing or using the Service You agree to be bound by these Terms and Conditions. If You disagree with any part of these Terms and Conditions then You may not access the Service.</p> <p>You represent that you are over the age of 18. The Company does not permit those under 18 to use the Service.</p> <p>Your access to and use of the Service is also conditioned on Your acceptance of and compliance with the Privacy Policy of the Company. Our Privacy Policy describes Our policies and procedures on the collection, use and disclosure of Your personal information when You use the Application or the Website and tells You about Your privacy rights and how the law protects You. Please read Our Privacy Policy carefully before using Our Service.</p>"},{"location":"company/terms-and-conditions/#links-to-other-websites","title":"Links to Other Websites","text":"<p>Our Service may contain links to third-party web sites or services that are not owned or controlled by the Company.</p> <p>The Company has no control over, and assumes no responsibility for, the content, privacy policies, or practices of any third party web sites or services. You further acknowledge and agree that the Company shall not be responsible or liable, directly or indirectly, for any damage or loss caused or alleged to be caused by or in connection with the use of or reliance on any such content, goods or services available on or through any such web sites or services.</p> <p>We strongly advise You to read the terms and conditions and privacy policies of any third-party web sites or services that You visit.</p>"},{"location":"company/terms-and-conditions/#termination","title":"Termination","text":"<p>We may terminate or suspend Your access immediately, without prior notice or liability, for any reason whatsoever, including without limitation if You breach these Terms and Conditions.</p> <p>Upon termination, Your right to use the Service will cease immediately.</p>"},{"location":"company/terms-and-conditions/#limitation-of-liability","title":"Limitation of Liability","text":"<p>Notwithstanding any damages that You might incur, the entire liability of the Company and any of its suppliers under any provision of this Terms and Your exclusive remedy for all of the foregoing shall be limited to the amount actually paid by You through the Service or 100 USD if You haven't purchased anything through the Service.</p> <p>To the maximum extent permitted by applicable law, in no event shall the Company or its suppliers be liable for any special, incidental, indirect, or consequential damages whatsoever (including, but not limited to, damages for loss of profits, loss of data or other information, for business interruption, for personal injury, loss of privacy arising out of or in any way related to the use of or inability to use the Service, third-party software and/or third-party hardware used with the Service, or otherwise in connection with any provision of this Terms), even if the Company or any supplier has been advised of the possibility of such damages and even if the remedy fails of its essential purpose.</p> <p>Some states do not allow the exclusion of implied warranties or limitation of liability for incidental or consequential damages, which means that some of the above limitations may not apply. In these states, each party's liability will be limited to the greatest extent permitted by law.</p>"},{"location":"company/terms-and-conditions/#as-is-and-as-available-disclaimer","title":"\"AS IS\" and \"AS AVAILABLE\" Disclaimer","text":"<p>The Service is provided to You \"AS IS\" and \"AS AVAILABLE\" and with all faults and defects without warranty of any kind. To the maximum extent permitted under applicable law, the Company, on its own behalf and on behalf of its Affiliates and its and their respective licensors and service providers, expressly disclaims all warranties, whether express, implied, statutory or otherwise, with respect to the Service, including all implied warranties of merchantability, fitness for a particular purpose, title and non-infringement, and warranties that may arise out of course of dealing, course of performance, usage or trade practice. Without limitation to the foregoing, the Company provides no warranty or undertaking, and makes no representation of any kind that the Service will meet Your requirements, achieve any intended results, be compatible or work with any other software, applications, systems or services, operate without interruption, meet any performance or reliability standards or be error free or that any errors or defects can or will be corrected.</p> <p>Without limiting the foregoing, neither the Company nor any of the company's provider makes any representation or warranty of any kind, express or implied: (i) as to the operation or availability of the Service, or the information, content, and materials or products included thereon; (ii) that the Service will be uninterrupted or error-free; (iii) as to the accuracy, reliability, or currency of any information or content provided through the Service; or (iv) that the Service, its servers, the content, or e-mails sent from or on behalf of the Company are free of viruses, scripts, trojan horses, worms, malware, timebombs or other harmful components.</p> <p>Some jurisdictions do not allow the exclusion of certain types of warranties or limitations on applicable statutory rights of a consumer, so some or all of the above exclusions and limitations may not apply to You. But in such a case the exclusions and limitations set forth in this section shall be applied to the greatest extent enforceable under applicable law.</p>"},{"location":"company/terms-and-conditions/#governing-law","title":"Governing Law","text":"<p>The laws of the Country, excluding its conflicts of law rules, shall govern this Terms and Your use of the Service. Your use of the Application may also be subject to other local, state, national, or international laws.</p>"},{"location":"company/terms-and-conditions/#disputes-resolution","title":"Disputes Resolution","text":"<p>If You have any concern or dispute about the Service, You agree to first try to resolve the dispute informally by contacting the Company.</p>"},{"location":"company/terms-and-conditions/#for-european-union-eu-users","title":"For European Union (EU) Users","text":"<p>If You are a European Union consumer, you will benefit from any mandatory provisions of the law of the country in which you are resident in.</p>"},{"location":"company/terms-and-conditions/#united-states-legal-compliance","title":"United States Legal Compliance","text":"<p>You represent and warrant that (i) You are not located in a country that is subject to the United States government embargo, or that has been designated by the United States government as a \"terrorist supporting\" country, and (ii) You are not listed on any United States government list of prohibited or restricted parties.</p>"},{"location":"company/terms-and-conditions/#severability-and-waiver","title":"Severability and Waiver","text":""},{"location":"company/terms-and-conditions/#severability","title":"Severability","text":"<p>If any provision of these Terms is held to be unenforceable or invalid, such provision will be changed and interpreted to accomplish the objectives of such provision to the greatest extent possible under applicable law and the remaining provisions will continue in full force and effect.</p>"},{"location":"company/terms-and-conditions/#waiver","title":"Waiver","text":"<p>Except as provided herein, the failure to exercise a right or to require performance of an obligation under these Terms shall not effect a party's ability to exercise such right or require such performance at any time thereafter nor shall the waiver of a breach constitute a waiver of any subsequent breach.</p>"},{"location":"company/terms-and-conditions/#translation-interpretation","title":"Translation Interpretation","text":"<p>These Terms and Conditions may have been translated if We have made them available to You on our Service. You agree that the original English text shall prevail in the case of a dispute.</p>"},{"location":"company/terms-and-conditions/#changes-to-these-terms-and-conditions","title":"Changes to These Terms and Conditions","text":"<p>We reserve the right, at Our sole discretion, to modify or replace these Terms at any time. If a revision is material We will make reasonable efforts to provide at least 30 days' notice prior to any new terms taking effect. What constitutes a material change will be determined at Our sole discretion.</p> <p>By continuing to access or use Our Service after those revisions become effective, You agree to be bound by the revised terms. If You do not agree to the new terms, in whole or in part, please stop using the website and the Service.</p>"},{"location":"company/terms-and-conditions/#contact-us","title":"Contact Us","text":"<p>If you have any questions about these Terms and Conditions, You can contact us:</p> <ul> <li> <p>By email: info@visionify.ai</p> </li> <li> <p>By visiting this page on our website: https://www.visionify.ai/contact-us</p> </li> </ul>"},{"location":"custom/","title":"Index","text":""},{"location":"custom/#python-types","title":"Python types","text":"<p>If you need a refresher about how to use Python type hints, check the first part of FastAPI's Python types intro.</p> <p>You can also check the mypy cheat sheet.</p> <p>In short (very short), you can declare a function with parameters like:</p> <pre><code>from typing import Optional\n\ndef type_example(name: str, formal: bool = False, intro: Optional[str] = None):\n    pass\n</code></pre> <p>And your editor (and Typer) will know that:</p> <ul> <li><code>name</code> is of type <code>str</code> and is a required parameter.</li> <li><code>formal</code> is a <code>bool</code> and is by default <code>False</code>.</li> <li><code>intro</code> is an optional <code>str</code>, by default is <code>None</code>.</li> </ul> <p>These type hints are what give you autocomplete in your editor and several other features.</p> <p>Typer is based on these type hints.</p>"},{"location":"custom/#intro","title":"Intro","text":"<p>This tutorial shows you how to use Typer with all its features, step by step.</p> <p>Each section gradually builds on the previous ones, but it's structured to separate topics, so that you can go directly to any specific one to solve your specific CLI needs.</p> <p>It is also built to work as a future reference.</p> <p>So you can come back and see exactly what you need.</p>"},{"location":"custom/#run-the-code","title":"Run the code","text":"<p>All the code blocks can be copied and used directly (they are tested Python files).</p> <p>To run any of the examples, copy the code to a file <code>main.py</code>, and run it:</p> <pre><code>$ python main.py\n\n\u2728 The magic happens here \u2728\n</code></pre> <p>It is HIGHLY encouraged that you write or copy the code, edit it and run it locally.</p> <p>Using it in your editor is what really shows you the benefits of Typer, seeing how little code you have to write, all the type checks, autocompletion, etc.</p> <p>And running the examples is what will really help you understand what is going on.</p> <p>You can learn a lot more by running some examples and playing around with them than by reading all the docs here.</p>"},{"location":"custom/#install-typer","title":"Install Typer","text":"<p>The first step is to install Typer.</p> <p>For the tutorial, you might want to install it with all the optional dependencies and features:</p> <pre><code>$ pip install \"typer[all]\"\n---&gt; 100%\nSuccessfully installed typer click shellingham rich\n</code></pre> <p>...that also includes <code>rich</code> and <code>shellingham</code>.</p>"},{"location":"custom/faqs/","title":"VisionAI Customization FAQs","text":"<p>Learn more about customization of models</p>"},{"location":"custom/faqs/#what-is-customization-of-models","title":"What is customization of models?","text":"<p>Customization of models lets you manage AI models based on your data and its semantics. Our solution help you choose the data and scenarios on which you wish to generate insights.</p>"},{"location":"custom/faqs/#does-visionai-solution-can-interact-with-my-existing-camera","title":"Does VisionAI solution can interact with my existing camera?","text":"<p>Our ready-to-deploy models can be installed on your system with the help of ARM templates. After this, you can add and customize camera settings per the scenarios. Since our solution works with most consumer and commercial-grade cameras, you\u2019ll likely not have difficulty using our services.</p>"},{"location":"custom/faqs/#how-can-i-handle-large-number-of-classes","title":"How can I handle large number of classes?","text":"<p>We have a dedicated team of annotators. Any number of classes can be added/removed from the model easily. We support model customization through class selection exclusively.</p>"},{"location":"custom/faqs/#how-do-i-get-a-solution-for-a-use-case-thats-not-included-in-visionai-tool-kit","title":"How do I get a solution for a use-case that's not included in VisionAI tool kit?","text":"<p>We are open to add new any use-case in VisionAI depending on customer's Business perspective. We would be building custom models for any new case.</p>"},{"location":"custom/licensing/","title":"License","text":"<p>Thank you for choosing our VisionAI workplace safety application to help ensure the safety of your employees and comply with relevant workplace safety regulations. In order to use our application, you must agree to the following custom license terms:</p> <p>You may use our application for your own internal business purposes only. You may not use our application for any other purpose, including but not limited to, any commercial purpose, or for any purpose that is unlawful or violates any third party rights.</p> <p>You may not modify, adapt, translate, or create derivative works based on our application, or any part of our application, except as expressly permitted by us.</p> <p>By using our workplace safety application, you agree to be bound by these custom license terms. If you do not agree to these terms, do not use our application. If you have any questions or concerns about our license terms, please contact us.</p>"},{"location":"custom/overview/","title":"Customization","text":"<p>We offer extensive cutomization for VisionAI Toolkit in the form of various custom integration services</p>"},{"location":"custom/overview/#overview","title":"Overview","text":"<p>Customization can take many different forms, such as:</p> <ul> <li> <p>Retraining:</p> <p>Model retraining is the process of adding new data to an existing AI model and evaluating the model performance by fine tuning.</p> <p>Find more details about this here.</p> </li> <li> <p>Custom Integration:</p> <p>Our Custom Integration services deal with enabling analytics support in the form of dynamic reporting, online alerts and high scalability with integration of Azure IoT hub.</p> <p>Find more details about this here.</p> </li> </ul>"},{"location":"custom/request/","title":"Custom Integration","text":"<p>Documentation for VisionAI customization</p>"},{"location":"custom/request/#overview","title":"Overview","text":"<p>VisionAI Toolkit provides end-to-end data customization and analytics services. We provide extensive analytics expertise, as well as domain expertise, to assist clients in transforming their businesses.</p>"},{"location":"custom/request/#process","title":"Process","text":"<p>We can assist you in optimising business use-cases so that you can focus on activities that generate true business value. </p> <p> </p> <p>This includes:</p> <ul> <li> <p>Continous Monitoring: We use the ease of a single web console to conveniently monitor your whole work place. We assist you in avoiding problems before they affect or are noticed by users. Using our monitoring service, learn more about:</p> <ol> <li>Event logs</li> <li>Root Cause Analysis</li> <li>Traffic monitoring</li> </ol> </li> <li> <p>Alerts: Quick alerts and notifications for events (eg. accidents/non-compliance) can be send through emails/messages.</p> </li> <li> <p>Reporting: Detailed reports can be generated for daily/monthly basis. Frequency of reports can be customized to accomodate user preferences. Granularity depends on user's business perspective.</p> </li> </ul> <p>For example, </p> <ol> <li> <p>Ensuring true compliance for Personal protective equipment(PPE) by generating instant alerts</p> </li> <li> <p>Providing daily and summary reports to give in-depth insights</p> </li> <li> <p>Providing flexibility by customizing different options </p> </li> </ol> <p>Current, real-time web applications pose unique horizontal scalability challenges.  Therefore, our current architecture uses pub/sub messaging mechanism to push events notifications to Redis/ Graffana. These can be integrated through Azure Event Grid to Azure IoT hub. An architecture to support this would be:</p> <p></p>"},{"location":"custom/retraining/","title":"Retraining","text":"<p>We offer retraining of models, as we have a well-defined process for working with clients, which could include steps such as data collection and labeling, model fine-tuning, and evaluation. We also have a strong infrastructure in place to support the processing and analysis of large volumes of data.</p> <p>Retraining steps would be: <pre><code>graph LR\nA[Gathering Custom data] --&gt; B[Labeling Data]\nB[Fine-tuning the model] --&gt; C[Evaluating the model]</code></pre></p>"},{"location":"custom/retraining/#overview","title":"Overview","text":"<p>Retraining of computer vision models refers to the process of updating a pre-trained model with additional data or new labels. This is often done to improve the performance of a model or to adapt it to a new task or application.</p> <p>The process of retraining a computer vision model typically involves the following steps:</p> <ul> <li> <p>Collecting new data: This involves gathering new images or video data that is relevant to the new task or application.</p> </li> <li> <p>Labeling the data: The new data needs to be labeled to provide the model with the correct information about what is in the images. We have a dedicated team of annotators to accomodate this.</p> </li> <li> <p>Fine-tuning the model: The pre-trained model is then fine-tuned on the new labeled data. This involves adjusting the model's parameters to better fit the new data and labels.</p> </li> <li> <p>Evaluating the model: The retrained model is then evaluated to determine how well it performs on the new task or application.</p> </li> </ul> <p>Our team  is up-to-date with the latest techniques and best practices in the field, as well as the ethical and legal considerations related to working with sensitive data.  They have a good understanding of the principles of computer vision and machine learning, as well as the specific requirements of the application, to ensure the retraining process is successful.</p>"},{"location":"overview/azure-managed-app/","title":"Azure Managed App","text":"<p>VisionAI Azure Managed Application is a  pre-built cloud solution that is deployed through Azure Marketplace to an Azure environment for end customers. </p>"},{"location":"overview/azure-managed-app/#overview","title":"Overview","text":"<p>VisionAI Azure Managed application is designed to provide a fast and secure way to deliver applications and services to customers while ensuring consistency and control.</p> <p>Basically, Managed Applications is a packaged solution that include all the necessary resources and components, such as virtual machines, storage accounts, networking resources, and security configurations. </p>"},{"location":"overview/azure-managed-app/#access-visionai-app","title":"Access VisionAI App","text":"<p>VisionAI Azure App is accessible by logging into Azure Market Place.</p> <p> It shows its Overview, different plans and ratings. Once we click, Get it Now. Following screen appears.</p> <p></p> <p>Enter all your information and click on Continue. It takes you to your dashboard as:</p> <p></p> <p>In Basics tab, Enter your project details in the following screen.</p> <p></p> <p>If you have empty resource group, please select that otherwise create new by clicking on Create new.</p> <p></p> <p>Verify Virtual Machine Settings. Click on Review+create. It takes some time to perform validation.</p> <p>The Managed Applications can be customized with branding, pricing, and support offerings, allowing MSPs to differentiate their offerings and provide added value to their customers.</p> <p>In summary, Azure Managed Applications offer a simplified and streamlined way to deploy and manage pre-built cloud solutions, enabling customers to focus on their core business functions while leaving the management and maintenance of the underlying infrastructure to Microsoft.</p>"},{"location":"overview/camera-placement-guide/","title":"Camera Placement Guide","text":"<p>Use your existing camera to integrate with VisionAI platform </p> <p>Proper camera placement is essential to ensure that all areas of the workplace are adequately covered.</p>"},{"location":"overview/camera-placement-guide/#general-guiudelines","title":"General guiudelines","text":"<p>When positioning cameras for various use situations, take into account the following general guidelines: </p> <ul> <li>Lighting: Install cameras underneath a light fittings so that they do not obscure the cameras.</li> <li>Backlighting: Avoid mounting cameras near to window or other areas to protect from backlighting issue. It affects image quality.</li> <li>Local policies: Take into account local placement policies and laws.</li> <li>Authorization: The installation of cameras should be authorized by a designated person or department within the organization. </li> <li>Maintenance: Cameras should be regularly maintained and checked to ensure they are functioning properly. The policy should specify who is responsible for maintaining the cameras and how often they should be checked.</li> </ul> <p>Note</p> <p>Overall, it's important to develop local policies for camera mounting that balance the need for surveillance with the protection of privacy rights. The policies should be reviewed and updated regularly to ensure they remain relevant and effective.</p>"},{"location":"overview/camera-placement-guide/#key-factors","title":"Key factors","text":"<p>When setting up cameras there are three key factors to consider: </p> <ol> <li>Camera height</li> <li>Camera-to-focal-point distance, and </li> <li>Camera angle relative to the floor plane.</li> </ol> <p>It's crucial to identify the direction in which the majority of people are walking in relation to the camera's field of view. This direction is important for optimal system performance.</p> <p></p>"},{"location":"overview/camera-placement-guide/#camera-height","title":"Camera height","text":"<p>Camera height is an important consideration when determining the field of view for workplace safety cameras. For example, cameras should be placed high enough to capture the entire area of interest, but not so high that the camera view becomes distorted or difficult to interpret. Additionally, the height of the camera should take into account any obstacles or obstructions that may block the camera's field of view.</p>"},{"location":"overview/camera-placement-guide/#camera-to-focal-point-distance","title":"Camera-to-focal-point distance","text":"<p>The camera-to-focal-point distance is the distance between the camera lens and the focal point of the area being monitored. This distance is important because it determines the level of detail captured by the camera. If the camera is too far away from the focal point, the resulting image may lack the necessary detail to accurately capture safety hazards or other important information.</p> <p></p> <p>This distance is measured on the floor plane.</p> <p></p> <p>From above, it looks like this:</p> <p></p> <p>The following illustration simulates camera views from the closest and farthest camera-to-focal-point distances.</p> Closest Farthest"},{"location":"overview/camera-placement-guide/#camera-angle-mounting-ranges","title":"Camera angle mounting ranges","text":"<p>The angle of the camera relative to the floor plane is important for capturing accurate footage of safety hazards and other workplace activities. For example, cameras should be positioned to capture a wide field of view, but not at such an extreme angle that the resulting image becomes distorted or difficult to interpret. Additionally, the angle of the camera should take into account the direction that people are walking in relation to the camera field of view, as this can impact the performance of the system.</p> <p>The following illustration simulates camera views using the leftmost (-) and rightmost (+) mounting angle recommendations:</p> Leftmost view Rightmost view <p>The following illustration shows camera placement and mounting angles from a birds-eye view.</p> <p></p> <p>It's important to carefully consider camera height, camera-to-focal-point distance, and camera angle relative to the floor plane. Taking these factors into account can help ensure that cameras are positioned to capture accurate footage of workplace hazards and other important safety information.</p>"},{"location":"overview/camera-placement-guide/#camera-view","title":"Camera View","text":"<p>The camera view refers to the field of vision captured by a camera. The camera view is determined by the placement of the camera and its angle of view.</p> <p>There are two primary modes of camera placement that are considered for VisionAI workplace safety scenarios:</p> <ul> <li>Ceiling-mounted and </li> <li>Straight-mounted.</li> </ul>"},{"location":"overview/camera-placement-guide/#ceiling-mounted-cameras","title":"Ceiling-mounted cameras","text":"<ul> <li>Ceiling-mounted cameras are typically installed on the ceiling or high up on a wall and are pointed downwards. They provide a wide-angle view of the area below.</li> <li>These cameras are ideal for monitoring larger areas, such as open workspaces, warehouses, or production floors, where a bird's eye view is necessary to capture all activities in the space.</li> <li>These cameras can also be used in areas where there are obstructions that would block the view of a straight-mounted camera.</li> <li> <p>These cameras can cover larger areas with fewer cameras, making them cost-effective and efficient.</p> <p>The following illustration provides simulations for the camera ceiling views.</p> Example 1 Example 2 </li> </ul>"},{"location":"overview/camera-placement-guide/#straight-mounted-cameras","title":"Straight-mounted cameras","text":"<ul> <li>Straight-mounted cameras are mounted at eye level or lower on a wall or a stand and are pointed straight ahead. They  provide more focused view of the area in front of them. </li> <li>These cameras are ideal for monitoring smaller areas, such as corridors, entrances and exits,  where a more focused view is required for capturing specific activities. </li> <li>These cameras are also useful for capturing facial features and other details as they are closer to the subject being monitored.</li> <li> <p>One of the advantages of straight-mounted cameras is that they are often easier to install and adjust, and they allow for more detailed and accurate identification of individuals. </p> <p>The following illustration provides simulations for the camera front views.</p> Example 1 Example 2 </li> </ul> <p>When determining which mode of camera placement to use, it's important to consider the specific needs of required use case. Factors such as the size of the area to be monitored, the level of detail required, and the presence of obstructions should all be taken into account to ensure that the cameras are installed in the most effective and efficient manner possible.</p>"},{"location":"overview/cameras/","title":"Cameras","text":""},{"location":"overview/cameras/#cameras","title":"Cameras","text":"<p>An organization can have multiple cameras that are installed at different places. They may be from different vendors and/or maybe using different security surveillance software. Most cameras however do support RTSP, RTMP or HLS streams as an output. Please refer to your camera vendor documentation to find this out. This module will help you onboard those cameras on visionai systems by using a simple named instance for each camera.</p>"},{"location":"overview/cameras/#add-camera","title":"Add camera","text":"<ul> <li>Add a named camera instance through the following command:</li> </ul> <pre><code>$ visionai camera add --name OFFICE-01 --url rtsp://192.168.0.1:554/1\n</code></pre>"},{"location":"overview/cameras/#add-scenario","title":"Add scenario","text":"<ul> <li>Add a scenario for a camera through the following command:</li> </ul> <pre><code>$ visionai camera add-scenario --name OFFICE-01 --scenario ppe-detection\n</code></pre>"},{"location":"overview/cameras/#list-cameras","title":"List cameras","text":"<ul> <li>List available cameras through the following command: <pre><code>$ visionai camera list\n</code></pre></li> </ul>"},{"location":"overview/cameras/#list-scenarios","title":"List scenarios","text":"<ul> <li>List scenarios configured for a camera through the following command:</li> </ul> <pre><code>$ visionai camera list-scenarios --name OFFICE-01\n</code></pre>"},{"location":"overview/cameras/#preview-camera","title":"Preview camera","text":"<ul> <li>Preview the camera system through the following command:</li> </ul> <pre><code>$ visionai camera preview --name OFFICE-01\n</code></pre>"},{"location":"overview/cameras/#remove-camera","title":"Remove camera","text":"<ul> <li>Remove a camera from the system through the following command:</li> </ul> <pre><code>$ visionai camera remove --name OFFICE-01\n</code></pre>"},{"location":"overview/cameras/#remove-scenario","title":"Remove scenario","text":"<ul> <li>Remove a scenario from a camera through the following command:</li> </ul> <pre><code>$ visionai camera remove-scenario --name OFFICE-01 --scenario ppe-detection\n</code></pre>"},{"location":"overview/cameras/#reset-camera","title":"Reset camera","text":"<ul> <li>Reset all camera configuration through the following command:</li> </ul> <pre><code>$ visionai camera reset\n</code></pre>"},{"location":"overview/faqs/","title":"VisionAI platform FAQs","text":"<p>Learn more about the platform</p>"},{"location":"overview/faqs/#what-is-visional","title":"What is VisionAl ?","text":"<p>VisionAI is ready-to-use Python Library for various Computer Vision Scenarios.</p>"},{"location":"overview/faqs/#what-scenarios-do-you-support","title":"What scenarios do you support?","text":"<p>VisionAI library is focused on common workplace and employee health &amp; safety scenarios. At a high-level these include the employee health and safety hazard &amp; fire warnings, equipment monitoring, vehicle monitoring people &amp; productivity monitoring, auspicious activity monitoring and common company compliance policies.</p>"},{"location":"overview/faqs/#do-i-need-to-install-get-new-cameras-to-run-this-system","title":"Do I need to install get new cameras to run this system?","text":"<p>No! You do not need any new camera or hardware to run this system. VisionAl works with your existing security camera infrastructure. We support RTSP, RTMP, HLS and other common video platforms. Current safety surveillance systems are just record and playback \u2013 we can bring a lot of operational and safety insights from the current camera systems.</p>"},{"location":"overview/faqs/#is-it-free-to-use-how-does-the-licensing-work","title":"Is it Free to use? How does the licensing work?","text":"<p>Licency details can be found at here.</p>"},{"location":"overview/faqs/#how-can-i-try-it-out-quickly","title":"How can I try it out quickly?","text":"<p>We recommend testing this on a beefy machine with a NVIDIA graphics card. To quickly test out a scenario you can follow these commands.</p> <p><pre><code>$ pip install visionai\n\n$ visionai web\n</code></pre> And then browse the different scenarios, create pipelines for our organization. We also provide a Azure Managed App which has all the dependencies pre-installed. </p>"},{"location":"overview/faqs/#how-can-i-customize-the-models-to-work-in-my-environment","title":"How can I customize the models to work in my environment?","text":"<p>We work with our clients to quickly create customized models for their use-cases. This is available to be purchased through Azure marketplace as a Consulting Service-we recommend this option for a quicker transaction. You can find more details here.</p>"},{"location":"overview/faqs/#how-do-i-ensure-that-my-images-are-not-used-in-training-other-models","title":"How do I ensure that my images are not used in training other models?","text":"<p>We take our customers data privacy very seriously. All our current models available in the community edition are based off of open-source datasets We have several customer specific models trained on private data, but those datasets are maintained on their own storage accounts. They are not used for training any publicly available models.</p>"},{"location":"overview/faqs/#we-already-have-some-vision-al-models-running-can-you-integrate-with-them","title":"We already have some Vision Al models running, can you integrate with them?","text":"<p>Our system is isoluated on its own and all it needs is a set of comera output. We are focused on building more use-cases to cover common safety and compliance scenarios As such, we would work with you to build a framework where these use cases are isolated and easy to. integrate into your organization.</p>"},{"location":"overview/faqs/#we-have-an-inhouse-ml-team-how-does-this-help-them","title":"We have an inhouse ML team. How does this help them?","text":"<p>Our license terms are flexible to provide your in-house ML team a starting point where they can build their new use-cases. We also provide a customer success team that can with you to understand your requirements and guide you in coming up with the right solutions for the problems you are working on.</p>"},{"location":"overview/how-it-works/","title":"How it works","text":""},{"location":"overview/how-it-works/#visionai","title":"VisionAI","text":"<p>The VisionAI application monitors employee activity, environment, and equipment in real-time. It uses cameras, sensors and other tools to detect any unsafe conditions or practices. The VisionAI application can also detect any hazardous materials or substances present in the workplace and alert the appropriate personnel.</p> <p>With VisionAI toolkit employers can reduce risks and improve safety for their employees.</p> <p>Workplace safety is an important issue in any business, and having a comprehensive workplace safety application can help to ensure the safety of employees, customers, and visitors. An effective safety application should be designed to integrate with existing security camera infrastructure, provide a wide range of scenarios to run, and enable users to configure their own alerting system. VisionAI toolkit is designed to meet these requirements.</p> <p>VisionAI toolkit works in the following 3 simple steps:</p> <pre><code>graph TD\nA[Start with existing camera infrastructure] --&gt; \nB[Pick-n-Choose scenarios ] --&gt; C[Get alerts and insights]</code></pre>"},{"location":"overview/how-it-works/#existing-camera-infrastructure","title":"Existing camera infrastructure","text":"<p>The first step is getting started with your existing security camera infrastructure. CCTV cameras should be located in all parts of the building, including high-traffic areas, exits, and entrances, as well as any areas that may be considered to be at higher risk. Having a comprehensive view of the building\u2019s layout will enable the safety application to detect any potential safety hazards, such as intruders, or suspicious activities.</p>"},{"location":"overview/how-it-works/#pick-n-choose-scenarios","title":"Pick-n-Choose scenarios","text":"<p>Once the security camera infrastructure is in place, users can then pick-n-choose the scenarios they would like the application to run. These might include the detection of people entering restricted areas, slip-and-fall, or the monitoring of activity in high-risk areas. Furthermore, the application can raise alerts when certain activities are taking place, such as a person loitering in a particular area, or a large group gathering in a restricted area.</p>"},{"location":"overview/how-it-works/#get-alerts-and-insights","title":"Get alerts and insights","text":"<p>Finally, users can configure their alerting system within the web-app. This can include email notifications, text messages, or even automated messages sent to the relevant authorities. Users can also set the parameters for when the alert should be triggered, such as when the number of people in a certain area exceeds a certain threshold, or when a particular activity is detected.</p> <p>In summary, having a comprehensive workplace safety application in place is essential for any business. Integrating the application with existing security camera infrastructure will enable users to detect any potential safety hazards, while setting up the right scenarios and alerting system will ensure that the safety application is always functioning as it should. With the right workplace safety application in place, businesses can enjoy peace of mind, knowing that their employees, customers, and visitors are safe and secure.</p>"},{"location":"overview/next-steps/","title":"Next steps","text":"<p>This provides a comprehensive guideline for the VisionAI toolkit's access path.</p> <p>In summary, the VisionAI toolkit is accessible via direct installation, web-app, and Azure managed app. This makes it more adaptable and dynamic.</p>"},{"location":"overview/next-steps/#install-the-application","title":"Install the application","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <p><pre><code>$ pip install visionai\n</code></pre> Test the scenario from your local web-cam by mentioning scenario name</p> <pre><code>$ visionai scenario test [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>NAME can be any of the scenarios integrated in VisionAI</p> <ul> <li>Example</li> </ul> <pre><code>$ visionai scenario test ppe-detection\n\nDownloading models for scenario: ppe-detection\nModel: ppe-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: ppe-detection..\n</code></pre> <p>You should be able to see the events generated on your console window with the detections of safety gloves, goggles, helmet, mask, safety-shoes and vest within the camera field of view.</p> </li> </ul>"},{"location":"overview/next-steps/#access-the-visionai-web-app","title":"Access the visionAI Web-app","text":"<p>VisionAI web-app, a software application that runs in a web browser and designed to provide a user-friendly interface and functionality that can be accessed from any device with an internet connection, without the need for installation on the device. It can be accessed by here.</p> <p>The app has built-in functionality to accomodate different scenarios and wide range of camera instances.</p> <p></p>"},{"location":"overview/next-steps/#access-the-azure-managed-app","title":"Access the Azure Managed-app","text":"<p>The VisionAI Azure Managed application is intended to provide customers with a quick and secure way to deliver applications and services while maintaining consistency and control.</p> <p>VisionAI Azure App is accessible by logging into Azure Market Place.</p> <p> The appeared screen shows its Overview, different plans and ratings. To access it, click on Get it Now and follow the sequence of steps. </p> <p>Find more details about these sections here.</p>"},{"location":"overview/scenarios/","title":"Scenarios","text":"<p>Scenarios form the building blocks of VisionAI platform. These scenarios are organized into <code>Suites</code>. Below we talk about different suites and the scenarios that are part of them.</p> <ul> <li>All scenarios are available as pick-n-choose scenarios. You can pick the scenarios you want based on your business needs. Each scenario is independently tested.</li> <li>Events provided by these scenarios are given below. Events are sent to Redis &amp; Azure EventHub pubsub systems for further integration.</li> <li>There are a few common events supported by all scenarios (daily summary, weekly summary etc.)</li> <li>Currently supported scenarios are highlighted by a \u2705. Roadmap scenarios are highlighted by a \ud83d\udcc5.</li> <li>Each of the scenarios can be quickly tested through <code>visionai run &lt;scenario-name&gt;</code> command. For example:</li> </ul> <pre><code>visionai run smoke-and-fire-detection\n</code></pre> <p>New scenario request</p> <p>This section lists down all the scenarios that are supported by the VisionAI platform. There are more scenarios added daily - please send a request to us about any additional scenarios you need.</p>"},{"location":"overview/scenarios/#privacy-suite","title":"Privacy Suite","text":"<p>For a majority of organizations - employee privacy is a top concern. Along with employee privacy, the organization needs to make sure that any data does not leave the premises. Any faces detected through Vision AI system need to be blurred, along with text, signage, computer screens and other sensitive information.</p> <p>Before any other scenarios are run, or before we store or process the images - the images are pre-processed through this privacy suite. As such, privacy suite is treated differently from other scenarios. Below examples provide a high-level overview of the privacy suite.</p> Status Scenario name Details Additional considerations \u2705 <code>face-blurring</code> Blur any faces detected More details \u2705 <code>text-blurring</code> Blue any text detected (paper, computer screens etc) More details \u2705 <code>license-plate-blurring</code> Blur any license plates detected More details \ud83d\udcc5 <code>signs-blurring</code> Blur any signs detected More details \ud83d\udcc5 <code>obstructed-camera</code> If camera feed is obstructed, send an alert More details"},{"location":"overview/scenarios/#hazard-warnings-suite","title":"Hazard Warnings Suite","text":"<p>Following scenarios provide hazard warning examples supported by VisionAI suite. Currently supported scenarios are highlighted by a \u2705. </p> Status Scenario name Supported Events Additional considerations \u2705 <code>smoke-and-fire-detection</code> <code>Smoke event detected</code> <code>Fire event detected</code> <code>Sparks detected</code> <code>Open flames detection</code> More details \u2705 <code>no-smoking-zone</code> <code>Smoking event detected</code> <code>Vaping event detected</code> More details \ud83d\udcc5 <code>spills-and-leak-detection</code> <code>Water puddle detected</code> <code>Water leak from equipment detected</code> <code>Spill event detected</code> <code>Slippery sign detected</code> \ud83d\udcc5 <code>missing-fire-extinguisher</code> <code>Fire extinguisher missing</code> \ud83d\udcc5 <code>blocked-exit-monitoring</code> <code>Blocked exit detected</code> \u2705 <code>rust-and-corrosion-detection</code> <code>Rust or corrosion event detected</code> More details"},{"location":"overview/scenarios/#worker-health-safety-suite","title":"Worker Health &amp; Safety Suite","text":"<p>Following scenarios provide Worker Health and Safety examples supported by VisionAI suite. (Also referred to as Personnel Health and Safety).</p> <p>Workplace Personnel Health &amp; Safety is important because it ensures that employees are safe and healthy in their work environment. This includes providing a safe and healthy work environment, proper safety training, and regular safety inspections. Additionally, it also includes enforcing safety policies to ensure that all employees are aware of and follow safety procedures, as well as encouraging a culture of safety within the workplace.</p> <p>Currently supported scenarios are highlighted by a \u2705. </p> <p>You can see real-time events generated as soon as person is detected without PPE (helmets, gloves, safety boots etc.). There are options to configure what PPE's are required for your scenario. This can be done through the VisionAI web-application which can be accessed on through http://localhost:3001.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>ppe-detection</code> <code>Person detected without helmet</code> <code>Person detected without gloves</code> <code>Person detected without safety boots</code> <code>Person detected without safety goggles</code> <code>Person detected without face mask</code> <code>Person detected without vest</code> <code>Person detected without full-body suit</code> <code>Person detected without PFAS</code> <code>Person detected without ear protection</code> More details \u2705 <code>working-at-heights</code> <code>Person detected without PFAS</code> <code>Steps detected without railings</code> <code>Person detected at height without parapets</code> <code>Ladder detected not in compliance</code> More details \u2705 <code>fall-and-accident-detection</code> <code>Person slip &amp; fall detected</code> <code>Potential collision/accident detected</code> <code>Wet floor detected</code> <code>Debris detected on floor</code> <code>Wet/slippery sign detected</code> \u2705 <code>posture-and-ergonomics</code> <code>Bend count per individual</code> Straight camera angle  More details \ud83d\udcc5 <code>empty-pallets-detection</code> <code>Empty pallets detected</code> <code>Partially empty pallets detected</code> \ud83d\udcc5 <code>spills-and-leaks-detection</code> <code>Water puddle detected</code> <code>Water leak from equipment detected</code> <code>Wet floor detected</code> <code>Spill event detected</code> <code>Slippery sign detected</code> \ud83d\udcc5 <code>hand-wash-compliance</code> <code>Missed hand wash</code> \u2705 <code>confined-spaces-monitoring</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details"},{"location":"overview/scenarios/#occupancy-policies","title":"Occupancy Policies","text":"<p>Occupancy Policies relate to counting and tracking employees and/or other personnel in the room. These could include people-counting and enforcing max-occupancy policies, or tracking people's dwell time in a confined space.</p> <p>Currently supported scenarios are highlighted by a \u2705. </p> <p>Occupancy Metrics</p> <ul> <li>Occupancy metrics is similar in structure to max-occupancy, or restricted areas scenarios.</li> <li>However it sends out a summary event is structured like this. This will give a granular summary event at the end of the day.</li> <li>Users can start with occupancy-metrics and then move to max-occupancy or restricted areas if they need to enforce policies. <pre><code>{\n  \"date\": \"2023-02-23\",\n  \"stations\": [{\n    \"id\": \"station_1\",\n    \"hours\": [\n        {\n          \"start_time\": \"2023-02-23T14:00:01\",\n          \"end_time\": \"2023-02-23T15:00:00\",\n          \"occupancy_cnt\": 14\n        }\n        ...\n    ]\n  }...]\n}\n</code></pre></li> </ul> <p>Also need to specify that the camera needs to be configured to have a good view of the stations where occupancy metrics need to be checked.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>max-occupancy</code> <code>Person count exceeds max limit</code> More details \u2705 <code>restricted-areas</code> <code>Person detected in restricted area</code> <code>Movement detected in restricted area</code> <code>Person detected after hours</code> <code>Movement detected after hours</code> More details \u2705 <code>dwell-time</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details \u2705 <code>station-occupancy</code> <code>Daily summary event</code> More details \ud83d\udcc5 <code>occupancy-metrics</code> <code>Daily summary event</code> \ud83d\udcc5 <code>authorized-personnel-only</code> <code>Unauthorized person detected</code>"},{"location":"overview/scenarios/#company-policies","title":"Company Policies","text":"<p>Company policies include specific scenarios that are relevant to your company. These could include scenarios like no-smoking/no-vaping zones, no food or drinks in certain areas, or no cell phones/pictures in certain areas. Some of these scenarios overlap with occupancy policies, but they are still useful to have here as separate scenarios.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>no-food-or-drinks-allowed</code> <code>Person with food detected</code> <code>Person with drinks detected</code> <code>Spill event detected</code> More details \u2705 <code>no-phone-text-pictures</code> <code>Cellphone usage detected</code> <code>Person detected taking pictures</code> More details \u2705 <code>no-smoking-or-vaping</code> <code>Smoking event detected</code> <code>Vaping event detected</code> More details \u2705 <code>no-children-pets-visitors</code> <code>Children detected</code> <code>Pets detected</code> <code>Visitors detected</code> More details"},{"location":"overview/scenarios/#suspicious-activity-detection","title":"Suspicious Activity detection","text":"<p>Suspicious activity detection suite relies on a combination of activity detection models and object detection models. These models are trained to detect suspicious activity in a variety of scenarios.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>vandalism-graffiti-company-property-destruction</code> <code>Motion detected in area (gross event)</code> <code>People detected in area (more granular event)</code> <code>Non-uniformed personnel detected in area</code> <code>Non badged personnel detected in area</code> <code>Vandalism detected in area (before &amp; after)</code> <code>Paint/graffiti detected in area (before &amp; after changes)</code> <code>Behavior analysis event showing company property destruction.</code> More details \u2705 <code>firearms-knives-detection</code> <code>Person brandishing firearm</code> <code>Person brandishing knives</code> More details"},{"location":"overview/scenarios/#next-steps","title":"Next Steps","text":"<p>Now that you have a better understanding of the scenarios that are available, you can start to think about how you can organize these scenarios into a solution that meets your needs. You can also go to the individual scenario page to learn more about it. We can customize each of these models for your use-cases and provide you with a solution that is tailored to your needs. You can contact us through this page.</p> <ol> <li> <p>This works by detecting a person's uniform and comparing it to a list of authorized personnel. This is a more advanced scenario and requires a custom model to be trained for your specific use-case.\u00a0\u21a9</p> </li> </ol>"},{"location":"overview/web-app-userguide/","title":"Visionai Web Application Instructions","text":"<p>The VisionAI  web application is the main interface through which users can interact with the system. </p> <p>It supports the following functionality: 1. Onboard any IP cameras onto the VisionAI System. 2. Pick and choose AI scenarios you want to run on these IP cameras. 3. Configure events, notifications (email/text) and video recording settings. 4. Configure video clips recording for different events.</p> <p>The web-application is available at http://localhost. </p>"},{"location":"overview/web-app-userguide/#basic-usage","title":"Basic Usage","text":"<ol> <li>Please open localhost in the browser.</li> <li>Use your default username/password as master/master.</li> <li>After this, you will be asked to create a new admin user. Please use a strong password and create an admin user.</li> </ol>"},{"location":"overview/web-app-userguide/#cameras","title":"Cameras","text":"<p>Once you are signed in, you will see a blank dashboard page.  Let\u2019s add an IP camera to the system. In order to do this, 1. Go to the \u201cCameras\u201d tab on the left menu bar. 2. Then Click on the  + button. 3. If you want to add multiple cameras at a time, then you can click on button, next to the add camera button.</p> <p></p> <p>A new pop-up window will appear to add cameras. You can enter the camera name, description, and RTSP URI for the camera. The RTSP URI can be obtained from the Camera or NVR documentation. You can ignore the other fields as they are optional. Click on \u201cAdd\u201d button.</p> <p></p> <p>If you want to add multiple cameras at a time, then you can click on button, next to the add camera button.</p> <p></p> <p>Once you have added the camera, it should appear on the Cameras window and should show the initial streaming for the camera. Add any additional cameras in a similar fashion. Once all cameras have been added, the front-screen should look like this:</p> <p></p>"},{"location":"overview/web-app-userguide/#scenarios","title":"Scenarios","text":"<p>Once you are signed in, you will see a blank dashboard page.  You may select any scenario from the list of active scenarios.</p> <p></p> <p>This shows details about the Scenario, the model version used, events supported and model accuracy, recall and F1 score metrics. You can now click on the \u201cGet this\u201d button again to apply the scenario to cameras.</p> <p></p> <p>In the next page, Select the Cameras for which you want to apply this scenario. Click \u201cSave and Next\u201d</p> <p></p> <p>In the next page, you can specify notification details.  Click on the Submit button once the details have been entered.</p> <p></p> <p>Additionally, we have a list of scenarios that our team is currently working on. It can be viewed under the Upcoming Scenarios section</p> <p></p>"},{"location":"overview/web-app-userguide/#events","title":"Events","text":"<p>This page will show the list of events that have occurred in the selected time frame.</p> <p></p>"},{"location":"overview/web-app-userguide/#graphs","title":"Graphs","text":"<p>This page summarizes the events that occurred in the form of graphical representation.</p> <p></p> <p></p>"},{"location":"overview/web-app-userguide/#settings","title":"Settings","text":"<ol> <li>User</li> </ol> <p>Click on the User tab and fill in the details as required. Later that user can be selected while setting up the notification details. </p> <p></p> <ol> <li>Notification Group</li> </ol> <p>Click on the Notification Group tab and you can make the required changes to the created groups. </p> <p></p> <p>Click on the Create Group  button and you will be able to create a group of users. </p> <p>Enter the name of group as per the requirement.</p> <p>Click on Edit button, available besides each of the available users. Select a group name, to which the user needs to be added.</p> <p></p> <p>Click on Edit Group button, which will let you make changes to the created group of users.</p> <p>Click on Delete button, besides the Group name. The group will then be deleted permanently.</p> <p></p>"},{"location":"privacy/blur-faces/","title":"Face Blur","text":"<p>Ensure the privacy of individuals in public spaces</p> <p> </p> Face blur as part of preprocessing"},{"location":"privacy/blur-faces/#overview","title":"Overview","text":"<p>Face blurring is a privacy model which is becoming increasingly popular in the digital age. It involves the use of technology to blur or obscure the facial features of individuals in digital images and videos. This technology can be used to protect the identity of individuals in images.</p> <p>The concept of face blurring is based on the idea that a person\u2019s identity should remain private, and that images of a person should not be shared without their consent. In a world where people are increasingly sharing images and videos of themselves and others, face blurring is becoming a necessary tool to protect people\u2019s privacy. This technology can be used to blur the faces of individuals in images, or even to remove them entirely.</p> <p>The face blurring technology is designed to be easy to use and understand. It can be used on both still images and videos, and can be applied in a matter of seconds with just a few clicks. It is also fairly simple to configure and requires no technical expertise. The user simply choose the image or video that they want to blur and the algorithm will automatically detect and blur the faces.</p>"},{"location":"privacy/blur-faces/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based Model for Face Blurring is designed to ensure that the privacy of individuals is respected while still allowing the public to have access to the video feed.</p> <p>This model uses a combination of facial recognition algorithms and image processing techniques to automatically blur faces in real-time video streams. The system is designed to detect faces in real-time, and then blur them out so that they are not recognizable. This model has been used in various applications including public surveillance, online video streaming, and social media platforms.</p>"},{"location":"privacy/blur-faces/#model-details","title":"Model Details","text":""},{"location":"privacy/blur-faces/#dataset","title":"Dataset","text":"<p>WIDER FACE dataset is a face detection benchmark dataset, of which images are selected from the publicly available WIDER dataset. WIDER FACE dataset is organized based on 61 event classes. For each event class, we randomly select 40%/10%/50% data as training, validation and testing sets.  The dataset contains faces with:</p> <ul> <li>Variant illumination scene images</li> <li>Multiple face expressions</li> <li>Different lighting conditions</li> <li>Variations in scale, pose and occlusion</li> </ul>"},{"location":"privacy/blur-faces/#model","title":"Model","text":"<p>The model is based off of the YOLOv5-face algorithm. The model is trained on WIDER FACE dataset. We intend to develop a model that generalizes well in real world situations. Implemented a custom logic for face blurring with the help of face detections from yolo face.</p>"},{"location":"privacy/blur-faces/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   32,203 v1 Both(Ceiling and Straight) 95%  93%  85%  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"privacy/blur-faces/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to Ensure the privacy of individuals.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect and blur the faces identified in this camera feed.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test face-blur\n\nDownloading models for scenario: face-blur\nModel: face-blur: https://workplaceos.blob.core.windows.net/models/yolov5s-face-blur/yolov5s-face-blur-0.0.1.zip\n\n\nStarting scenario: face-blur..\n</code></pre> </li> <li> <p>You should be able to see faces being blurred as part of preprocessing.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"privacy/blur-faces/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"privacy/blur-faces/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"privacy/blur-license-plates/","title":"Licence  Plates blur","text":"<p>Ensure the privacy of vehicle owners in images and videos by blurring licence plates</p> <p> </p> Licence-plate blur as part of preprocessing"},{"location":"privacy/blur-license-plates/#overview","title":"Overview","text":"<p>Blurring license plates is a common technique used to protect the privacy of vehicle owners in images and videos. The process involves using computer vision based model to obscure the alphanumeric characters on a license plate, making them unreadable.</p> <p>Blurring license plates can be an effective way to protect the privacy of vehicle owners, particularly in situations where their vehicles may be captured on camera without their knowledge or consent. This can include surveillance footage, dashcam footage, or even photographs taken by bystanders.</p> <p>Licence plate blurring technology can be relatively easy to use and understand. The algorithm detects licence plates and blurs them. The tool can be used on images as well as on videos.</p>"},{"location":"privacy/blur-license-plates/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based model for license plate blurring can be a useful and intended application in certain contexts, such as when capturing images or video in public places or in situations where license plates may contain sensitive or identifying information. This technology can help protect the privacy and security of individuals by blurring or obscuring license plate numbers.</p> <p>The model uses a detection algorithm followed by computer vision techniques to obscure licence plates in images and videos. This model can be used for various applications including privacy protection, surveillance or investigation operations.</p> <p>Overall, license plate blurring models have a range of applications in various industries, all of which aim to protect individual privacy and prevent the misuse of sensitive information.</p>"},{"location":"privacy/blur-license-plates/#dataset","title":"Dataset","text":"<p>The datasets for this scenario consists of images and videos with licence plates. It is compiled in a manner to reflect real-world complexities. The dataset has licence plates with:</p> <ul> <li>Variations in the environment</li> <li>Different seasonal changes</li> <li>Different types of vehicles</li> <li>Different distances from the camera</li> <li>Different lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using security camera feeds</li> </ul>"},{"location":"privacy/blur-license-plates/#model","title":"Model","text":"<p>The model is based on the YOLOv5 algorithm to detect licence plates. It is trained on the curated dataset. Licence plate blurring is performed using computer vision-based blurring operations. The model is developed in a way that it generalizes well for different environments and situations.</p>"},{"location":"privacy/blur-license-plates/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   23,219 v1 Both(Ceiling and Straight) 97%  96%  98%  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"privacy/blur-license-plates/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to ensure the privacy of vehicle owners.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect and blur the licence plates identified in this camera feed.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test licence-blur\n\nDownloading models for scenario: licence-blur\nModel: licence-blur: https://workplaceos.blob.core.windows.net/models/yolov5s-licence-blur/yolov5s-licence-blur-0.0.1.zip\n\n\nStarting scenario: licence-blur..\n</code></pre> </li> <li> <p>You should be able to see faces being blurred as part of preprocessing.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"privacy/blur-license-plates/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"privacy/blur-license-plates/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"privacy/blur-screens/","title":"Blur Screens","text":"<p>Ensure the privacy of individuals by obscuring docomputer monitor and other gadget screens in camera feeds</p> <p> </p> Blur screens as part of pre-processing"},{"location":"privacy/blur-screens/#overview","title":"Overview","text":"<p>Blurring Computer Monitor screens   is a common technique used to protect the privacy of individuals in images and videos. The process involves using computer vision based model to obscure the alphanumeric characters on a computer monitor, making them unreadable. Blurring Computer Monitor screens can be an effective way to protect the privacy of individuals, particularly in situations where their screens may be captured on camera without their knowledge or consent. This can include surveillance footage, dashcam footage, or even photographs taken by bystanders.  </p>"},{"location":"privacy/blur-screens/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based model for signs/document blurring can be particularly useful when sharing documents containing personally identifiable information (PII), such as social security numbers, driver's license numbers, and financial account numbers. It can also be useful when sharing documents that contain trade secrets, confidential business information, or other sensitive data that could be used to harm individuals or organizations.</p> <p>The model uses a detection algorithm followed by computer vision techniques to obscure texts in images and videos. The model works in a way that it ensures that documents and signs are fully and effectively obscured so that it cannot be read or easily recovered by others.</p>"},{"location":"privacy/blur-screens/#model","title":"Model","text":"<p>We would be releasing the model for blurring computer monitor screens in Q2-2023.</p>"},{"location":"privacy/blur-screens/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"privacy/blur-signs/","title":"Signs/Text blur","text":"<p>Ensure the privacy of individuals by obscuring documents and other signs in camera feeds</p> <p> </p> Text blur as part of preprocessing"},{"location":"privacy/blur-signs/#overview","title":"Overview","text":"<p>Blurring documents and other signs is a common technique used to protect the privacy of individuals in images and videos. A computer vision-based model is used in the process to make the characters unreadable.</p> <p>Blurring documents can be an effective way to protect sensitive information and preserve privacy. By blurring or obscuring sensitive information on a document, you can help prevent others from accessing confidential information or using the information for malicious purposes.</p> <p>There are many use cases for document blurring, where blurring or obscuring sensitive information can help protect privacy and prevent unauthorized access to sensitive data. Here are some of them:</p> <ul> <li> <p>Redacting personal information in legal documents: Legal documents often contain sensitive information, such as social security numbers, addresses, and birth dates. By blurring or redacting this information, legal professionals can help protect their clients' privacy and prevent identity theft.</p> </li> <li> <p>Blurring confidential business information in corporate documents: Corporate documents, such as financial reports and contracts, often contain confidential business information. Blurring or redacting this information can help protect the company's intellectual property.</p> </li> <li> <p>Blurring identifying information in online images: Online images, including social media posts and blog articles, often include identifiable information. By blurring this information, content creators can help protect the privacy of individuals.</p> </li> <li> <p>Blurring confidential information in government documents: Government documents, including classified information and sensitive documents, often require blurring or redaction to prevent unauthorized access to confidential information.</p> </li> </ul>"},{"location":"privacy/blur-signs/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based model for signs/document blurring can be particularly useful when sharing documents containing personally identifiable information (PII), such as social security numbers, driver's license numbers, and financial account numbers. It can also be useful when sharing documents that contain trade secrets, confidential business information, or other sensitive data that could be used to harm individuals or organizations.</p> <p>The model uses a detection algorithm followed by computer vision techniques to obscure texts in images and videos. The model works in a way that it ensures that documents and signs are fully and effectively obscured so that it cannot be read or easily recovered by others.</p>"},{"location":"privacy/blur-signs/#dataset","title":"Dataset","text":"<p>The dataset for this scenario consists of images and videos with different types of signs and documents. It is constructed in a manner to reflect real-world intricacies. The dataset has documents with:</p> <ul> <li>Variations in the environment</li> <li>Different types of documents</li> <li>Different types of signs</li> <li>Different distances from the camera</li> <li>Different lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using security camera feeds</li> </ul>"},{"location":"privacy/blur-signs/#model","title":"Model","text":"<p>The model is based on the YOLOv5 algorithm to detect documents. Document/signs blurring is performed using computer vision-based blurring algorithms. The model is developed in a way that it generalizes well for different environments and situations.</p> <p>Currently, we have document blurring model and it is based on Yolov5.</p>"},{"location":"privacy/blur-signs/#model-card","title":"Model card","text":"Dataset size Version Precision Recall  mAP   10k v2 97%  97%  99%  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"privacy/blur-signs/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to ensure the privacy of individuals.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect and blur the documents/signs identified in this camera feed.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test text-blur\n\nDownloading models for scenario: text-blur\nModel: text-blur: https://workplaceos.blob.core.windows.net/models/yolov5s-licence-blur/yolov5s-text-blur-0.0.1.zip\n\n\nStarting scenario: text-blur..\n</code></pre> </li> <li> <p>You should be able to see faces being blurred as part of preprocessing.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p> <p>For more details visit VisionAI web application</p>"},{"location":"privacy/blur-signs/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with the GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"privacy/blur-signs/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"privacy/employee-privacy/","title":"Employee Privacy","text":"<p>Employee privacy is a concern for many companies. The use of cameras in the workplace can be a sensitive issue. Employees may feel that their privacy is being violated. They may also feel that their personal space is being invaded. This can lead to a loss of trust and confidence in the company.</p> <p>Privacy policies are put in place to protect employees from unwarranted surveillance. They also help to ensure that employees are not subjected to any form of discrimination or harassment.</p> <p>To send real-time alerts to employees, managers, and other stakeholders if there is a potential violation or deviation from established policies and procedures.</p> <p>These are a set of rules and regulations that a company creates and enforces to ensure that it operates in accordance with applicable laws, regulations, and ethical standards. The purpose of compliance policies is to help companies prevent legal and ethical violations, promote responsible conduct, and maintain their reputation and public trust. Some of these are  No pictures and no mobile phones in certain areas etc. There are many events that could trigger an alert for non-adherence to privacy policies. Here are a few use cases:</p> <ul> <li>Blur faces</li> <li>Blur signs/text</li> <li>Blur screens</li> <li>Blur license plates</li> <li>Obstructed camera view</li> </ul>"},{"location":"privacy/obstructed-camera-view/","title":"Obstructed Camera View","text":"<p>Keep your camera view clear with our obstructed camera detection model.</p> <p>Detection of obstructed camera event</p>"},{"location":"privacy/obstructed-camera-view/#overview","title":"Overview","text":"<p>The obstructed camera detection model uses computer vision techniques to analyze the camera's video feed and identify if there is any obstruction present in the camera's field of view. The model utilizes deep learning techniques to learn the features of an unobstructed camera view and detects the presence of obstructions by analyzing the changes in the image features.</p> <p>The obstructed camera detection model can operate in real-time, providing continuous monitoring of the camera's field of view. This model can detect various types of obstructions, including partial obstructions, and can provide an alert when an obstruction is detected.</p>"},{"location":"privacy/obstructed-camera-view/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>The aim of an obstructed camera detection model is to detect whether a cameras field of view is obstructed or not. This model is designed to detect various types of obstructions such as fingers, tape, post-it notes, or other physical objects that might block the camera view. The obstructed camera detection model can be useful in different settings such as surveillance systems, video conferencing, or any other applications that require a clear and unobstructed camera view.</p> <p>In summary, the obstructed camera detection model is a computer vision-based algorithm that uses deep learning techniques to detect obstructions in a camera's field of view. This model can be useful in various settings, providing real-time monitoring of camera views and alerting when obstructions are present.</p>"},{"location":"privacy/obstructed-camera-view/#model-details","title":"Model Details","text":""},{"location":"privacy/obstructed-camera-view/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from various sources. </p>"},{"location":"privacy/obstructed-camera-view/#model","title":"Model","text":"<p>The model to detect obstructed camera event is in progress and it will be released soon. </p>"},{"location":"privacy/obstructed-camera-view/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li> <p>We use existing camera feeds from the premises to monitor whether the camera view is obstructed or not. We detect partial obstructions that may only partially block the camera view, such as fingers partially covering the lens. </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</p> </li> <li> <p>When an instance of obstructed camera is detected, an alert will be raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test obstructed-camera-detection\n\nDownloading models for scenario: obstructed-camera-detection\nModel: obstructed-camera-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-obstructed-camera-detection/yolov5s-obstructed-camera-detection-0.0.1.zip\n\n\nStarting scenario: obstructed-camera-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of obstructed cameras.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"privacy/obstructed-camera-view/#features","title":"Features","text":"<p>Here are the features of the obstructed camera detection model:</p> <ul> <li> <p>Real-time monitoring: The model can monitor the camera's field of view in real-time, providing immediate detection of any obstructions.</p> </li> <li> <p>Detection of various obstructions: The model can detect various types of obstructions such as fingers, tape, post-it notes, or other physical objects that might block the camera view.</p> </li> <li> <p>High accuracy: The model uses deep learning techniques and can achieve high accuracy in detecting obstructions.</p> </li> <li> <p>Easy integration: The model can be integrated with different applications such as video conferencing, surveillance systems, or any other system that requires an unobstructed camera view.</p> </li> </ul>"},{"location":"privacy/obstructed-camera-view/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"privacy/obstructed-camera-view/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"reference/changelog/","title":"Changelog","text":""},{"location":"reference/changelog/#visionai-changelog","title":"VisionAI Changelog","text":""},{"location":"reference/changelog/#020-february-14-2023","title":"0.2.0  February 14, 2023","text":"<ul> <li>\ud83d\udc9a Migrate all documentation to public site.</li> <li>\ud83c\udfa8 Added documentation for difference scenarios.</li> <li>\ud83d\ude9a Support for occupancy monitoring scenario.</li> <li>\ud83d\udd25 Support for better smoke-and-fire detection scenario.</li> </ul>"},{"location":"reference/changelog/#0118-february-14-2023","title":"0.1.18  February 14, 2023","text":"<ul> <li>\ud83d\udc9a Added support for grafana and redis servers.</li> <li>\ud83c\udfa8 Added support for event engine, and publishing to redis</li> <li>\ud83d\ude9a Added commands for <code>visionai init|status|stop</code> which can install all dependencies.</li> <li>\ud83d\udd25 Removed dependency on torch and OpenCV packages. Now the package size goes down significantly.</li> <li>\ud83d\udcdd Updated documentation to reflect the changes.</li> <li>\u2728 Docker networking changes - now all containers connect to bridge network.</li> <li>\ud83d\udd25 Added support for slip-and-fall detection model.</li> <li>\ud83d\udd25 Added support for phone detection and people taking pictures scenarios.</li> </ul>"},{"location":"reference/changelog/#0117-february-9-2023","title":"0.1.17 February 9, 2023","text":"<ul> <li>\ud83d\udccc Added support for <code>visionai web start|stop|status</code> commands with API server support.</li> <li>Ensure We can pull visionify/visionai-api to local machine</li> <li>Run this as a container with model-repo/ and config/ folder shared.</li> <li>Ensure back-to-back stop/start would work.</li> <li>Ensure we can just do <code>web start</code> without doing <code>web install</code></li> <li>Removed <code>web install</code> as it can cause confusion</li> </ul>"},{"location":"reference/changelog/#0116-february-8-2023","title":"0.1.16 February 8, 2023","text":"<ul> <li>\u2728 Support for <code>visionai web start|stop|status</code> commands.</li> <li>\ud83c\udfa8 Pull latest images from dockerhub before starting web server.</li> <li>\ud83d\ude9a Support for alias for all commands (like <code>visionai camera add</code> and <code>visionai cameras add</code>)</li> <li>\ud83d\udd25 Add support for <code>face-blur</code> scenario. You can test it with <code>visionai scenario test face-blur</code> now.</li> <li>\ud83d\udcdd Tested support for Ubuntu (with NVIDIA graphics card), MacOS, and Windows 10.</li> </ul>"},{"location":"reference/changelog/#0115-february-7-2023","title":"0.1.15 February 7, 2023","text":"<ul> <li>\ud83d\udc1b On linux we were using incorrect nvidia_smi package.</li> <li>\ud83c\udfa8 Add support for common spelling errors during commands (like scenarios instead of scenario)</li> <li>\ud83d\ude9a Move scenario.json file to this repo - so everything is in one place.</li> </ul>"},{"location":"reference/changelog/#0114-february-3-2023","title":"0.1.14 February 3, 2023","text":"<ul> <li>\u2728 Support for <code>visionai scenario test</code> command.</li> <li>\u2728 Support for Triton server running on MacOS (tested)</li> <li>\ud83d\udd25 Simplified scenario command names (don't have to specify --name anymore)</li> <li>\ud83d\udcdd Renamed all cli files to _app - to avoid confusion between models.py &amp; models/ module.</li> <li>\ud83d\udcdd Move add-scenario and remove-scenario to camera module (these are camera operations.)</li> <li>\ud83d\udd25 Show nice progress bar while any docker image is being pulled.</li> <li>\ud83e\uddea Added results.show() method to detection that uses cv2.imshow() to show the results locally.</li> </ul>"},{"location":"reference/changelog/#0112-january-31-2023","title":"0.1.12 January 31, 2023","text":"<ul> <li>\u2728 Support for managing triton server</li> <li>\ud83c\udfa8 Start/stop triton server from CLI.</li> <li>\ud83d\udcdd Get/print models status coming from triton.</li> <li>\ud83d\udd25 Implemented pretty printing through rich library for models</li> <li>\ud83e\uddea CI Tests to test both before &amp; after package creation</li> <li>\ud83d\udc1b Fix versioning bug (that broke the previous version)</li> </ul>"},{"location":"reference/changelog/#0111-january-27-2023","title":"0.1.11 January 27, 2023","text":"<ul> <li>Support for Triton models (through http/grpc)</li> <li>Implemented yolov5 backend for triton</li> <li>Implemented Autoshape wrapper for NMS &amp; scaling</li> <li>Added easy test case for reproducing.</li> <li>Updated schema for models, fix test cases for it.</li> </ul>"},{"location":"reference/changelog/#0110-january-25-2023","title":"0.1.10 January 25, 2023","text":"<ul> <li>Implemented download models for scenarios</li> <li>Added cv2, torch, numpy dependencies for inference</li> <li>Added support for <code>--version</code> &amp; <code>--verbose</code> options to cli</li> <li>CLI Test cases to use <code>python -m visionai</code> to replicate user behavior</li> </ul>"},{"location":"reference/changelog/#017-january-24-2023","title":"0.1.7 January 24, 2023","text":"<ul> <li>Implemented scenarios functionality</li> <li>Docker compose integration</li> <li>Makefile integration</li> </ul>"},{"location":"reference/changelog/#017-january-22-2023","title":"0.1.7 January 22, 2023","text":"<ul> <li>Implemented camera add/delete functionality</li> </ul>"},{"location":"reference/changelog/#016-january-20-2023","title":"0.1.6 January 20, 2023","text":"<ul> <li>Implemented initial set of commands in different files (dummy implementation)</li> <li>Testing commands individually or through the main application</li> </ul>"},{"location":"reference/changelog/#013-january-16-2023","title":"0.1.3 January 16, 2023","text":"<ul> <li>Basic overview and usage documentation is updated.</li> <li>Started using a termy JS script to show terminal animations nicely</li> </ul>"},{"location":"reference/changelog/#012-january-14-20123","title":"0.1.2 January 14, 20123","text":"<ul> <li>Made MkDocs documents based on Typer format</li> <li>Registered CNAME to point to https://docs.visionify.ai</li> </ul>"},{"location":"reference/changelog/#011-january-11-2023","title":"0.1.1 January 11, 2023","text":"<ul> <li>Updated Azure DevOps CI/CD to automatically publish package on each merge</li> <li>Initial set of commands for visionai application</li> <li>Made <code>visionai</code> as a callable CLI application through poetry</li> </ul>"},{"location":"reference/changelog/#010-january-10-2023","title":"0.1.0 January 10, 2023","text":"<ul> <li>Initial release: <code>pip install visionai</code></li> <li>Pushed package to <code>PyPI</code> repository</li> </ul>"},{"location":"reference/notes/","title":"Notes","text":""},{"location":"reference/notes/#definition-list","title":"Definition List","text":"<code>Lorem ipsum dolor sit amet</code> <p>Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis.</p> <code>Cras arcu libero</code> <p>Aliquam metus eros, pretium sed nulla venenatis, faucibus auctor ex. Proin ut eros sed sapien ullamcorper consequat. Nunc ligula ante.</p> <p>Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor.</p>"},{"location":"reference/notes/#partially-completed-list","title":"Partially completed list","text":"<ul> <li> Lorem ipsum dolor sit amet, consectetur adipiscing elit</li> <li> Vestibulum convallis sit amet nisi a tincidunt<ul> <li> In hac habitasse platea dictumst</li> <li> In scelerisque nibh non dolor mollis congue sed et metus</li> <li> Praesent sed risus massa</li> </ul> </li> <li> Aenean pretium efficitur erat, donec pharetra, ligula non scelerisque</li> </ul>"},{"location":"reference/notes/#tables","title":"Tables","text":"<p>Suite   Scenarios   Supported events Hazard Warnings Smoke and Fire Detection    \"Smoke event detected Fire event detected Sparks detected Open flames detection\"     No smoking/no-vaping zones  \"Smoking event detected Vaping event detected\"     Spills &amp; Leaks detection (Liquids)  \"Water puddle detected Water leak from equipment detected Spill event detected Slippery sign detected\"     Gas leak detection  Gas leak detected     Missing fire-extinguisher   Missing fire extinguisher     Blocked exit monitoring Blocked exit detected     Equipment temperature   \"Temperature exceeds limit Temperature subceeds limit\"     Slip/trip and fall detection    Blocker on pathway detected.     Equipment rust-and-corrosion    Rust or corrosion event detected</p>"},{"location":"reference/notes/#hazard-warnings","title":"Hazard Warnings","text":"Scenario name Supported Events <code>smoke-and-fire-detection</code> \u2705 Fire event detected  Smoke Event Detected  Sparks Detected  Open Flames Detected <code>no-smoking-no-vaping-zones</code> \u2705 Smoking event detected  Vaping event detected <code>smoke-and-fire-detection</code> \u2705 Fire event detected  Smoke Event Detected  Sparks Detected  Open Flames Detected <p>Smoke and Fire Detection   No smoking/no-vaping zones   Spills &amp; Leaks detection (Liquids)   Gas leak detection   Missing fire-extinguisher   Blocked exit monitoring   Equipment temperature   Slip/trip and fall detection   Equipment rust-and-corrosion</p>"},{"location":"reference/notes/#callout","title":"Callout","text":"<p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Abstract</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Warning</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Danger</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Success</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Question</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Tip</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Quote</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>VisionAI Comment</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"reference/notes/#images","title":"Images","text":"Image caption Image caption"},{"location":"reference/notes/#side-by-side-code-blocks","title":"Side-by-side code-blocks","text":"Material for MkDocsInsiders <pre><code>name: ci # (1)!\non:\n  push:\n    branches:\n      - master # (2)!\n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.x\n      - uses: actions/cache@v2\n        with:\n          key: ${{ github.ref }}\n          path: .cache\n      - run: pip install mkdocs-material # (3)!\n      - run: mkdocs gh-deploy --force\n</code></pre> <ol> <li> <p>You can change the name to your liking.</p> </li> <li> <p>At some point, GitHub renamed <code>master</code> to <code>main</code>. If your default branch     is named <code>master</code>, you can safely remove <code>main</code>, vice versa.</p> </li> <li> <p>This is the place to install further [MkDocs plugins] or Markdown     extensions with <code>pip</code> to be used during the build:</p> <pre><code>pip install \\\n  mkdocs-material \\\n  mkdocs-awesome-pages-plugin \\\n  ...\n</code></pre> </li> </ol> <pre><code>name: ci\non:\n  push:\n    branches:\n      - master\n      - main\npermissions:\n  contents: write\njobs:\n  deploy:\n    runs-on: ubuntu-latest\n    if: github.event.repository.fork == false\n    steps:\n      - uses: actions/checkout@v3\n      - uses: actions/setup-python@v4\n        with:\n          python-version: 3.x\n      - uses: actions/cache@v2\n        with:\n          key: ${{ github.ref }}\n          path: .cache\n      - run: apt-get install pngquant # (1)!\n      - run: pip install git+https://${GH_TOKEN}@github.com/squidfunk/mkdocs-material-insiders.git\n      - run: mkdocs gh-deploy --force\nenv:\n  GH_TOKEN: ${{ secrets.GH_TOKEN }} # (2)!\n</code></pre>"},{"location":"reference/notes/#code-blocks-annotations","title":"Code blocks annotations","text":"<p>Material for MkDocs is published as a [Python package] and can be installed with <code>pip</code>, ideally by using a [virtual environment]. Open up a terminal and install Material for MkDocs with:</p> <pre><code>theme:\n  features:\n    - content.code.annotate # (1)\n</code></pre> <ol> <li>\ud83d\udc77 I'm a code annotation! I can contain <code>code</code>, formatted     text, images, ... basically anything that can be written in Markdown.</li> </ol>"},{"location":"reference/notes/#versiontags-flags","title":"Version/tags flags","text":"<p>[ 9.0.0][Code copy button support] \u00b7  Feature flag</p>"},{"location":"reference/notes/#adding-a-title-to-a-code-block","title":"Adding a title to a code-block","text":"bubble_sort.py<pre><code>def bubble_sort(items):\n    for i in range(len(items)):\n        for j in range(len(items) - 1 - i):\n            if items[j] &gt; items[j + 1]:\n                items[j], items[j + 1] = items[j + 1], items[j]\n</code></pre>"},{"location":"reference/notes/#hover-important-information","title":"Hover, Important information","text":"<p>TODO:</p> Icon with tooltip<pre><code> {title=\"Important information\" }\n</code></pre> <p></p> <p>hello this is important.</p> <p></p>"},{"location":"reference/notes/#adding-footnotes","title":"Adding footnotes","text":"<p>This is how you add footnotes.</p>"},{"location":"reference/notes/#add-buttons","title":"Add buttons","text":"<p>Subscribe to our newsletter</p> <p>another button</p> <p>Subscribe to our newsletter</p>"},{"location":"reference/notes/#data-tables","title":"Data tables","text":"This is table 1<pre><code>| Method      | Description                          |\n| ----------- | ------------------------------------ |\n| `GET`       | :material-check:     Fetch resource  |\n| `PUT`       | :material-check-all: Update resource |\n| `DELETE`    | :material-close:     Delete resource |\n</code></pre> Method Description <code>GET</code>      Fetch resource <code>PUT</code>  Update resource <code>DELETE</code>      Delete resource"},{"location":"reference/notes/#text-highlighting","title":"Text highlighting","text":"<ul> <li>This was marked</li> <li>This was inserted</li> <li>This was deleted</li> </ul>"},{"location":"reference/notes/#text-block-with-title","title":"Text block with title","text":"<p>Emoji<pre><code>Here you can add anything..\n</code></pre> </p>"},{"location":"reference/notes/#mermaid","title":"Mermaid","text":""},{"location":"reference/notes/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>sequenceDiagram\n    actor       user     as User\n    participant p   as Button\n\n    user     -&gt;&gt;  p: click</code></pre>"},{"location":"reference/notes/#flowchart","title":"Flowchart","text":"<pre><code>graph TD\n    A[Hard edge] --&gt;|Link text| B(Round edge)\n    B --&gt; C{Decision}\n    C --&gt;|One| D[Result one]\n    C --&gt;|Two| E[Result two]</code></pre>"},{"location":"reference/notes/#another-chart","title":"Another chart","text":"<pre><code>flowchart TD\n    Start --&gt;|Decision 1| A\n    A --&gt;|Yes| B\n    A --&gt;|No| C\n    B --&gt;|Action 1| End\n    C --&gt;|Decision 2| D\n    D --&gt;|Yes| End\n    D --&gt;|No| A</code></pre>"},{"location":"reference/notes/#pi-chart","title":"Pi chart","text":"<pre><code>pie title Accuracy\n    \"Dogs\" : 386\n    \"Cats\" : 85\n    \"Rats\" : 15</code></pre>"},{"location":"reference/notes/#user-journey","title":"User Journey","text":"<pre><code>---\ntitle: My working day\n---\njourney\n    section Go to work\n      Make tea: 5: Me\n      Go upstairs: 3: Me\n      Do work: 1: Me, Cat\n    section Go home\n      Go downstairs: 5: Me\n      Sit down: 5: Me</code></pre>"},{"location":"reference/notes/#class-diagram","title":"Class Diagram","text":"<pre><code>---\ntitle: Animal example\n---\nclassDiagram\n    note \"From Duck till Zebra\"\n    Animal &lt;|-- Duck\n    note for Duck \"can fly\\ncan swim\\ncan dive\\ncan help in debugging\"\n    Animal &lt;|-- Fish\n    Animal &lt;|-- Zebra\n    Animal : +int age\n    Animal : +String gender\n    Animal: +isMammal()\n    Animal: +mate()\n    class Duck{\n        +String beakColor\n        +swim()\n        +quack()\n    }\n    class Fish{\n        -int sizeInFeet\n        -canEat()\n    }\n    class Zebra{\n        +bool is_wild\n        +run()\n    }</code></pre>"},{"location":"reference/notes/#sequence-diagram_1","title":"Sequence Diagram","text":"<pre><code>---\ntitle: Sequence Diagram Example\n---\nsequenceDiagram\n    Consumer--&gt;API: Book something\n    API--&gt;BookingService: Start booking process\n    break when the booking process fails\n        API--&gt;Consumer: show failure\n    end\n    API--&gt;BillingService: Start billing process\n</code></pre> <ol> <li> <p>This is a footnote.\u00a0\u21a9</p> </li> <li> <p>This is another footnote.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/reference/","title":"<code>visionai</code>","text":"<p>VisionAI Toolkit</p> <p>VisionAI tookit provides a large number of ready-to-deploy scenarios built using latest computer vision frameworks. Supports many of the common workplace health and safety use-cases.</p> <p>Start by exploring scenarios through visionai scenario list command. After that, you can create a pipeline through the pipeline commands. Once a pipeline is configured, you can run the pipeline on the any number of cameras.</p> <p>Running the toolkit does assume a NVIDIA GPU powered machine for efficient performance. Please see the system requirements on the documentation.</p> <p>You can instead opt to install it through Azure Managed VM, with preconfigured machines &amp; recommended hardware support. You can find information about this on our documentation website.</p> <p>Visit https://docs.visionify.ai for more details.</p> <p>Usage:</p> <pre><code>$ visionai [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--verbose</code>: [default: False]</li> <li><code>--version</code>: [default: False]</li> <li><code>--install-completion</code>: Install completion for the current shell.</li> <li><code>--show-completion</code>: Show completion for the current shell, to copy it or customize the installation.</li> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <pre><code>\u256d\u2500 Commands \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256e\n\u2502 auth         Authentication commands                           \u2502\n\u2502 camera       Add/remove/manage cameras                         \u2502\n\u2502 device       Device commands                                   \u2502\n\u2502 init         Initialize VisionAI library                       \u2502\n\u2502 model        Manage models                                     \u2502\n\u2502 pipeline     Manage pipelines                                  \u2502\n\u2502 scenario     Add/remove scenarios to camera                    \u2502\n\u2502 status       Print status of all running containers.           \u2502\n\u2502 stop         Stop all running containers.                      \u2502\n\u2502 web          Start/stop web server                             \u2502\n\u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre>"},{"location":"reference/reference/#visionai-auth","title":"<code>visionai auth</code>","text":"<p>Authorization (logging in/out)</p> <p>Login and get authorization token etc.</p> <p>You can login/logout check authorization token with this.</p> <p>Usage:</p> <pre><code>$ visionai auth [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>login</code>: Login with an application token.</li> <li><code>logout</code>: Logout from your session Get the auth token...</li> <li><code>status</code>: Check login status Check the current login...</li> </ul>"},{"location":"reference/reference/#visionai-auth-login","title":"<code>visionai auth login</code>","text":"<p>Login with an application token.</p> <p>Get the auth token from our website</p> <p>Usage:</p> <pre><code>$ visionai auth login [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--token TEXT</code>: Authenticate the app through token  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-auth-logout","title":"<code>visionai auth logout</code>","text":"<p>Logout from your session</p> <p>Get the auth token from our website</p> <p>Usage:</p> <pre><code>$ visionai auth logout [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-auth-status","title":"<code>visionai auth status</code>","text":"<p>Check login status</p> <p>Check the current login system.</p> <p>Usage:</p> <pre><code>$ visionai auth status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera","title":"<code>visionai camera</code>","text":"<p>Manage cameras</p> <p>An organization can have multiple cameras that are installed at different places. They may be from different vendors and/or maybe using different security surveillance software. Most cameras however do support RTSP, RTMP or HLS streams as an output. Please refer to your camera vendor documentation to find this out. This module will help you onboard those cameras on visionai systems by using a simple named instance for each camera.</p> <p>Usage:</p> <pre><code>$ visionai camera [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>add</code>: Add a named camera instance Add a camera as a...</li> <li><code>add-scenario</code>: Add a scenario for a camera Add an individual...</li> <li><code>list</code>: List available cameras Print cameras...</li> <li><code>list-scenario</code>: List scenarios configured for a camera</li> <li><code>list-scenarios</code>: List scenarios configured for a camera</li> <li><code>preview</code>: Preview the camera system View the camera...</li> <li><code>remove</code>: Remove a camera from the system Specify a...</li> <li><code>remove-scenario</code>: Remove a scenario from a camera Specify a...</li> <li><code>reset</code>: Reset all camera configuration.</li> </ul>"},{"location":"reference/reference/#visionai-camera-add","title":"<code>visionai camera add</code>","text":"<p>Add a named camera instance</p> <p>Add a camera as a named instance in the system. For adding a camera we support RTSP, HLS, HTTP(S) systems. To add a camera you need to provide a name for the camera, URI for the camera (including any username/password within the URI itself), description for camera (about its location, where its pointing, who is the vendor etc.).</p> <p>Before the camera is added - we need to test out if the camera instance is valid. We need to be able to read from the camera and calculate its FPS. Show this information on the screen.</p> <p>Usage:</p> <pre><code>$ visionai camera add [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: Camera Name  [required]</li> <li><code>--uri TEXT</code>: URI for camera  [required]</li> <li><code>--description TEXT</code>: Description  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-add-scenario","title":"<code>visionai camera add-scenario</code>","text":"<p>Add a scenario for a camera</p> <p>Add an individual scenario to be run for a camera. Specify the names for scenario and camera.</p> <p>Usage:</p> <pre><code>$ visionai camera add-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: camera name  [required]</li> <li><code>--scenario TEXT</code>: scenario name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-list","title":"<code>visionai camera list</code>","text":"<p>List available cameras</p> <p>Print cameras available in the system and the scenarios / routines that are set up for them.</p> <p>Usage:</p> <pre><code>$ visionai camera list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-list-scenario","title":"<code>visionai camera list-scenario</code>","text":"<p>List scenarios configured for a camera</p> <p>Usage:</p> <pre><code>$ visionai camera list-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name  [default: ]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-list-scenarios","title":"<code>visionai camera list-scenarios</code>","text":"<p>List scenarios configured for a camera</p> <p>Usage:</p> <pre><code>$ visionai camera list-scenarios [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name  [default: ]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-preview","title":"<code>visionai camera preview</code>","text":"<p>Preview the camera system</p> <p>View the camera feed, review FPS etc available for camera.</p> <p>Usage:</p> <pre><code>$ visionai camera preview [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: camera name to preview  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-remove","title":"<code>visionai camera remove</code>","text":"<p>Remove a camera from the system</p> <p>Specify a named camera that needs to be removed from the system. Once removed, all the scenarios and pre-process routines associated with the camera will be removed.</p> <p>Usage:</p> <pre><code>$ visionai camera remove [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: [default: Camera name]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-remove-scenario","title":"<code>visionai camera remove-scenario</code>","text":"<p>Remove a scenario from a camera</p> <p>Specify a named scenario that needs to be removed from the system. Once removed, all the scenarios and pre-process routines associated with the scenario will be removed.</p> <p>Usage:</p> <pre><code>$ visionai camera remove-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: camera  [required]</li> <li><code>--scenario TEXT</code>: scenario name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-camera-reset","title":"<code>visionai camera reset</code>","text":"<p>Reset all camera configuration.</p> <p>All cameras and their scenarios would be removed from the system. Any earlier configuration is backed up as a timed json backup file.</p> <p>Usage:</p> <pre><code>$ visionai camera reset [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--confirm / --no-confirm</code>: Confirm delete  [default: False]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras","title":"<code>visionai cameras</code>","text":"<p>... alias for camera</p> <p>Usage:</p> <pre><code>$ visionai cameras [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>add</code>: Add a named camera instance Add a camera as a...</li> <li><code>add-scenario</code>: Add a scenario for a camera Add an individual...</li> <li><code>list</code>: List available cameras Print cameras...</li> <li><code>list-scenario</code>: List scenarios configured for a camera</li> <li><code>list-scenarios</code>: List scenarios configured for a camera</li> <li><code>preview</code>: Preview the camera system View the camera...</li> <li><code>remove</code>: Remove a camera from the system Specify a...</li> <li><code>remove-scenario</code>: Remove a scenario from a camera Specify a...</li> <li><code>reset</code>: Reset all camera configuration.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-add","title":"<code>visionai cameras add</code>","text":"<p>Add a named camera instance</p> <p>Add a camera as a named instance in the system. For adding a camera we support RTSP, HLS, HTTP(S) systems. To add a camera you need to provide a name for the camera, URI for the camera (including any username/password within the URI itself), description for camera (about its location, where its pointing, who is the vendor etc.).</p> <p>Before the camera is added - we need to test out if the camera instance is valid. We need to be able to read from the camera and calculate its FPS. Show this information on the screen.</p> <p>Usage:</p> <pre><code>$ visionai cameras add [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: Camera Name  [required]</li> <li><code>--uri TEXT</code>: URI for camera  [required]</li> <li><code>--description TEXT</code>: Description  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-add-scenario","title":"<code>visionai cameras add-scenario</code>","text":"<p>Add a scenario for a camera</p> <p>Add an individual scenario to be run for a camera. Specify the names for scenario and camera.</p> <p>Usage:</p> <pre><code>$ visionai cameras add-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: camera name  [required]</li> <li><code>--scenario TEXT</code>: scenario name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-list","title":"<code>visionai cameras list</code>","text":"<p>List available cameras</p> <p>Print cameras available in the system and the scenarios / routines that are set up for them.</p> <p>Usage:</p> <pre><code>$ visionai cameras list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-list-scenario","title":"<code>visionai cameras list-scenario</code>","text":"<p>List scenarios configured for a camera</p> <p>Usage:</p> <pre><code>$ visionai cameras list-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name  [default: ]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-list-scenarios","title":"<code>visionai cameras list-scenarios</code>","text":"<p>List scenarios configured for a camera</p> <p>Usage:</p> <pre><code>$ visionai cameras list-scenarios [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name  [default: ]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-preview","title":"<code>visionai cameras preview</code>","text":"<p>Preview the camera system</p> <p>View the camera feed, review FPS etc available for camera.</p> <p>Usage:</p> <pre><code>$ visionai cameras preview [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: camera name to preview  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-remove","title":"<code>visionai cameras remove</code>","text":"<p>Remove a camera from the system</p> <p>Specify a named camera that needs to be removed from the system. Once removed, all the scenarios and pre-process routines associated with the camera will be removed.</p> <p>Usage:</p> <pre><code>$ visionai cameras remove [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: [default: Camera name]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-remove-scenario","title":"<code>visionai cameras remove-scenario</code>","text":"<p>Remove a scenario from a camera</p> <p>Specify a named scenario that needs to be removed from the system. Once removed, all the scenarios and pre-process routines associated with the scenario will be removed.</p> <p>Usage:</p> <pre><code>$ visionai cameras remove-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: camera  [required]</li> <li><code>--scenario TEXT</code>: scenario name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-cameras-reset","title":"<code>visionai cameras reset</code>","text":"<p>Reset all camera configuration.</p> <p>All cameras and their scenarios would be removed from the system. Any earlier configuration is backed up as a timed json backup file.</p> <p>Usage:</p> <pre><code>$ visionai cameras reset [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--confirm / --no-confirm</code>: Confirm delete  [default: False]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-device","title":"<code>visionai device</code>","text":"<p>Manage device features</p> <p>Since scenarios run on individual edge-devices, and we don't have enough control over the CPU, Memory, GPU statistics - it is imperative that we have strong methods for validating if a scenario can run on a chosen platform. This module provides many utilities to check CPU, Memory and GPU statistics for the edge device. We also provide an Azure Managed service where these scenarios can be configured and run on your premise on pre-validated VM machines.</p> <p>Usage:</p> <pre><code>$ visionai device [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list</code>: List available devices Get a list of all...</li> <li><code>modules</code>: List running modules on the device Again this...</li> <li><code>select</code>: Select a device Not sure why is this needed...</li> <li><code>stats</code>: Machine health (GPU/Mem stats) Show machine...</li> </ul>"},{"location":"reference/reference/#visionai-device-list","title":"<code>visionai device list</code>","text":"<p>List available devices</p> <p>Get a list of all available [processing] devices</p> <p>Usage:</p> <pre><code>$ visionai device list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-device-modules","title":"<code>visionai device modules</code>","text":"<p>List running modules on the device</p> <p>Again this does not make much sense at this time. Let's revisit.</p> <p>Usage:</p> <pre><code>$ visionai device modules [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-device-select","title":"<code>visionai device select</code>","text":"<p>Select a device</p> <p>Not sure why is this needed at this time.</p> <p>Usage:</p> <pre><code>$ visionai device select [OPTIONS] DEVICE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DEVICE</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-device-stats","title":"<code>visionai device stats</code>","text":"<p>Machine health (GPU/Mem stats)</p> <p>Show machine health (GPU/memory stats). This can be used to determine if more scenarios can be run on the machine or not.</p> <p>Usage:</p> <pre><code>$ visionai device stats [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-devices","title":"<code>visionai devices</code>","text":"<p>... alias for device</p> <p>Usage:</p> <pre><code>$ visionai devices [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>list</code>: List available devices Get a list of all...</li> <li><code>modules</code>: List running modules on the device Again this...</li> <li><code>select</code>: Select a device Not sure why is this needed...</li> <li><code>stats</code>: Machine health (GPU/Mem stats) Show machine...</li> </ul>"},{"location":"reference/reference/#visionai-devices-list","title":"<code>visionai devices list</code>","text":"<p>List available devices</p> <p>Get a list of all available [processing] devices</p> <p>Usage:</p> <pre><code>$ visionai devices list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-devices-modules","title":"<code>visionai devices modules</code>","text":"<p>List running modules on the device</p> <p>Again this does not make much sense at this time. Let's revisit.</p> <p>Usage:</p> <pre><code>$ visionai devices modules [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-devices-select","title":"<code>visionai devices select</code>","text":"<p>Select a device</p> <p>Not sure why is this needed at this time.</p> <p>Usage:</p> <pre><code>$ visionai devices select [OPTIONS] DEVICE\n</code></pre> <p>Arguments:</p> <ul> <li><code>DEVICE</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-devices-stats","title":"<code>visionai devices stats</code>","text":"<p>Machine health (GPU/Mem stats)</p> <p>Show machine health (GPU/memory stats). This can be used to determine if more scenarios can be run on the machine or not.</p> <p>Usage:</p> <pre><code>$ visionai devices stats [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model","title":"<code>visionai model</code>","text":"<p>Serve models</p> <p>Before we can run any scenarios - the models necessary for them must be ready. We use Triton inference server to make the best use of GPU/CPU resources available on the machine in order to serve our models. Any models that are available in models-repo folder would be served after this (TODO - only serve models configured in scenarios).</p> <p>Usage:</p> <pre><code>$ visionai model [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>check</code>: Check model-server status &amp; print helpful...</li> <li><code>serve</code>: Start serving all available models.</li> <li><code>start</code>: Start serving all available models.</li> <li><code>status</code>: Show the status of serving models Shows how...</li> <li><code>stop</code>: Stop serving all models.</li> </ul>"},{"location":"reference/reference/#visionai-model-check","title":"<code>visionai model check</code>","text":"<p>Check model-server status &amp; print helpful debug info.</p> <p>TODO: Goal of the check command is to identify any configuration/dependency issues that we can inform to user that he can fix on his end. This could be like missing dependency, missing software package, missing driver details etc.</p> <ul> <li>Check if model-server is running or not.</li> <li>Check if triton-client can access model-server</li> <li>Check what are the models served</li> <li>Print all of this in a pretty manner [checkbox based]</li> <li>Check container logs &amp; show them here.</li> <li>grep container logs for common errors &amp; highlight that in output</li> </ul> <p>Usage:</p> <pre><code>$ visionai model check [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model-serve","title":"<code>visionai model serve</code>","text":"<p>Start serving all available models.</p> <p>All models present in the models-repo/ will be served. We use triton inference server to serve them. The triton server will be at http://localhost:8000, grpc://localhost:8001.</p> <p>Please make sure these two ports are not used by anyone else.</p> <p>Usage:</p> <pre><code>$ visionai model serve [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model-start","title":"<code>visionai model start</code>","text":"<p>Start serving all available models.</p> <p>All models present in the models-repo/ will be served. We use triton inference server to serve them. The triton server will be at http://localhost:8000, grpc://localhost:8001.</p> <p>Please make sure these two ports are not used by anyone else.</p> <p>Usage:</p> <pre><code>$ visionai model start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model-status","title":"<code>visionai model status</code>","text":"<p>Show the status of serving models</p> <p>Shows how many models are being served, metrics for the models etc.</p> <p>Usage:</p> <pre><code>$ visionai model status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-model-stop","title":"<code>visionai model stop</code>","text":"<p>Stop serving all models.</p> <p>This method will stop serving all models. Any inference running will all be stopped as well.</p> <p>Usage:</p> <pre><code>$ visionai model stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models","title":"<code>visionai models</code>","text":"<p>... alias for model</p> <p>Usage:</p> <pre><code>$ visionai models [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>check</code>: Check model-server status &amp; print helpful...</li> <li><code>serve</code>: Start serving all available models.</li> <li><code>start</code>: Start serving all available models.</li> <li><code>status</code>: Show the status of serving models Shows how...</li> <li><code>stop</code>: Stop serving all models.</li> </ul>"},{"location":"reference/reference/#visionai-models-check","title":"<code>visionai models check</code>","text":"<p>Check model-server status &amp; print helpful debug info.</p> <p>TODO: Goal of the check command is to identify any configuration/dependency issues that we can inform to user that he can fix on his end. This could be like missing dependency, missing software package, missing driver details etc.</p> <ul> <li>Check if model-server is running or not.</li> <li>Check if triton-client can access model-server</li> <li>Check what are the models served</li> <li>Print all of this in a pretty manner [checkbox based]</li> <li>Check container logs &amp; show them here.</li> <li>grep container logs for common errors &amp; highlight that in output</li> </ul> <p>Usage:</p> <pre><code>$ visionai models check [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models-serve","title":"<code>visionai models serve</code>","text":"<p>Start serving all available models.</p> <p>All models present in the models-repo/ will be served. We use triton inference server to serve them. The triton server will be at http://localhost:8000, grpc://localhost:8001.</p> <p>Please make sure these two ports are not used by anyone else.</p> <p>Usage:</p> <pre><code>$ visionai models serve [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models-start","title":"<code>visionai models start</code>","text":"<p>Start serving all available models.</p> <p>All models present in the models-repo/ will be served. We use triton inference server to serve them. The triton server will be at http://localhost:8000, grpc://localhost:8001.</p> <p>Please make sure these two ports are not used by anyone else.</p> <p>Usage:</p> <pre><code>$ visionai models start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models-status","title":"<code>visionai models status</code>","text":"<p>Show the status of serving models</p> <p>Shows how many models are being served, metrics for the models etc.</p> <p>Usage:</p> <pre><code>$ visionai models status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-models-stop","title":"<code>visionai models stop</code>","text":"<p>Stop serving all models.</p> <p>This method will stop serving all models. Any inference running will all be stopped as well.</p> <p>Usage:</p> <pre><code>$ visionai models stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline","title":"<code>visionai pipeline</code>","text":"<p>Manage pipelines</p> <p>Pipeline is a sequence of preprocess routines and scenarios to be run on a given set of cameras. Each pipeline can be configured to run specific scenarios - each scenario with their own customizations for event notifications. This module provides robust methods for managing pipelines, showing their details, adding/remove cameras from pipelines and running a pipeline.</p> <p>Usage:</p> <pre><code>$ visionai pipeline [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>add-camera</code>: Add a camera to a pipeline Each pipeline...</li> <li><code>add-preprocess</code>: Add a preprocess routine to a pipeline...</li> <li><code>add-scenario</code>: Add a scenario to a pipeline The order of the...</li> <li><code>create</code>: Create a named pipeline Create a named...</li> <li><code>remove-camera</code>: Remove a camera from a pipeline This method...</li> <li><code>reset</code>: Reset the pipeline to original state.</li> <li><code>run</code>: Run a pipeline of scenarios on given cameras...</li> <li><code>show</code>: Show details of a pipeline Show what is...</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-add-camera","title":"<code>visionai pipeline add-camera</code>","text":"<p>Add a camera to a pipeline</p> <p>Each pipeline consists of a bunch of scenarios to run and which cameras they need to be run on. This method allows the user to add one or more named camera instance to a pipeline. Please note the camera instance has to be created prior to adding it here.</p>"},{"location":"reference/reference/#add-a-camera","title":"add a camera","text":"<p>$ visionai camera add --name OFFICE-01 --uri https://youtube.com</p>"},{"location":"reference/reference/#add-camera-to-pipeline","title":"add camera to pipeline","text":"<p>$ visionai pipeline --name test_pipe add-camera --name OFFICE-01</p> <p>@arg pipeline - specify a named pipeline @arg camera - specify name of the camera to add</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline add-camera [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--camera TEXT</code>: camera to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-add-preprocess","title":"<code>visionai pipeline add-preprocess</code>","text":"<p>Add a preprocess routine to a pipeline</p> <p>Preprocessing tasks are run prior to scenarios. The order in which multiple preprocess tasks are added does not matter. All added preprocess routines are executed in different threads.</p> <p>$ visionai pipeline --name test_pipe add-preprocess --name face-blur</p> <p>$ visionai pipeline --name test_pipe add-preprocess --name text-blur</p> <p>$ visionai pipeline --name test_pipe show</p> <p>$ visionai pipeline --name test_pipe run</p> <p>@arg pipeline - specify a named pipeline @arg preprocess - specify name of the preprocess task to run</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline add-preprocess [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--preprocess TEXT</code>: preprocess routine to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-add-scenario","title":"<code>visionai pipeline add-scenario</code>","text":"<p>Add a scenario to a pipeline</p> <p>The order of the scenarios does not matter. All added scenarios are run in different threads. All scenarios are run after pre-processing stage is done.</p> <p>$visionai pipeline --name test_pipe add-scenario --name smoke-and-fire</p> <p>$visionai pipeline --name test_pipe add-scenario --name ppe-detection</p> <p>$visionai pipeline --name test_pipe run</p> <p>@arg pipeline - specify a named pipeline @arg scenario - specify name of the scenario to run</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline add-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--scenario TEXT</code>: scenario to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-create","title":"<code>visionai pipeline create</code>","text":"<p>Create a named pipeline</p> <p>Create a named pipeline. Pipeline is a list of scenarios to be run for specific cameras. The flow is as follows. Create a pipeline using:</p> <p>visionai pipeline create --name test_pipe</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name smoke-and-fire</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name ppe-detection</p> <p>visionai pipeline add-preprocess --pipeline test_pipe  --name face-blur</p> <p>visionai pipeline add-preprocess --pipeline test_pipe  --name text-blur</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name max-occupancy</p> <p>visionai pipeline show --pipeline test_pipe</p> <p>visionai pipeline add-camera --pipeline test_pipe  --name CAMERA-01</p> <p>visionai pipeline add-camera --pipeline test_pipe  --name CAMERA-02</p> <p>visionai pipeline show --pipeline test_pipe</p> <p>visionai pipeline run --pipeline test_pipe</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline create [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-remove-camera","title":"<code>visionai pipeline remove-camera</code>","text":"<p>Remove a camera from a pipeline</p> <p>This method can be used to remove a camera from a pipeline.</p> <p>$ visionai pipeline --name test_pipe remove-camera --name OFFICE-01</p> <p>@arg pipeline - specify a named pipeline @arg camera - specify name of the camera to remove</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline remove-camera [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--camera TEXT</code>: camera to remove  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-reset","title":"<code>visionai pipeline reset</code>","text":"<p>Reset the pipeline to original state.</p> <p>Deletes all cameras, scenarios and scenario configuration from the pipeline. Its as if the pipeline has been deleted and created from scratch again.</p> <p>$ visionai pipeline --name test_pipe reset</p> <p>@arg pipeline - pipeline to reset</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline reset [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-run","title":"<code>visionai pipeline run</code>","text":"<p>Run a pipeline of scenarios on given cameras</p> <p>Specify different scenarios to run on one or more cameras. This method can be directly used to specify scenarios and cameras directly. Else you can configure a named pipeline and then run it here.</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline run [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: Pipeline to run  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipeline-show","title":"<code>visionai pipeline show</code>","text":"<p>Show details of a pipeline</p> <p>Show what is configured in the current pipeline.</p> <p>$ visionai pipeline --name test_pipe show</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipeline show [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines","title":"<code>visionai pipelines</code>","text":"<p>... alias for pipeline</p> <p>Usage:</p> <pre><code>$ visionai pipelines [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>add-camera</code>: Add a camera to a pipeline Each pipeline...</li> <li><code>add-preprocess</code>: Add a preprocess routine to a pipeline...</li> <li><code>add-scenario</code>: Add a scenario to a pipeline The order of the...</li> <li><code>create</code>: Create a named pipeline Create a named...</li> <li><code>remove-camera</code>: Remove a camera from a pipeline This method...</li> <li><code>reset</code>: Reset the pipeline to original state.</li> <li><code>run</code>: Run a pipeline of scenarios on given cameras...</li> <li><code>show</code>: Show details of a pipeline Show what is...</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-add-camera","title":"<code>visionai pipelines add-camera</code>","text":"<p>Add a camera to a pipeline</p> <p>Each pipeline consists of a bunch of scenarios to run and which cameras they need to be run on. This method allows the user to add one or more named camera instance to a pipeline. Please note the camera instance has to be created prior to adding it here.</p>"},{"location":"reference/reference/#add-a-camera_1","title":"add a camera","text":"<p>$ visionai camera add --name OFFICE-01 --uri https://youtube.com</p>"},{"location":"reference/reference/#add-camera-to-pipeline_1","title":"add camera to pipeline","text":"<p>$ visionai pipeline --name test_pipe add-camera --name OFFICE-01</p> <p>@arg pipeline - specify a named pipeline @arg camera - specify name of the camera to add</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines add-camera [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--camera TEXT</code>: camera to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-add-preprocess","title":"<code>visionai pipelines add-preprocess</code>","text":"<p>Add a preprocess routine to a pipeline</p> <p>Preprocessing tasks are run prior to scenarios. The order in which multiple preprocess tasks are added does not matter. All added preprocess routines are executed in different threads.</p> <p>$ visionai pipeline --name test_pipe add-preprocess --name face-blur</p> <p>$ visionai pipeline --name test_pipe add-preprocess --name text-blur</p> <p>$ visionai pipeline --name test_pipe show</p> <p>$ visionai pipeline --name test_pipe run</p> <p>@arg pipeline - specify a named pipeline @arg preprocess - specify name of the preprocess task to run</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines add-preprocess [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--preprocess TEXT</code>: preprocess routine to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-add-scenario","title":"<code>visionai pipelines add-scenario</code>","text":"<p>Add a scenario to a pipeline</p> <p>The order of the scenarios does not matter. All added scenarios are run in different threads. All scenarios are run after pre-processing stage is done.</p> <p>$visionai pipeline --name test_pipe add-scenario --name smoke-and-fire</p> <p>$visionai pipeline --name test_pipe add-scenario --name ppe-detection</p> <p>$visionai pipeline --name test_pipe run</p> <p>@arg pipeline - specify a named pipeline @arg scenario - specify name of the scenario to run</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines add-scenario [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--scenario TEXT</code>: scenario to add  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-create","title":"<code>visionai pipelines create</code>","text":"<p>Create a named pipeline</p> <p>Create a named pipeline. Pipeline is a list of scenarios to be run for specific cameras. The flow is as follows. Create a pipeline using:</p> <p>visionai pipeline create --name test_pipe</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name smoke-and-fire</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name ppe-detection</p> <p>visionai pipeline add-preprocess --pipeline test_pipe  --name face-blur</p> <p>visionai pipeline add-preprocess --pipeline test_pipe  --name text-blur</p> <p>visionai pipeline add-scenario --pipeline test_pipe  --name max-occupancy</p> <p>visionai pipeline show --pipeline test_pipe</p> <p>visionai pipeline add-camera --pipeline test_pipe  --name CAMERA-01</p> <p>visionai pipeline add-camera --pipeline test_pipe  --name CAMERA-02</p> <p>visionai pipeline show --pipeline test_pipe</p> <p>visionai pipeline run --pipeline test_pipe</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines create [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--name TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-remove-camera","title":"<code>visionai pipelines remove-camera</code>","text":"<p>Remove a camera from a pipeline</p> <p>This method can be used to remove a camera from a pipeline.</p> <p>$ visionai pipeline --name test_pipe remove-camera --name OFFICE-01</p> <p>@arg pipeline - specify a named pipeline @arg camera - specify name of the camera to remove</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines remove-camera [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--camera TEXT</code>: camera to remove  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-reset","title":"<code>visionai pipelines reset</code>","text":"<p>Reset the pipeline to original state.</p> <p>Deletes all cameras, scenarios and scenario configuration from the pipeline. Its as if the pipeline has been deleted and created from scratch again.</p> <p>$ visionai pipeline --name test_pipe reset</p> <p>@arg pipeline - pipeline to reset</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines reset [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-run","title":"<code>visionai pipelines run</code>","text":"<p>Run a pipeline of scenarios on given cameras</p> <p>Specify different scenarios to run on one or more cameras. This method can be directly used to specify scenarios and cameras directly. Else you can configure a named pipeline and then run it here.</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines run [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: Pipeline to run  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-pipelines-show","title":"<code>visionai pipelines show</code>","text":"<p>Show details of a pipeline</p> <p>Show what is configured in the current pipeline.</p> <p>$ visionai pipeline --name test_pipe show</p> <p>@arg pipeline - specify a named pipeline</p> <p>@return None</p> <p>Usage:</p> <pre><code>$ visionai pipelines show [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--pipeline TEXT</code>: pipeline name  [required]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenario","title":"<code>visionai scenario</code>","text":"<p>Manage scenarios</p> <p>An organization can have multiple scenarios that are installed at different places. They may be from different vendors and/or maybe using different security surveillance software. Most scenarios however do support RTSP, RTMP or HLS streams as an output. Please refer to your scenario vendor documentation to find this out. This module will help you onboard those scenarios on visionai systems by using a simple named instance for each scenario.</p> <p>Usage:</p> <pre><code>$ visionai scenario [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>download</code>: Download models for scenarios Ex: visionai...</li> <li><code>list</code>: List all scenarios available List all...</li> <li><code>test</code>: Run the scenario locally to test it out.</li> </ul>"},{"location":"reference/reference/#visionai-scenario-download","title":"<code>visionai scenario download</code>","text":"<p>Download models for scenarios</p> <p>Ex: visionai scenario download ppe-detection  # download ppe-detection scenario Ex: visionai scenario download all            # download all configured scenarios for the org Ex: visionai scenario download world          # download all available scenarios</p> <p>Download models for a given scenario, or download models for all scenarios that have been configured.</p> <p>Usage:</p> <pre><code>$ visionai scenario download [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenario-list","title":"<code>visionai scenario list</code>","text":"<p>List all scenarios available</p> <p>List all scenarios available in the system. This includes scenarios that may or maynot be applied to any specific camera.</p> <p>Usage:</p> <pre><code>$ visionai scenario list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenario-test","title":"<code>visionai scenario test</code>","text":"<p>Run the scenario locally to test it out.</p> <ul> <li>Download the model if not available.</li> <li>Pull the model server container image.</li> <li>Start the model server container with this model.</li> <li>Run inference with this model</li> </ul> <p>Usage:</p> <pre><code>$ visionai scenario test [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name (default is webcam)  [default: 0]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenarios","title":"<code>visionai scenarios</code>","text":"<p>... alias for scenario</p> <p>Usage:</p> <pre><code>$ visionai scenarios [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>download</code>: Download models for scenarios Ex: visionai...</li> <li><code>list</code>: List all scenarios available List all...</li> <li><code>test</code>: Run the scenario locally to test it out.</li> </ul>"},{"location":"reference/reference/#visionai-scenarios-download","title":"<code>visionai scenarios download</code>","text":"<p>Download models for scenarios</p> <p>Ex: visionai scenario download ppe-detection  # download ppe-detection scenario Ex: visionai scenario download all            # download all configured scenarios for the org Ex: visionai scenario download world          # download all available scenarios</p> <p>Download models for a given scenario, or download models for all scenarios that have been configured.</p> <p>Usage:</p> <pre><code>$ visionai scenarios download [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenarios-list","title":"<code>visionai scenarios list</code>","text":"<p>List all scenarios available</p> <p>List all scenarios available in the system. This includes scenarios that may or maynot be applied to any specific camera.</p> <p>Usage:</p> <pre><code>$ visionai scenarios list [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-scenarios-test","title":"<code>visionai scenarios test</code>","text":"<p>Run the scenario locally to test it out.</p> <ul> <li>Download the model if not available.</li> <li>Pull the model server container image.</li> <li>Start the model server container with this model.</li> <li>Run inference with this model</li> </ul> <p>Usage:</p> <pre><code>$ visionai scenarios test [OPTIONS] NAME\n</code></pre> <p>Arguments:</p> <ul> <li><code>NAME</code>: [required]</li> </ul> <p>Options:</p> <ul> <li><code>--camera TEXT</code>: Camera name (default is webcam)  [default: 0]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-ui","title":"<code>visionai ui</code>","text":"<p>Start/stop web-app</p> <p>Start and stop the VisionAI web-app which can be a more intuitive way of managing cameras, pipelines and scenarios. Web-app also provides a live-stream view of the cameras.</p> <p>Usage:</p> <pre><code>$ visionai ui [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>start</code>: Start web server Use this function to start...</li> <li><code>status</code>: Web service status Use this function to get...</li> <li><code>stop</code>: Stop web server Use this function to stop...</li> </ul>"},{"location":"reference/reference/#visionai-ui-start","title":"<code>visionai ui start</code>","text":"<p>Start web server</p> <p>Use this function to start the web-service. Web service can be used for more intuitive configuration for the cameras and scenarios. Web-app is also the place to view event details, camera live-stream etc.</p> <p>Usage:</p> <pre><code>$ visionai ui start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-ui-status","title":"<code>visionai ui status</code>","text":"<p>Web service status</p> <p>Use this function to get the status of the web-service. (if its running or not. This function also prints diagnostic information like last few log messages etc.)</p> <p>Usage:</p> <pre><code>$ visionai ui status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--tail INTEGER</code>: tail number of lines  [default: 20]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-ui-stop","title":"<code>visionai ui stop</code>","text":"<p>Stop web server</p> <p>Use this function to stop already running web-service. There can be a single instance of the web-service supported currently. So there is no need for any arguments for this function.</p> <p>Usage:</p> <pre><code>$ visionai ui stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--web TEXT</code></li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-web","title":"<code>visionai web</code>","text":"<p>... alias for ui</p> <p>Usage:</p> <pre><code>$ visionai web [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul> <p>Commands:</p> <ul> <li><code>start</code>: Start web server Use this function to start...</li> <li><code>status</code>: Web service status Use this function to get...</li> <li><code>stop</code>: Stop web server Use this function to stop...</li> </ul>"},{"location":"reference/reference/#visionai-web-start","title":"<code>visionai web start</code>","text":"<p>Start web server</p> <p>Use this function to start the web-service. Web service can be used for more intuitive configuration for the cameras and scenarios. Web-app is also the place to view event details, camera live-stream etc.</p> <p>Usage:</p> <pre><code>$ visionai web start [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-web-status","title":"<code>visionai web status</code>","text":"<p>Web service status</p> <p>Use this function to get the status of the web-service. (if its running or not. This function also prints diagnostic information like last few log messages etc.)</p> <p>Usage:</p> <pre><code>$ visionai web status [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--tail INTEGER</code>: tail number of lines  [default: 20]</li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/reference/#visionai-web-stop","title":"<code>visionai web stop</code>","text":"<p>Stop web server</p> <p>Use this function to stop already running web-service. There can be a single instance of the web-service supported currently. So there is no need for any arguments for this function.</p> <p>Usage:</p> <pre><code>$ visionai web stop [OPTIONS]\n</code></pre> <p>Options:</p> <ul> <li><code>--web TEXT</code></li> <li><code>--help</code>: Show this message and exit.</li> </ul>"},{"location":"reference/troubleshooting/","title":"Troubleshooting","text":""},{"location":"reference/troubleshooting/#windows-installation-through-wsl2","title":"Windows Installation through WSL2","text":"<p>Installation on a Windows system is usually challenging because of the various Docker and NVIDIA dependencies. If you come across any issues - please do not hesitate to reach out to us.</p>"},{"location":"reference/troubleshooting/#install-wsl-2-windows-subsystem-for-linux-2","title":"Install WSL 2 (Windows Subsystem for Linux 2)","text":"<p>Install WSL2 using the following steps.</p> <ol> <li>Open Command Prompt, and run the following command:</li> </ol> <pre><code>wsl.exe --install\n</code></pre> <ol> <li> <p>This should install WSL Ubuntu image on your desktop. Once installed - it will ask you to reboot the system. </p> </li> <li> <p>Once reboot is complete, WSL will prompt you for username &amp; password for your Linux system. Enter this information.</p> </li> </ol>"},{"location":"reference/troubleshooting/#install-docker","title":"Install Docker","text":"<ol> <li> <p>Download docker desktop from here.</p> </li> <li> <p>Follow the on-screen instructions and install Docker Desktop. </p> </li> <li> <p>Reboot the system again.</p> </li> </ol>"},{"location":"reference/troubleshooting/#set-wsl-to-ubuntu","title":"Set WSL to Ubuntu","text":"<ol> <li>Set the default Linux distribution to Ubuntu on WSL.</li> </ol> <pre><code>wsl.exe --set-version Ubuntu\n</code></pre> <ol> <li>Start the Linux Subsystem</li> </ol> <pre><code>wsl.exe\n</code></pre>"},{"location":"reference/troubleshooting/#complete-installation","title":"Complete installation","text":"<ol> <li>This concludes installing Docker and WSL Subsystems on Windows Environment. From this point onwards, you can follow the Ubuntu instructions. For example, next step is to install VisionAI package using <code>pip install visionai</code> command.</li> </ol>"},{"location":"scenarios/","title":"Scenarios","text":"<p>Scenarios form the building blocks of VisionAI platform. These scenarios are organized into <code>Suites</code>. Below we talk about different suites and the scenarios that are part of them.</p> <ul> <li>All scenarios are available as pick-n-choose scenarios. You can pick the scenarios you want based on your business needs. Each scenario is independently tested.</li> <li>Events provided by these scenarios are given below. Events are sent to Redis &amp; Azure EventHub pubsub systems for further integration.</li> <li>There are a few common events supported by all scenarios (daily summary, weekly summary etc.)</li> <li>Currently supported scenarios are highlighted by a \u2705. Roadmap scenarios are highlighted by a \ud83d\udcc5.</li> <li>Each of the scenarios can be quickly tested through <code>visionai run &lt;scenario-name&gt;</code> command. For example:</li> </ul> <pre><code>visionai run smoke-and-fire-detection\n</code></pre> <p>New scenario request</p> <p>This section lists down all the scenarios that are supported by the VisionAI platform. There are more scenarios added daily - please send a request to us about any additional scenarios you need.</p>"},{"location":"scenarios/#privacy-suite","title":"Privacy Suite","text":"<p>For a majority of organizations - employee privacy is a top concern. Along with employee privacy, the organization needs to make sure that any data does not leave the premises. Any faces detected through Vision AI system need to be blurred, along with text, signage, computer screens and other sensitive information.</p> <p>Before any other scenarios are run, or before we store or process the images - the images are pre-processed through this privacy suite. As such, privacy suite is treated differently from other scenarios. Below examples provide a high-level overview of the privacy suite.</p> Status Scenario name Details Additional considerations \u2705 <code>face-blurring</code> Blur any faces detected More details \u2705 <code>text-blurring</code> Blue any text detected (paper, computer screens etc) More details \u2705 <code>license-plate-blurring</code> Blur any license plates detected More details \ud83d\udcc5 <code>signs-blurring</code> Blur any signs detected More details \ud83d\udcc5 <code>obstructed-camera</code> If camera feed is obstructed, send an alert More details"},{"location":"scenarios/#hazard-warnings-suite","title":"Hazard Warnings Suite","text":"<p>Following scenarios provide hazard warning examples supported by VisionAI suite. Currently supported scenarios are highlighted by a \u2705. </p> Status Scenario name Supported Events Additional considerations \u2705 <code>smoke-and-fire-detection</code> <code>Smoke event detected</code> <code>Fire event detected</code> <code>Sparks detected</code> <code>Open flames detection</code> More details \u2705 <code>no-smoking-zone</code> <code>Smoking event detected</code> <code>Vaping event detected</code> More details \ud83d\udcc5 <code>spills-and-leak-detection</code> <code>Water puddle detected</code> <code>Water leak from equipment detected</code> <code>Spill event detected</code> <code>Slippery sign detected</code> \ud83d\udcc5 <code>missing-fire-extinguisher</code> <code>Fire extinguisher missing</code> \ud83d\udcc5 <code>blocked-exit-monitoring</code> <code>Blocked exit detected</code> \u2705 <code>rust-and-corrosion-detection</code> <code>Rust or corrosion event detected</code> More details"},{"location":"scenarios/#worker-health-safety-suite","title":"Worker Health &amp; Safety Suite","text":"<p>Following scenarios provide Worker Health and Safety examples supported by VisionAI suite. (Also referred to as Personnel Health and Safety).</p> <p>Workplace Personnel Health &amp; Safety is important because it ensures that employees are safe and healthy in their work environment. This includes providing a safe and healthy work environment, proper safety training, and regular safety inspections. Additionally, it also includes enforcing safety policies to ensure that all employees are aware of and follow safety procedures, as well as encouraging a culture of safety within the workplace.</p> <p>Currently supported scenarios are highlighted by a \u2705. </p> <p>You can see real-time events generated as soon as person is detected without PPE (helmets, gloves, safety boots etc.). There are options to configure what PPE's are required for your scenario. This can be done through the VisionAI web-application which can be accessed on through http://localhost:3001.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>ppe-detection</code> <code>Person detected without helmet</code> <code>Person detected without gloves</code> <code>Person detected without safety boots</code> <code>Person detected without safety goggles</code> <code>Person detected without face mask</code> <code>Person detected without vest</code> <code>Person detected without full-body suit</code> <code>Person detected without PFAS</code> <code>Person detected without ear protection</code> More details \u2705 <code>working-at-heights</code> <code>Person detected without PFAS</code> <code>Steps detected without railings</code> <code>Person detected at height without parapets</code> <code>Ladder detected not in compliance</code> More details \u2705 <code>fall-and-accident-detection</code> <code>Person slip &amp; fall detected</code> <code>Potential collision/accident detected</code> <code>Wet floor detected</code> <code>Debris detected on floor</code> <code>Wet/slippery sign detected</code> \u2705 <code>posture-and-ergonomics</code> <code>Bend count per individual</code> Straight camera angle  More details \ud83d\udcc5 <code>empty-pallets-detection</code> <code>Empty pallets detected</code> <code>Partially empty pallets detected</code> \ud83d\udcc5 <code>spills-and-leaks-detection</code> <code>Water puddle detected</code> <code>Water leak from equipment detected</code> <code>Wet floor detected</code> <code>Spill event detected</code> <code>Slippery sign detected</code> \ud83d\udcc5 <code>hand-wash-compliance</code> <code>Missed hand wash</code> \u2705 <code>confined-spaces-monitoring</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details"},{"location":"scenarios/#occupancy-policies","title":"Occupancy Policies","text":"<p>Occupancy Policies relate to counting and tracking employees and/or other personnel in the room. These could include people-counting and enforcing max-occupancy policies, or tracking people's dwell time in a confined space.</p> <p>Currently supported scenarios are highlighted by a \u2705. </p> <p>Occupancy Metrics</p> <ul> <li>Occupancy metrics is similar in structure to max-occupancy, or restricted areas scenarios.</li> <li>However it sends out a summary event is structured like this. This will give a granular summary event at the end of the day.</li> <li>Users can start with occupancy-metrics and then move to max-occupancy or restricted areas if they need to enforce policies. <pre><code>{\n  \"date\": \"2023-02-23\",\n  \"stations\": [{\n    \"id\": \"station_1\",\n    \"hours\": [\n        {\n          \"start_time\": \"2023-02-23T14:00:01\",\n          \"end_time\": \"2023-02-23T15:00:00\",\n          \"occupancy_cnt\": 14\n        }\n        ...\n    ]\n  }...]\n}\n</code></pre></li> </ul> <p>Also need to specify that the camera needs to be configured to have a good view of the stations where occupancy metrics need to be checked.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>max-occupancy</code> <code>Person count exceeds max limit</code> More details \u2705 <code>restricted-areas</code> <code>Person detected in restricted area</code> <code>Movement detected in restricted area</code> <code>Person detected after hours</code> <code>Movement detected after hours</code> More details \u2705 <code>dwell-time</code> <code>Person detected</code> <code>Person left</code> <code>Person dwell time exceeds limit</code> <code>Person detected without motion</code> <code>Person fall detected</code> More details \u2705 <code>station-occupancy</code> <code>Daily summary event</code> More details \ud83d\udcc5 <code>occupancy-metrics</code> <code>Daily summary event</code> \ud83d\udcc5 <code>authorized-personnel-only</code> <code>Unauthorized person detected</code>"},{"location":"scenarios/#company-policies","title":"Company Policies","text":"<p>Company policies include specific scenarios that are relevant to your company. These could include scenarios like no-smoking/no-vaping zones, no food or drinks in certain areas, or no cell phones/pictures in certain areas. Some of these scenarios overlap with occupancy policies, but they are still useful to have here as separate scenarios.</p> Status Scenario name Supported Events Additional considerations \ud83d\udcc5 <code>no-food-or-drinks-allowed</code> <code>Person with food detected</code> <code>Person with drinks detected</code> <code>Spill event detected</code> More details \u2705 <code>no-phone-text-pictures</code> <code>Cellphone usage detected</code> <code>Person detected taking pictures</code> More details \u2705 <code>no-smoking-or-vaping</code> <code>Smoking event detected</code> <code>Vaping event detected</code> More details \u2705 <code>no-children-pets-visitors</code> <code>Children detected</code> <code>Pets detected</code> <code>Visitors detected</code> More details"},{"location":"scenarios/#suspicious-activity-detection","title":"Suspicious Activity detection","text":"<p>Suspicious activity detection suite relies on a combination of activity detection models and object detection models. These models are trained to detect suspicious activity in a variety of scenarios.</p> Status Scenario name Supported Events Additional considerations \u2705 <code>vandalism-graffiti-company-property-destruction</code> <code>Motion detected in area (gross event)</code> <code>People detected in area (more granular event)</code> <code>Non-uniformed personnel detected in area</code> <code>Non badged personnel detected in area</code> <code>Vandalism detected in area (before &amp; after)</code> <code>Paint/graffiti detected in area (before &amp; after changes)</code> <code>Behavior analysis event showing company property destruction.</code> More details \u2705 <code>firearms-knives-detection</code> <code>Person brandishing firearm</code> <code>Person brandishing knives</code> More details"},{"location":"scenarios/#next-steps","title":"Next Steps","text":"<p>Now that you have a better understanding of the scenarios that are available, you can start to think about how you can organize these scenarios into a solution that meets your needs. You can also go to the individual scenario page to learn more about it. We can customize each of these models for your use-cases and provide you with a solution that is tailored to your needs. You can contact us through this page.</p> <ol> <li> <p>This works by detecting a person's uniform and comparing it to a list of authorized personnel. This is a more advanced scenario and requires a custom model to be trained for your specific use-case.\u00a0\u21a9</p> </li> </ol>"},{"location":"scenarios/aggressive-behavior/","title":"Aggressive Behavior","text":"<p>Create a safer and more productive work environment with our real-time Aggressive behaviour detection system.</p> <p> </p> Detection of Aggressive Behavior"},{"location":"scenarios/aggressive-behavior/#overview","title":"Overview","text":"<p>A workplace that is free from bullying, fighting, and aggressive behavior can help to improve employee well-being and overall job satisfaction. This can result in higher levels of productivity, better employee retention rates, and a more positive work environment. Aggressive behavior is a serious problem at workplaces. It can lead to serious injuries and even death. It is important to detect these behaviors early on, and preventive measures can be taken to address the issue before it escalates.</p> <p>These models are an important tool for promoting a safe and respectful environment at workplaces and other settings, and they have the potential to make a real difference in the lives of those who may be vulnerable to bullying or aggression.</p>"},{"location":"scenarios/aggressive-behavior/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's agressive behaviour detection model is designed to promote a safe, healthy, and productive workplace environment for all employees. The model is able to provide real-time alerts when it detects aggressive behavior. This will enable management to intervene and prevent escalation of the situation.</p>"},{"location":"scenarios/aggressive-behavior/#model-details","title":"Model Details","text":""},{"location":"scenarios/aggressive-behavior/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world data from different workplaces. The dataset consists of images and videos collected from various sources.</p>"},{"location":"scenarios/aggressive-behavior/#model","title":"Model","text":""},{"location":"scenarios/aggressive-behavior/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   32,20 v1 Ceiling 95%  93%  85%"},{"location":"scenarios/aggressive-behavior/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor the presence of people and analyse their behaviour.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>An alarming system is in place as part of an aggressive behavior detection solution.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test aggressive-behavior-detection\n\nDownloading models for scenario: aggressive-behavior-detection\nModel: aggressive-behavior-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-aggressive-behavior-detection/yolov5s-aggressive-behavior-detection-0.0.1.zip\n\n\nStarting scenario: aggressive-behavior-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of aggressive behavior within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/aggressive-behavior/#features","title":"Features","text":"<ul> <li> <p>Easy to use: The model is easy to use and can be deployed in a variety of settings, including workplaces and other public spaces.</p> </li> <li> <p>Alert system: The model is able to generate alerts when it detects signs of bullying, fighting, or aggressive behavior, allowing management to take appropriate action to address the issue.</p> </li> <li> <p>Real-time monitoring: The detection model is be able to monitor and analyze interactions among employees in real-time to identify any signs of bullying, fighting, or aggressive behavior.</p> </li> </ul>"},{"location":"scenarios/aggressive-behavior/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/aggressive-behavior/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/authorized-personnel-occupancy/","title":"Authorized Personnel","text":"<p>An intelligent way to enhance security and prevent unauthorized access to restricted areas.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#overview","title":"Overview","text":"<p>Unauthorized entry or access refers to any member of the public other than employees entering areas of the restricted business premises or a warehouse. This could be done by an individual, a group of people, visitors or children/pets wrongly entering a restricted area. There can be different ways in which a security breach can happen and an unauthorized access can pose various risks;</p> <ul> <li>Safety risk</li> <li>Security risk</li> <li>Risk of non-compliance</li> <li>Risk of injury</li> </ul> <p>It is important for organizations to maintain security and controlled access at the workplace. However, conventional surveillance methods are often complex, human-oriented, expensive and challenging to automate. In addition, the existing solutions cannot detect intrusion after an unauthorized entry has taken place.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Make your workplace safer and smarter with our VisionAI monitoring, a complete solution that helps you enforce security policy adherence and deter intruders effectively. Now, you can easily detect any attempts to gain unauthorized access with our fully automated system that guards your facility 24/7 and sends instant alerts to help you prevent a security breach before it occurs. </p> <p>Our system offers reliable detection and is easy to integrate with your existing camera infrastructure, allowing you to scale your system with a few simple clicks.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Children detected in area</li> <li>Visitors detected in area</li> <li>Person without uniform detected</li> <li>Person without badge detected</li> </ul> <p>It is recommended that any instance of above detected events be reported to the appropriate authority. An event data for this scenario has information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>Type of personnel identified</li> </ul>"},{"location":"scenarios/authorized-personnel-occupancy/#model-details","title":"Model Details","text":""},{"location":"scenarios/authorized-personnel-occupancy/#dataset","title":"Dataset","text":"<p>The dataset comprises relevant, high-quality, labeled videos and images from diverse sources. The dataset is evenly distributed and balanced with an equal number of examples for each category to avoid bias toward one class. It contains variations with different real-world scenarios to render effective and efficient results.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   22,20 v1 Both(Ceiling and Straight) 95%  93%  85%"},{"location":"scenarios/authorized-personnel-occupancy/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for access control protects against unauthorized access and physical security incidents before they occur. Different scenarios of unauthorized access have been taken into account for real-time detection and alerts which include;</p> <ul> <li>Visitors detected in the area</li> <li>Employees without uniform trying to gain access </li> <li>Employees without badges trying to enter the premise </li> <li>Children/pets wrongly entering restricted areas</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test auth-personnel-detection\n\nDownloading models for scenario: auth-personnel-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: auth-personnel-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with authorized personnel identified within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/authorized-personnel-occupancy/#features","title":"Features","text":"<p>Some potential features of VisionAI for identifying authorized personnel could include: - Improved safety and security - Enhanced visual monitoring of each entry and exit point across all locations - Remotely monitor and instantly investigate any security concerns taking place even after working hours - Immediately get notified of potential physical security breaches, address them quickly and operate effectively.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/authorized-personnel-occupancy/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/authorized-personnel/","title":"Authorized Personnel","text":"<p>An intelligent way to enhance security and prevent unauthorized access to restricted areas.</p>"},{"location":"scenarios/authorized-personnel/#overview","title":"Overview","text":"<p>Unauthorized entry or access refers to any member of the public other than employees entering areas of the restricted business premises or a warehouse. This could be done by an individual, a group of people, visitors or children/pets wrongly entering a restricted area. There can be different ways in which a security breach can happen and an unauthorized access can pose various risks;</p> <ul> <li>Safety risk</li> <li>Security risk</li> <li>Risk of non-compliance</li> <li>Risk of injury</li> </ul> <p>It is important for organizations to maintain security and controlled access at the workplace. However, conventional surveillance methods are often complex, human-oriented, expensive and challenging to automate. In addition, the existing solutions cannot detect intrusion after an unauthorized entry has taken place.</p>"},{"location":"scenarios/authorized-personnel/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Make your workplace safer and smarter with our VisionAI monitoring, a complete solution that helps you enforce security policy adherence and deter intruders effectively. Now, you can easily detect any attempts to gain unauthorized access with our fully automated system that guards your facility 24/7 and sends instant alerts to help you prevent a security breach before it occurs. </p> <p>Our system offers reliable detection and is easy to integrate with your existing camera infrastructure, allowing you to scale your system with a few simple clicks.</p>"},{"location":"scenarios/authorized-personnel/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Children detected in area</li> <li>Visitors detected in area</li> <li>Person without uniform detected</li> <li>Person without badge detected</li> </ul> <p>It is recommended that any instance of above detected events be reported to the appropriate authority. An event data for this scenario has information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>Type of personnel identified</li> </ul>"},{"location":"scenarios/authorized-personnel/#model-details","title":"Model Details","text":""},{"location":"scenarios/authorized-personnel/#dataset","title":"Dataset","text":"<p>The dataset comprises relevant, high-quality, labeled videos and images from diverse sources. The dataset is evenly distributed and balanced with an equal number of examples for each category to avoid bias toward one class. It contains variations with different real-world scenarios to render effective and efficient results.</p>"},{"location":"scenarios/authorized-personnel/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   22,20 v1 Both(ceiling and Straight) 95%  93%  85%"},{"location":"scenarios/authorized-personnel/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for access control protects against unauthorized access and physical security incidents before they occur. Different scenarios of unauthorized access have been taken into account for real-time detection and alerts which include;</p> <ul> <li>Visitors detected in the area</li> <li>Employees without uniform trying to gain access </li> <li>Employees without badges trying to enter the premise </li> <li>Children/pets wrongly entering restricted areas</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test auth-personnel-detection\n\nDownloading models for scenario: auth-personnel-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: auth-personnel-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with authorized personnel identified within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/authorized-personnel/#features","title":"Features","text":"<p>Some potential features of VisionAI for identifying authorized personnel could include: - Improved safety and security - Enhanced visual monitoring of each entry and exit point across all locations - Remotely monitor and instantly investigate any security concerns taking place even after working hours - Immediately get notified of potential physical security breaches, address them quickly and operate effectively.</p>"},{"location":"scenarios/authorized-personnel/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/authorized-personnel/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/blocked-exit/","title":"Blocked Exit monitoring","text":"<p>Ensure that your organziation is safe &amp; compliant for any emergency evacuations. If any of your exits are blocked - get an event notifications so it can be quickly resolved.</p> <p>Detection of blocked exit</p>"},{"location":"scenarios/blocked-exit/#overview","title":"Overview","text":"<p>Every second counts in emergency situations such as fires, earthquakes, or other disasters, and people need to evacuate the premises as quickly as possible to avoid being trapped or injured. Therefore, the presence of functional emergency exits and a well-designed escape route plan is of paramount importance. </p> <p>Any blockages or obstructions in the exits could prove fatal as they hinder people's ability to evacuate safely and quickly. However, the accumulation of debris, fire flames, and toxic gasses can block exits. Time is of the essence, and wrong escape plans can impede people from escaping quickly and safely. </p> <p>Moreover, People may try to force their way through blocked exits, leading to injuries or even fatalities. In addition, blocked exits can hinder the efforts of rescue teams trying to enter the building to help those in need. This makes it critical to preempt operational exits and detect blocked ones before the situation worsens.</p>"},{"location":"scenarios/blocked-exit/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI based emergency rescue solution leverages cutting-edge AI-powered, real-time detection and monitoring of emergency exits with alerts and warnings for safe and secure evacuation in times of crisis. </p> <p>With our Blocked Exit Monitoring solution, you can rest assured that your premises are being monitored continuously, providing early warnings in case of any blockages in emergency exits. Our models can be deployed instantly and can augment your existing camera infrastructure with just a few clicks.</p>"},{"location":"scenarios/blocked-exit/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Blocked exit detected</li> </ul>"},{"location":"scenarios/blocked-exit/#model-details","title":"Model Details","text":""},{"location":"scenarios/blocked-exit/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li>Images of blocked exits: These images would provide visual examples of what a typical blocked exit looks like.</li> </ul> <p>Images of different types of blockages which includes: </p> <ul> <li> <p>Images of debris: Images of different types of debris, such as fallen objects or construction materials, would help the model to recognize obstructions in the path to the exit.</p> </li> <li> <p>Images of fire: Providing images of fire, smoke, and flames would help the model to recognize the different visual parameters that indicate a fire hazard.</p> </li> <li> <p>Images of toxic gasses: Images of different types of toxic gasses, such as carbon monoxide, would help the model to recognize the signs of gas leaks and other potential hazards.</p> </li> <li> <p>Images of overcrowding: Providing images of overcrowded spaces would help the model to recognize the risk of blockages in case of an emergency.</p> </li> <li> <p>Images of unobstructed exits: Providing images of exits that are not blocked would help the model to distinguish between blocked and unblocked exits.</p> </li> </ul>"},{"location":"scenarios/blocked-exit/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   22,20 v1 Both(ceiling and Straight) 95%  93%  85%"},{"location":"scenarios/blocked-exit/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: - We use existing camera feeds from the premises to monitor blocked exits - VisionAI system is run at the edge. It uses the camera feeds for processing.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test blocked-exit\n</code></pre> <p>Downloading models for scenario: blocked-exit Model: blocked-exit: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip</p> <p>Starting scenario: blocked-exit..</p> <p>```</p> </li> <li> <p>You should be able to see the events generated on your console window with the detections of maximum occupancy event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/blocked-exit/#features","title":"Features","text":"<p>Some potential features of VisionAI for monitoring maximum occupancy include:</p> <ul> <li> <p>Real-time monitoring of maximum occupancy: VisionAI can monitor blocked exits in real-time, providing an automated and seamless approach to crowd management.</p> </li> <li> <p>Instant alerts and warnings: VisionAI can send instant alerts and warnings for detected blocked exits</p> </li> <li> <p>Easy to deploy: VisionAI can be easily deployed with minimal effort, allowing businesses to leverage our AI-based technology with minimal effort.</p> </li> </ul>"},{"location":"scenarios/blocked-exit/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/blocked-exit/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/compliance-policies/","title":"Compliance policies","text":"<p>Compliance policies are put in place by companies to ensure the safety of employees, customers, and the general public. To send real-time alerts to employees, managers, and other stakeholders if there is a potential violation or deviation from established policies and procedures.</p> <p>These are a set of rules and regulations that a company creates and enforces to ensure that it operates in accordance with applicable laws, regulations, and ethical standards. The purpose of compliance policies is to help companies prevent legal and ethical violations, promote responsible conduct, and maintain their reputation and public trust. Some of these are  No pictures and no mobile phones in certain areas etc. There are many events that could trigger an alert for non-adherence to a compliance policy. Here are a few use cases:</p> <ul> <li>No food or drinks</li> <li>No phone, text, pictures</li> <li>No Smoking zones</li> <li>No children/visitors</li> </ul>"},{"location":"scenarios/confined-spaces-monitoring/","title":"Confined Spaces Monitoring","text":"<p>Ensure safety of employees in confined spaces. Get real-time alerts when workers are present in the space for too long.</p>"},{"location":"scenarios/confined-spaces-monitoring/#overview","title":"Overview","text":"<p>Confined spaces refer to areas that are partially or fully enclosed and are not designed for continuous human occupancy. Examples include tanks, silos, storage bins, manholes, and underground vaults. These spaces can be hazardous due to limited ventilation, lack of natural light, and potential for hazardous atmospheric conditions.</p> <p>Workers entering confined spaces are at risk of being overcome by toxic gases, asphyxiation, or other hazards. In addition, workers may be trapped in the confined space if an emergency occurs. Therefore, it is important to monitor confined spaces to ensure that they remain safe for workers.</p> <p>To monitor confined spaces, cameras can be used - with an oversight manager observing these spaces. Along with cameras other IoT devices can be used to measure atmospheric conditions such as air quality, temperature, humidity, and toxic gas levels.</p>"},{"location":"scenarios/confined-spaces-monitoring/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to monitor confined spaces by providing real-time video feeds of the area. These cameras can be used to monitor the presence of workers in the confined space, as well as the duration of how long they are present within the space. Companies can put compliance policies in place to ensure that workers are not present in the confined space for an extended period of time. Camera based monitors can track workers entering the premises, and monitor their total duration of stay; and if that exceeds the compliance policy, an alert can be raised.</p> <p>It is important to note that these camera based monitoring provides should be supplanted by strong compliance processes to ensure their accuracy and reliability. In addition, workers entering confined spaces should always be trained on proper use of the monitoring equipment and be familiar with the hazards associated with confined spaces.</p>"},{"location":"scenarios/confined-spaces-monitoring/#model-details","title":"Model Details","text":""},{"location":"scenarios/confined-spaces-monitoring/#dataset","title":"Dataset","text":"<p>The datasets for this scenario is based off of people detection and tracking algorithms that are used in the industry. The dataset is a combination of images and videos from various sources. The dataset is curated to ensure that it is representative of the real world. It has equal distributions for:</p> <ul> <li>Indoor vs Outdoor environments</li> <li>Male vs Female</li> <li>Day vs Night</li> <li>Different types of clothing</li> <li>Different distances from the camera</li> <li>Various lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using seurity camera feeds</li> </ul> <p>Total number of images used was 387,644</p>"},{"location":"scenarios/confined-spaces-monitoring/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset curated by our team.</p>"},{"location":"scenarios/confined-spaces-monitoring/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   2678 v2 Ceiling 94% 96% 94% <p>The model is light-weight enough to be run on any edge devices.</p>"},{"location":"scenarios/confined-spaces-monitoring/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: - We use existing camera feeds from the premises to monitor the presence of workers in the confined space. - VisionAI system is run at the edge. It uses the camera feeds for processing. - We detect and track people identified in this camera feed. - We monitor the total duration of stay of these people in the confined space. - If the duration of stay exceeds the compliance policy, an alert is raised.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test obstructed-camera-detection\n\nDownloading models for scenario: obstructed-camera-detection\nModel: obstructed-camera-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-obstructed-camera-detection/yolov5s-obstructed-camera-detection-0.0.1.zip\n\n\nStarting scenario: obstructed-camera-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of people exceeding the duration limit within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/confined-spaces-monitoring/#events-supported","title":"Events Supported","text":"<p>This scenario supports the following events:</p> <ul> <li>Person detected: This event is generated when a person is detected in the camera feed.</li> <li>Person left: This event is generated when a person is no longer detected in the camera feed.</li> <li>Person duration exceeded: This event is generated when a person is detected for more than the specified duration in the camera feed. The duration amount is configurable through the web-app.</li> </ul>"},{"location":"scenarios/confined-spaces-monitoring/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/confined-spaces-monitoring/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/custom-scenarios/","title":"Custom scenarios","text":"<p>Coming soon</p>"},{"location":"scenarios/employee-privacy/","title":"Employee Privacy","text":"<p>Employee privacy is a concern for many companies. The use of cameras in the workplace can be a sensitive issue. Employees may feel that their privacy is being violated. They may also feel that their personal space is being invaded. This can lead to a loss of trust and confidence in the company.</p> <p>Privacy policies are put in place to protect employees from unwarranted surveillance. They also help to ensure that employees are not subjected to any form of discrimination or harassment.</p> <p>To send real-time alerts to employees, managers, and other stakeholders if there is a potential violation or deviation from established policies and procedures.</p> <p>These are a set of rules and regulations that a company creates and enforces to ensure that it operates in accordance with applicable laws, regulations, and ethical standards. The purpose of compliance policies is to help companies prevent legal and ethical violations, promote responsible conduct, and maintain their reputation and public trust. Some of these are  No pictures and no mobile phones in certain areas etc. There are many events that could trigger an alert for non-adherence to privacy policies. Here are a few use cases:</p> <ul> <li>Blur faces</li> <li>Blur signs/text</li> <li>Blur screens</li> <li>Blur license plates</li> <li>Obstructed camera view</li> </ul>"},{"location":"scenarios/ergonomics/","title":"Ergonomics Monitoring","text":"<p>Ensure safety and comfort of employees by monitoring ergonomics. </p>"},{"location":"scenarios/ergonomics/#overview","title":"Overview","text":"<p>Ergonomics is the study of designing and arranging products, systems, and environments to fit the capabilities and limitations of people, with the goal of improving efficiency, safety, and comfort. The primary focus of ergonomics is to create environments that optimize human performance and well-being.</p>"},{"location":"scenarios/ergonomics/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to monitor productivity and workers health by providing real-time video feeds of different areas. These cameras can be used to monitor and track number of objects moved by persons from one place to the other, as well as the bending information of workers while performing this work. </p> <p>Companies can put compliance policies in place to ensure that workers are made aware of work-related injuries and illnesses\u202fdue to unnecessary bending.  </p> <p>It is important to note that these camera-based monitoring provides should be supplanted by strong compliance processes to ensure their accuracy and reliability. In addition, workers working in companies should always be trained on ergonomics and its significance for safety and its impact on human health.</p>"},{"location":"scenarios/ergonomics/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Bend count event per individual</li> </ul>"},{"location":"scenarios/ergonomics/#model-details","title":"Model Details","text":""},{"location":"scenarios/ergonomics/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Person and Book classes are considered for model building. Person class is considered here because the problem of ergonomics is related to pose estimation and the object used to show movement is the book. It is the object carried by a person from one location to the other. Other objects can be considered as per the requirement.  </p> <p>The dataset is made up of a large number of images and it is curated to ensure a true  representation of the real world for: </p> <ul> <li> <p>Indoor vs Outdoor environments </p> </li> <li> <p>Variations in time  </p> </li> <li> <p>Different types of clothing for persons </p> </li> <li> <p>Different distances from the camera </p> </li> <li> <p>Various lighting conditions </p> </li> <li> <p>Various camera angles, resolutions and calibrations </p> </li> <li> <p>Using security camera feeds </p> </li> </ul>"},{"location":"scenarios/ergonomics/#model","title":"Model","text":"<p>The Yolov5 pre-trained model for detecting person and book (an example of an item) classes are used to build the model. DenseNet is employed to estimate each person's landmarks. These landmarks are used to estimate poses. This is mostly used to track a person's bending motion. To guarantee the counting of boxes, object tracking using a strong sort algorithm is also built. </p> <p>This provides ergonomics data that may be utilised for a variety of tasks, such as alarm generation when the number of bends exceeds predetermined levels and productivity counting to determine how many objects were transported from one location to another. </p>"},{"location":"scenarios/ergonomics/#model-card","title":"Model card","text":"<p>The DenseNet Model for Landmark detection</p> Dataset size Version Camera support Precision Recall  mAP   10894 v2 Straight 84% 72% 84% <p>The model is lightweight enough to be run on any edge device. </p>"},{"location":"scenarios/ergonomics/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor the presence of workers. </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect and track the number of objects transported and we monitor the total number of bending motions of a person while working.</p> </li> </ul> <p>=== \"Test now with online Web-Cam\"      To test this model &amp; scenario, you can use the following steps:</p> <pre><code> - Install the visionai package from PyPI\n\n    ```console\n    $ pip install visionai\n\n    ```\n\n - Test the scenario from your local web-cam\n\n\n    ```console\n    $ visionai scenario test ergonomics\n\n    ```\n\n    Downloading models for scenario: ergonomics\n\n\n\n    Starting scenario: ergonomics..\n\n    ```\n- You should be able to see the events generated on your console window with the detections of firearms and knives event within the camera field of view.\n</code></pre> With RTSP Camera - PipelinesWith Azure Setup <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/ergonomics/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please [contact us] (../company/contact.md).</p>"},{"location":"scenarios/ergonomics/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/exclusion-zones/","title":"Restricted areas/times","text":"<p>Secure restricted areas with our powerful AI-based monitoring system that detects and prevents unauthorized access in real-time</p> <p> </p> Detection of unauthorized entry event"},{"location":"scenarios/exclusion-zones/#overview","title":"Overview","text":"<p>Unauthorized access to restricted zones at the workplace can lead to theft, accidents, and other security breaches. Be it valuable assets, sensitive information or providing employee safety, maintaining high-security and controlled access for restricted zones at the workplace is essential for all organizations. However, monitoring and controlling access to these areas is often an expensive and error-prone process, requiring continuous manual surveillance by security personnel.</p> <p>Another major problem with existing systems cannot detect intrusion after an unauthorized access has been made.This renders biometric, sensors and security personnels ineffective after an unauthorized access has been already made.</p>"},{"location":"scenarios/exclusion-zones/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>With our Vision AI monitoring you can authorize access as well as continuous monitor live feeds inside a restricted area for real-time detection of unauthorized personnel. Our fully automated detection models are not only more powerful and accurate than existing systems but also more affordable and easy to integrate into existing infrastructure allowing users to scale the power of i-based real-time detection with a few simple clicks.</p>"},{"location":"scenarios/exclusion-zones/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Person detected in restricted area</li> <li>Movement detected in restricted area</li> <li>Person detected after hours</li> <li>Movement detected after hours\"</li> </ul> <p>It is recommended that any instance of unauthorized entry be reported to the appropriate authority. An event data for a unauthozrized entry in exclusion zones may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>Image of the event</li> </ul>"},{"location":"scenarios/exclusion-zones/#model-details","title":"Model Details","text":""},{"location":"scenarios/exclusion-zones/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li> <p>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</p> </li> <li> <p>Different angles and perspectives: The dataset includes images captured from different angles and perspectives, such as from above, below, or from the side of subjects</p> </li> <li> <p>Different modes of unauthorized access: The dataset includes images of individuals attempting to gain unauthorized access in different ways, such as climbing over fences, breaking locks, using counterfeit credentials, or attempting to sneak past security personnel.</p> </li> <li> <p>Diversity of individuals: The dataset includes images of individuals from different genders, ages, and ethnicities, to ensure that the AI model is able to accurately detect unauthorized access attempts regardless of the individual's appearance.</p> </li> </ul>"},{"location":"scenarios/exclusion-zones/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team.</p>"},{"location":"scenarios/exclusion-zones/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   10k v2 Ceiling 98% 95% 95%"},{"location":"scenarios/exclusion-zones/#scenario-details","title":"Scenario details","text":"<p>Real-time detection and alerts for different kinds unauthorized access which includes but are not limited to:</p> <ul> <li>When an unauthorized person follows an authorized person through a secure area without proper authorization</li> <li>When an individual lingering around restricted areas without proper authorization</li> <li>Forceful entry </li> <li>Use of counterfeit access credentials</li> <li>Unauthorized access attempts during off-hours</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test exclusion-detection\n\nDownloading models for scenario: exclusion-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: exclusion-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of unauthorized access or forceful entry within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/exclusion-zones/#features","title":"Features","text":"<p>Some potential features of VisionAI for detecting missing fire extinguishers could include:</p> <ul> <li> <p>Lightning Fast and Response Time: Ultra-fast Processing for real-time inference results and feedback (~30 frames per second processing) with customizable telemetry and inference results for your requirements.</p> </li> <li> <p>Scalability and Instant Deployment: Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. </p> </li> <li> <p>Custom Integrations: Our custom smart dashboards and real-time alert/notification systems can be tailored to fit your specific needs be it simple dashboards or complex ERP integrations.</p> </li> <li> <p>Multiple channels for notifications: Employee Role-based notifications and alerts through different omni channels like emails, messages, custom alert systems, etc.</p> </li> <li> <p>Pre-Processing and Privacy by design: Our Pre-processing enhances Image quality before further analysis  While  maintaining data privacy by blurring out faces and other sensitive information present in a frame.</p> </li> </ul>"},{"location":"scenarios/exclusion-zones/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/exclusion-zones/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/fall-and-accident-detection/","title":"Fall &amp; Accident detection","text":"<p>Detect potential collision/accident and wet floor, inspect slip and fall instances with VisionAI.</p>"},{"location":"scenarios/fall-and-accident-detection/#overview","title":"Overview","text":"<p>Fall &amp; Accident computer vision based detection system is designed to detect potential safety hazards in a given environment. The system uses video data from cameras placed in the area to identify a range of potential hazards, including person slip &amp; fall, potential collision/accident, wet floor, debris on the floor, and wet/slippery signs.</p> <p>To detect these hazards, the system uses deep learning-based algorithms to analyze the video data and identify specific patterns and features that correspond to each type of hazard. For example, to detect a person slip &amp; fall, the system may look for sudden changes in movement, unusual body positions, or signs of distress.</p> <p>Similarly, to detect potential collisions or accidents, the system may analyze the movement of people or objects in the environment and identify situations where there is a high likelihood of a collision or other accident occurring.</p>"},{"location":"scenarios/fall-and-accident-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect slip and fall with high accuracy. Additionally, our model trained on real-world images minimizes false-positives or false-negatives.  </p> <p>The cameras scan every frame to ensure there are no accidents related to slip and fall cases. </p> <p>To detect a wet floor, the system may look for areas where there is a significant change in reflectance or texture, which could indicate the presence of moisture.</p> <p>To detect debris on the floor, the system may analyze the texture and shape of objects in the environment and identify items that are out of place or could potentially cause a tripping hazard.</p> <p>Finally, to detect wet/slippery signs, the system may analyze the shape and color of signs in the environment and identify those that indicate a wet or slippery floor.</p> <p>Overall, the system is designed to help improve safety in a range of environments, from factories and warehouses to retail stores and public spaces. By detecting potential hazards in real-time, the system can alert workers or visitors to potential dangers and help prevent accidents and injuries.</p>"},{"location":"scenarios/fall-and-accident-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/fall-and-accident-detection/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world data from different workplaces. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/fall-and-accident-detection/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   72,20 v1 Both(Ceiling and Straight) 95%  93%  85%"},{"location":"scenarios/fall-and-accident-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li> <p>We use existing camera feeds from the premises to detect potential collision/accident and wet floor, monitor and detect occurrences of slip and fall incidents.  </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</p> </li> <li> <p>An alert will be raised, when a potential collision/accident and wet floor is detected and/or occurrence slip and fall instance.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test fall-and-accident-detection\n\nDownloading models for scenario: fall-and-accident-detection\nModel: fall-and-accident-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-fall-and-accident-detection/yolov5s-fall-and-accident-detection-0.0.1.zip\n\n\nStarting scenario: fall-and-accident-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of potential collision/accident and wet floor, slip and fall instances within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/fall-and-accident-detection/#features","title":"Features","text":"<ul> <li> <p>Continuous monitoring: The model continuously monitors the user movements to ensure that they are safe and alert the user or emergency services if necessary. This includes monitoring the user's heart rate, breathing, and other vital signs to detect any signs of distress or injury.</p> </li> <li> <p>Alerting system: The model is able to alert supervisors or managers and/or emergency services when it identifies a range of potential hazards, including person slip &amp; fall, potential collision/accident, wet floor, debris on the floor, and wet/slippery signs.</p> </li> <li> <p>Customization: The model must be customizable to fit the needs of different users. This includes settings for sensitivity, activity recognition, and user-specific parameters such as age, weight, and height.</p> </li> </ul>"},{"location":"scenarios/fall-and-accident-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/fall-and-accident-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/firearms-and-knives/","title":"Firearms And Knives Detection","text":"<p>New Technology aims to improve Firearm Detection and Save Lives with VisionAI.</p> <p> </p> Detection of firearms and knives event"},{"location":"scenarios/firearms-and-knives/#overview","title":"Overview","text":"<p>Firearms detection refers to the use of technology and methods to identify the presence of firearms in a particular location or setting. The goal of firearms detection is to prevent violence and ensure public safety by detecting and responding to the presence of firearms including knives, guns and other weapons.</p> <p>There are various methods and technologies used for firearms detection, including metal detectors, X-ray machines, and millimeter-wave scanners. All these solutions are invasive. </p> <p>With the advent in technology, our VisionAI solution for fire-arms detection is non-invasive in nature and it works by analyzing video footage to detect the presence of firearms or other weapons.</p>"},{"location":"scenarios/firearms-and-knives/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to detect Firearms and knives events by providing real-time video feeds of the factory area. The cameras scan every frame to ensure there is no sign of firearms and knives.</p>"},{"location":"scenarios/firearms-and-knives/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Person brandishing firearm</li> <li>Person brandishing knives</li> </ul>"},{"location":"scenarios/firearms-and-knives/#model-details","title":"Model Details","text":""},{"location":"scenarios/firearms-and-knives/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world firearms and knives events. The dataset consists of images and videos collected from various sources including UC Berkeley Anomaly Detection Dataset, UCF Crime Dataset etc.</p>"},{"location":"scenarios/firearms-and-knives/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team.</p>"},{"location":"scenarios/firearms-and-knives/#model-card","title":"Model card","text":"Dataset size Version Camera support Accuracy Recall F1 score 4230 v7 Ceiling 98.2 95.2 95 <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/firearms-and-knives/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to detect firearms and knives events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people in the camera feed and we monitor whether the person is carrying any firearms and knives.</li> <li>If the person is detected with this event, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test firearms-knives-detection\n\nDownloading models for scenario: firearms-knives-detection\n\n\n\nStarting scenario: firearms-knives-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of firearms and knives within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/firearms-and-knives/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy</p> <p>Trained and Tested to give the best results. Our systems are trained to detect firearms and knives at the earliest detection with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time</p> <p>Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives</p> <p>Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment </p> <p>Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations</p> <p>Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/firearms-and-knives/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/firearms-and-knives/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/hand-wash/","title":"Hand Wash","text":"<p>Enhance hygiene compliance with our cutting-edge hand wash detection model, designed to accurately monitor and promote effective hand washing practices.</p> <p> </p> Detection of hand-wash"},{"location":"scenarios/hand-wash/#overview","title":"Overview","text":"<p>Hand hygiene is critical to preventing the spread of infectious diseases. However, ensuring that individuals properly wash their hands at appropriate times can be challenging, particularly in high-traffic areas. </p> <p>A hand wash detection model can help address this challenge by automatically detecting and monitoring hand washing behaviors, providing real-time feedback and alerts to individuals who may need to improve their hygiene practices. This can enhance overall hygiene compliance, reduce the spread of germs and diseases, and promote a safer and healthier environment for all.</p>"},{"location":"scenarios/hand-wash/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based hand-wash system is designed to detect and ensure no one misses hand wash. The system uses image processing and machine learning algorithms to analyze the hand region in images or videos and detect hand wash based on specific features.</p> <p>Overall, the hand wash detection model is an important tool for promoting hygiene and preventing the spread of disease in a range of environments, from hospitals and schools to offices and public spaces. By detecting whether people have used hand wash, the system can help encourage good hygiene practices and reduce the risk of infection.</p>"},{"location":"scenarios/hand-wash/#model-details","title":"Model Details","text":""},{"location":"scenarios/hand-wash/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/hand-wash/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   7326 v5 Straight 79% 84% 76%"},{"location":"scenarios/hand-wash/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor and detect the instances of missing hand wash.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>An alarming system is in place as part of an hand wash detection solution. </p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test hand-wash-detection\n\nDownloading models for scenario: hand-wash-detection\nModel: hand-wash-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-hand-wash-detection/yolov5s-hand-wash-detection-0.0.4.zip\n\n\nStarting scenario: hand-wash-detection..\n</code></pre> </li> <li> <p>You should be able to see the information generated on your console window with the detections of missing hand-wash event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/hand-wash/#features","title":"Features","text":"<ul> <li> <p>Hand region detection: The system should be able to accurately detect the hand region in an image or video, which can be done using skin color segmentation or hand detection algorithms.</p> </li> <li> <p>Real-time performance: The system should be able to operate in real-time, analyzing images or videos quickly and accurately to detect whether a person has missed hand wash or not.</p> </li> <li> <p>Robustness: The system should be able to perform well under varying conditions, such as different lighting conditions, hand positions, or hand appearances due to age, skin color, or skin conditions.</p> </li> </ul>"},{"location":"scenarios/hand-wash/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/hand-wash/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/hazard-warning-suite/","title":"Hazard warning Suite","text":"<p>A \"hazard warning suite\" could refer to a set of scenarios that are designed to monitor, detect, and alert individuals or systems of potential hazards in a given environment. The suite could include various types of sensors and technologies, such as cameras, microphones, and environmental sensors, that can detect hazards like fires, toxic gas leaks, chemical spills, and other potential threats.</p> <p>The suite also include artificial intelligence techniques that can analyze sensor data and detect patterns or anomalies that could indicate the presence of a hazard. Once a hazard is detected, the suite could automatically trigger alarms, notifications, or alerts to relevant personnel, emergency services, or other systems to take appropriate action.</p> <p>In addition to real-time hazard detection and alerts, a hazard warning suite also include provisions for post-event analysis and reporting. This could help organizations identify patterns of hazards and develop strategies to prevent or mitigate future incidents.</p> <p>Overall, a hazard warning suite can be an important tool for ensuring the safety of workers, visitors, and the environment in a variety of settings, such as factories, warehouses, research facilities, and other high-risk environments.</p> <ul> <li>Smoke and Fire Detection</li> <li>No smoking/no vaping</li> <li>Spills &amp; leaks detection</li> <li>Missing fire extinguisher</li> <li>Blocked exit monitoring</li> <li>Equipment rust and corrosion</li> </ul>"},{"location":"scenarios/intrusion-detection/","title":"Intrusion Detection","text":"<p>An advanced physical intrusion detection system powered with Computer Vision and AI.</p>"},{"location":"scenarios/intrusion-detection/#overview","title":"Overview","text":"<p>Intrusion detection is a system to monitor suspicious activities indicating an intrusion. Intrusion detection is important to detect and prevent unauthorized access to the facility, production area and other restricted areas within a building or manufacturing setup. An intrusion detection system helps prevent theft and vandalism, protecting the facility from malicious activities that can disrupt production, damage equipment, and compromise sensitive information. In addition, an effective intrusion detection system helps to ensure compliance with regulations and standards related to security and safety in the industry.</p> <p>The current physical intrusion detection systems have a limited detection range and a number of other problems.  Existing solutions: - Do not provide comprehensive coverage across a large sprawling area. - Can be affected by environmental factors in the outdoors and raise false alarms. - Are vulnerable to tampering. - Do not offer much flexibility in terms of getting integrated well with other security measures like video surveillance not cost-effective. </p>"},{"location":"scenarios/intrusion-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Expand your security capabilities with VisionAI, a modern AI and ML solution to detect intrusions and protect your premises against all potential physical intrusion threats. Our model can accurately identify suspicious behaviors that may indicate a physical intrusion and instantly alerts the security personnel, allowing them to take appropriate and timely action against it to avoid associated dangers.</p> <p>Our smart solution seamlessly integrates with the existing camera infrastructure and analyzes the real-time video feed. It can help in different ways, offers a comprehensive solution to detect all forms of physical intrusion, and ensures compliance with all security and safety regulations.</p>"},{"location":"scenarios/intrusion-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/intrusion-detection/#dataset","title":"Dataset","text":"<p>The dataset consists of high-quality images and videos collected from diverse sources and is designed to reflect real-world scenarios. The dataset is representative of all types of environments that should be taken care of while detecting physical intrusion.  </p> <ul> <li> <p>Different locations/environments \u2013 The dataset includes images/videos from a variety of indoor and outdoor environments, locations, weather conditions and building layouts.</p> </li> <li> <p>Diversity of intruders \u2013 The dataset considers images and videos of intruders from different backgrounds trying to intrude in various poses such as standing, walking, running, crawling etc. </p> </li> <li> <p>Balanced - The dataset is evenly distributed and balanced between intrusion and non-intrusion examples to prevent bias towards one class of data.</p> </li> <li> <p>Different angles and perspectives - The dataset includes images and videos captured from different angles and lighting conditions to ensure the model can detect intrusion in various real-world scenarios.</p> </li> </ul>"},{"location":"scenarios/intrusion-detection/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   2326 v5 Ceiling 65%  71%  71%"},{"location":"scenarios/intrusion-detection/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for intrusion detection can be deployed to a physical perimeter integrated with the existing camera infrastructure and works in different scenarios to detect physical intrusion. The model is equipped to detect the following;</p> <ul> <li>Any person with suspicious behavior</li> <li>Anybody trying to intrude in a building or perimeter</li> <li>Anybody trying to trespass a restricted area without permission</li> <li>Anybody loitering around a restricted area for an extended period </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test intrusion-detection\n\nDownloading models for scenario: intrusion-detection\nModel: intrusion-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-intrusion-detection/yolov5s-intrusion-detection-0.0.1.zip\n\n\nStarting scenario: intrusion-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with detection of intrusions within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/intrusion-detection/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The model can be deployed to a physical perimeter integrated with the existing camera infrastructure and works in real-time to detect physical intrusion.</p> </li> <li> <p>Comprehensive coverage: The model can detect intrusion in a variety of environments, locations, weather conditions and building layouts.</p> </li> <li> <p>Customizable: The model can be customized to detect intrusion in a specific area or location based on user requirements.</p> </li> </ul> <p>-Integration: The model can be integrated with other security measures like video surveillance to provide a comprehensive solution to detect all forms of physical intrusion.</p> <ul> <li>Alert system: The solution has an alert system to notify the security personnel in case of an intrusion.</li> </ul>"},{"location":"scenarios/intrusion-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/intrusion-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/loitering/","title":"Loitering Detection","text":"<p>Keeping Public Spaces Safe: Innovations in Loitering Detection and Prevention with Vision AI.</p> <p> </p> Loitering detection event"},{"location":"scenarios/loitering/#overview","title":"Overview","text":"<p>Loitering detection refers to the use of technology to identify and monitor individuals who are loitering in public spaces, with the aim of preventing crime, reducing security risks, and maintaining public safety. Loitering is typically defined as lingering or remaining in a particular location for an extended period of time, without a legitimate reason to be there.</p> <p>Loitering detection technologies may include sensors, cameras, and other monitoring systems that can detect and track individuals in public spaces. Some of these technologies can be integrated with machine learning and artificial intelligence (AI) algorithms to analyze data and identify patterns of behavior that may be indicative of loitering.</p> <p>Loitering detection technologies can be used in a variety of settings, including transportation hubs, shopping centers, and other public areas where large groups of people may congregate. These technologies can help identify potential security threats, such as individuals who may be carrying weapons or engaging in suspicious activities.</p> <p>However, there are also concerns about privacy and civil liberties when it comes to the use of loitering detection technologies. Critics argue that these technologies can be used to target marginalized communities, and may contribute to a climate of suspicion and discrimination.</p> <p>Overall, the use of loitering detection technologies is a complex issue that requires careful consideration of both security and privacy concerns. While these technologies can play an important role in maintaining public safety, it is important to ensure that their use is balanced with respect for individual rights and freedoms.</p>"},{"location":"scenarios/loitering/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to for the detection of loitering events by providing real-time video feeds of the factory area. The cameras scan every frame and raise an event when a person enters from an usually closed location, person detected during off-hours, person detected for extended duration of time.</p>"},{"location":"scenarios/loitering/#model-details","title":"Model Details","text":""},{"location":"scenarios/loitering/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world loitering detection events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/loitering/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   8280 v1 Both(Ceiling and Straight) 85.0%  81.7%  79.0%"},{"location":"scenarios/loitering/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises for raising loitering events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>From the camera feed we monitor if a person enters from an usually closed location, person detected during off-hours, person detected for extended duration of time.</li> <li>If loitering event is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test loitering-detection\n\nDownloading models for scenario: loitering-detection\n\n\nStarting scenario: loitering-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of loitering within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/loitering/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Customization: Our systems are customizable to fit your needs. We can train our models with your own data and provide you with a custom solution. We also provide a custom license for our software if you wish to use it in a closed environment.</p> </li> <li> <p>Unmatched accuracy: Trained and Tested to give the best results. Our systems are trained to detect loitering events with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time: Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Scalability and Deployment: Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> </ul>"},{"location":"scenarios/loitering/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/loitering/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/max-occupancy-count/","title":"Maximum Occupancy","text":"<p>Transform the way you manage occupancy in real-time with our cutting-edge Computer Vision Occupancy Monitoring Solution.</p> <p> </p> Maximum Occupancy monitoring event"},{"location":"scenarios/max-occupancy-count/#overview","title":"Overview","text":"<p>Effective crowd management is critical for many workplaces like airports, hospitals, factories, and retail shops, among others. One key aspect is maintaining compliance with maximum occupancy limits which is  crucial for maintaining safety, mitigating potential injuries, and legal issues. </p> <p>Existing solutions for tracking maximum occupancy typically rely on manual monitoring, which can be labor-intensive, prone to errors, and time-consuming while other systems such as sensors and RFID (Radio-Frequency Identification) tags produce a lot of false readings, have limited range and incur significant expenses.</p> <p>As such, there is a need for more efficient and reliable methods for monitoring and managing maximum occupancy. A promising solution lies in the use of computer vision technology, which can accurately detect and track individuals in real-time, providing an automated and seamless approach to crowd management.</p>"},{"location":"scenarios/max-occupancy-count/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Maintain workplace occupancy limits flawlessly by leveraging computer vision-powered occupancy monitoring. Monitor Occupancy levels in Real-Time, get instant alerts and warnings whenever count exceeds threshold limit. With our ready-to-deploy models, businesses can effortlessly adhere to regulations and maintain a safe environment without the need for manual monitoring or complex sensor installations. </p> <p>A single camera can cover a wide area, allowing businesses to leverage our AI-based technology with minimal effort. You can easily augment your existing infrastructure and get started with our models with just a few clicks.</p> <p> </p> monitoring maximum occupancy"},{"location":"scenarios/max-occupancy-count/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Person count exceeds limit (Max occupancy exceeded)</li> <li>Person count is below limit (Max occupancy not exceeded)</li> </ul>"},{"location":"scenarios/max-occupancy-count/#event-data","title":"Event Data","text":"<p>An event data for maximum occupancy scenario may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>type of event (Max occupancy exceeded, etc.)</li> <li>Image of the event</li> </ul>"},{"location":"scenarios/max-occupancy-count/#model-details","title":"Model Details","text":""},{"location":"scenarios/max-occupancy-count/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos of people in different scenarios.    </p>"},{"location":"scenarios/max-occupancy-count/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   2470 v1 Both(Ceiling and Straight) 85.0%  78.6%  71.0%"},{"location":"scenarios/max-occupancy-count/#scenario-details","title":"Scenario details","text":"<p>Real-time detection and alerts for different scenarios includes but are not limited to:</p> <ul> <li>When person count exceeds the predefined limit</li> <li>When the threshold is about to be reached as a safety warning</li> <li>Warnings based on population flow</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test max-occupancy\n</code></pre> <p>Downloading models for scenario: max-occupancy Model: max-occupancy: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip</p> <p>Starting scenario: max-occupancy..</p> <p>```</p> </li> <li> <p>You should be able to see the events generated on your console window with the detections of maximum occupancy event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/max-occupancy-count/#features","title":"Features","text":"<p>Some potential features of VisionAI for monitoring maximum occupancy include:</p> <ul> <li> <p>Real-time monitoring of maximum occupancy: VisionAI can monitor maximum occupancy in real-time, providing an automated and seamless approach to crowd management.</p> </li> <li> <p>Instant alerts and warnings: VisionAI can send instant alerts and warnings whenever count exceeds threshold limit.</p> </li> <li> <p>Easy to deploy: VisionAI can be easily deployed with minimal effort, allowing businesses to leverage our AI-based technology with minimal effort.</p> </li> </ul>"},{"location":"scenarios/max-occupancy-count/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/max-occupancy-count/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/missing-fire-extinguisher/","title":"Missing Fire Extinguisher","text":"<p>Strengthen your smoke &amp; fire detection compliance - through adding custom logic for checking for missing fire extinguisher from required places.</p> <p> </p> MMissing Fire extinguisher"},{"location":"scenarios/missing-fire-extinguisher/#overview","title":"Overview","text":"<p>Fire Extinguishers prove to be a crucial preventive measure against unexpected fires. These are essential components of safety features that can help contain early fires before they escalate into large ones. Adequately installed fire extinguishers in the building offer round-the-clock protection against unexpected fires, and a majority of fires can be put out using handy fire extinguishers.</p> <p>However, if a fire breaks out, missing fire extinguishers can increase the risk of injury and damage and can also have legal and regulatory obligations. Failure to comply with regulations can incur fines or legal issues. It also raises a question about the business\u2019s reputation and leads to a loss of trust from customers. The existing fire warning and safety systems also cannot identify early fire signs, and none of them offer detection of missing fire extinguishers.</p>"},{"location":"scenarios/missing-fire-extinguisher/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Make your workplace safer with our VisionAI monitoring, a computer vision and deep learning-based solution that helps you detect missing fire extinguishers by analyzing visual data, making it easier for businesses to ensure that they have the necessary safety equipment in place.</p> <p>Our fully automated system guards your facility 24/7. It sends instant alerts whenever a missing fire extinguisher is detected, allowing businesses to achieve improved fire safety, compliance with regulations, cost savings, and peace of mind. </p>"},{"location":"scenarios/missing-fire-extinguisher/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Missing fire extinguisher</li> </ul> <p>It is recommended that any instance of a missing fire extinguisher be reported to the appropriate authority. An event data for a missing fire extinguisher may include information such as:</p> <ul> <li>Date and time the missing fire extinguisher was discovered</li> <li>Location of the missing fire extinguisher, including the building, floor, and room number</li> <li>Type of fire extinguisher that is missing</li> </ul>"},{"location":"scenarios/missing-fire-extinguisher/#model-details","title":"Model Details","text":""},{"location":"scenarios/missing-fire-extinguisher/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with;</p> <ul> <li> <p>Different locations: Different locations within an industrial setting where fire extinguishers are usually installed, like emergency exits, heavy machinery, near combustible material etc., all have been considered within the dataset.</p> </li> <li> <p>Different angles and perspectives: The dataset includes images captured from different angles and perspectives, such as from above, below, or from the side, in a crowded space or fire extinguishers obscured behind other objects in different locations.</p> </li> <li> <p>Different lighting conditions: The dataset includes images in different lighting conditions, like where the fire extinguisher is clearly visible, partially visible or obstructed.</p> </li> <li> <p>Different classes: The dataset is balanced between the two classes, present and missing fire extinguishers, to avoid bias in the model.  </p> </li> </ul>"},{"location":"scenarios/missing-fire-extinguisher/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   4726 v1 Ceiling 91.0%  89.6%  84.0%"},{"location":"scenarios/missing-fire-extinguisher/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution detects missing fire extinguishers in different scenarios within an industrial setting where the presence of fire extinguishers is expected. These scenarios can be;</p> <ul> <li> <p>Fire extinguishers are generally wall or pillar mounted. Our model is trained to detect the missing fire extinguishers in these locations.</p> </li> <li> <p>Our state-of-the-art models can detect missing fire extinguishers near hazardous/combustible material inside a manufacturing plant.</p> </li> <li> <p>There are specific equipment or machinery that require the availability of fire extinguishers in close proximity for safety reasons. Our model can identify any such space if a fire extinguisher is missing.</p> </li> <li> <p>Also, the model can detect missing fire extinguishers near emergency exits, where they are installed for quick access in case of fire.  </p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test miss-fire-exting-detection\n\nDownloading models for scenario: miss-fire-exting-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: miss-fire-exting-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/missing-fire-extinguisher/#features","title":"Features","text":"<p>Some potential features of VisionAI for detecting missing fire extinguishers could include:</p> <ul> <li> <p>Continuous monitoring: An system could continuously monitor fire extinguisher locations and track any changes in real-time, enabling prompt detection of missing fire extinguishers.</p> </li> <li> <p>Location tracking: The system could use sensors or other location tracking devices to monitor the precise location of fire extinguishers and track any movements or changes in their location.</p> </li> <li> <p>Alerts and notifications: When a missing fire extinguisher is detected, the system could automatically generate an alert or notification to the appropriate personnel or authorities, enabling prompt corrective action.</p> </li> <li> <p>Historical data analysis: Over time, the system could collect and analyze historical data on fire extinguisher locations, enabling identification of trends or patterns that may indicate underlying fire safety issues.</p> </li> </ul> <p>Note</p> <p>Overall, an AI-based system for detecting missing fire extinguishers could enable more proactive and efficient fire safety monitoring and management, helping to prevent fires and ensure the safety of occupants and property.</p>"},{"location":"scenarios/missing-fire-extinguisher/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/missing-fire-extinguisher/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-food-or-drinks/","title":"No Food, No Drinks","text":"<p>An easier, smarter way to enforce policies with VisionAI</p> <p> </p> Event: No food, No drinks"},{"location":"scenarios/no-food-or-drinks/#overview","title":"Overview","text":"<p>Implementing a \u2018No Food, No Drinks\u2019 policy can be challenging. However, for some industries like healthcare, manufacturing, textiles, laboratories and pharmaceuticals, it is imperative to have an effective \u2018no food, no drinks\u2019 policy to maintain strict hygiene and safety standards essential to prevent product contamination. Unfortunately, the current mechanisms rely on manual inspections, are highly human-oriented, and are difficult to automate, depending on enforcement by supervisors and security personnel.</p> <p>Manual inspections can be inconsistent and subjective. Also, humans are prone to errors; they may miss food or drink items that are not easily visible. Furthermore, manual inspections may not be able to cover all areas of the workplace, and they can create privacy concerns for employees. All these factors can compromise the effectiveness of the policy and may lead to non-compliance.</p>"},{"location":"scenarios/no-food-or-drinks/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Implement VisionAI solution to address these problems by providing consistent, objective, and cost-effective enforcement while minimizing privacy concerns. Our smart solution seamlessly integrates with your existing camera infrastructure to capture all areas where food and drinks may be present. The model works by detecting and identifying individuals carrying or consuming foods or beverages. In addition, the model is trained to recognize different types of foods and drinks and the actions associated with consuming them, such as holding a cup or bottle and lifting it to the mouth, chewing, swallowing etc.</p> <p>The model analyzes the video feed in real-time and works with the greatest accuracy. It instantly alerts the appropriate personnel to take action and proves to be an effective tool for enforcing a \u2018no food and drinks\u2019 policy in an organizational setting, improving hygiene and safety measures, and ensuring compliance with regulations.</p>"},{"location":"scenarios/no-food-or-drinks/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-food-or-drinks/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. The dataset is representative of the types of people, settings, and situations where the policy will be enforced. It is evenly distributed with:</p> <ul> <li> <p>Different locations: Different locations within an industrial setting where food and drinks are not allowed, like production areas, offices or workstations, and storage areas, have been considered within the dataset.</p> </li> <li> <p>Different angles and perspectives: The dataset includes images captured from different angles and perspectives, such as a front-facing view to have a clear view of the person for any food item they may be carrying, a top-down view for when the person is seated or when food or drinks are on a table, side view, low/high angle view and oblique view to detect from a diagonal or slanted perspective. An oblique view is useful when carrying food or drinks in a bag or container.</p> </li> <li> <p>Different lighting conditions: The dataset includes images of lighting conditions, like where the food items are partially visible or obstructed.</p> </li> <li> <p>Different versions: The images in the dataset have variations in the appearance of people, food, and drinks, so the model can learn to recognize them in different contexts.</p> </li> </ul>"},{"location":"scenarios/no-food-or-drinks/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   3220 v1 Both(Ceiling and Straight) 75.0%  81.6%  84.0%"},{"location":"scenarios/no-food-or-drinks/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for detecting food items and drinks works in different scenarios within an industrial setting. Our model can be deployed at the entrance/exit points or inside to monitor and see whether employees or visitors carry food items or beverages. The model is equipped to detect the following: - Person carrying a food item - Person carrying any beverage - Any spill event taking place within the specified area</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-food-or-drinks\n\nDownloading models for scenario: no-food-or-drinks\nModel: no-food-or-drinks: https://workplaceos.blob.core.windows.net/models/yolov5s-no-food-or-drinks/yolov5s-no-food-or-drinks-0.0.1.zip\n\n\nStarting scenario: no-food-or-drinks..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with detection of food items and drinks within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/no-food-or-drinks/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The model analyzes the video feed in real-time and works with the greatest accuracy. It is an effective tool for enforcing a \u2018no food and drinks\u2019 policy in an organizational setting, improving hygiene and safety measures, and ensuring compliance with regulations.</p> </li> <li> <p>Cost-effective: The model is cost-effective and does not require any additional hardware or software. It can be deployed with existing camera infrastructure.</p> </li> <li> <p>Privacy: The model is designed to protect the privacy of employees and visitors. It does not capture or store any personal information, and it does not require any personal information to be provided by the user.</p> </li> <li> <p>Customizable: The model can be customized to suit the needs of the user. It can be trained with custom data to detect and recognize different types of food items and drinks.</p> </li> </ul>"},{"location":"scenarios/no-food-or-drinks/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/no-food-or-drinks/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-phone-usage/","title":"Mobile Phone Usage Detection","text":"<p>Enabling businesses to overcome digital distractions and misuse of mobile phones at workplaces.</p>"},{"location":"scenarios/no-phone-usage/#overview","title":"Overview","text":"<p>Mobile phones at workplaces are proving to be an insidious way to execute malicious purposes. Prevent industrial espionage and reinforce security measures with the most reliable mobile phone usage detection models powered by AI and Deep Learning.  Visionify's computer vision solutions are more accurate than the conventional methods, can safely detect mobile phone usage (people taking pictures, recording videos/audios, sending texts in prohibited areas). We offer instant integration with your existing camera infrastructure, and quick results. </p>"},{"location":"scenarios/no-phone-usage/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect mobile phone usage by providing real-time video feeds at workplaces. This system can be used to detect an event of workers using mobile phones aganist compliance policies.</p> <p>Enforce any company policies on mobile-usage in the workplace. These camera based detection processes should be supplimented by strong compliance practices. If workers are prohibited from mobile usage, ensure that they are aware of the policy and the consequences of violating it. So, if an employee is found to be using mobile phone, an appropriate action needs to be taken.</p>"},{"location":"scenarios/no-phone-usage/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-phone-usage/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Person class is considered for model building. </p> <p>Basically, COCO is a  large-scale dataset and it provides real-world data representation including:</p> <ul> <li>Indoor vs Outdoor environments</li> <li>Male vs Female</li> <li>Day vs Night</li> <li>Different types of clothing</li> <li>Different distances from the camera</li> <li>Various lighting conditions</li> <li>Various camera angles and resolutions</li> <li>Using seurity camera feeds</li> </ul>"},{"location":"scenarios/no-phone-usage/#model","title":"Model","text":"<p>The model is built using Yolov5 pre-trained model for person and mobile classes. The yolov5 model is used to identify the human body landmarks of the subject.</p>"},{"location":"scenarios/no-phone-usage/#yolov5-model-card","title":"Yolov5 model card","text":"Dataset size Version Camera support Precision Recall  mAP   28421 v1 Straight 84% 85% 81%"},{"location":"scenarios/no-phone-usage/#landmark-detection-model-card","title":"Landmark detection model card","text":"Dataset size Version Camera support Precision Recall  mAP   45181 v1 Straight 84% 72% 84% <p>The model is light-weight enough to be run on any edge devices.</p>"},{"location":"scenarios/no-phone-usage/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: - We use existing camera feeds from the premises to detect mobile phone usage by employees. - VisionAI system is able to run on edge devices. It uses camera feeds for processing.  - We detect the mobile phone usage event and an alert is raised.</p> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test mobile-usage-detection\n\nDownloading models for scenario: mobile-usage-detection\nModel: mobile-usage-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-mobile-usage-detection/yolov5s-mobile-usage-detection-0.0.1.zip\n\n\nStarting scenario: mobile-usage-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with phone usage being detected within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/no-phone-usage/#features","title":"Features","text":"<ul> <li>Unparalleled Accuracy and faster detection: Our models, not only trained to detect the presence of a mobile device but detect its usage, are equipped to achieve an outstanding accuracy of up to 92%.   </li> <li>Seamless integration and Scalability: An end-to-end solution, integrates seamlessly with your existing camera network and is ready to detect. Easily expandable by adding more IP cameras to the network. </li> <li>Integrated Solution: It is an integrated system combining surveillance and mobile phone detection in one system.</li> <li>Absolute Privacy: We understand your concerns about data privacy and take a proactive approach to preserve it. Our models are privacy oriented by design.</li> <li>Automate and Grow: Leverage the precision and power of the groundbreaking computer vision technology, automate complex tasks and detect flaws sooner to achieve better performance and reduced costs.   </li> <li>Versatile Framework: We offer flexibility in deployment; the model can operate at the Edge, in the cloud, or any self-hosted environment. </li> </ul>"},{"location":"scenarios/no-phone-usage/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/no-phone-usage/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-smoking-hazard/","title":"No Smoking/No Vaping","text":"<p>No smoking &amp; No vaping zone enforcements with Vision AI.</p> <p> </p> Detection of Smoking event"},{"location":"scenarios/no-smoking-hazard/#overview","title":"Overview","text":"<p>Smoking and vaping are typically banned in workplaces like manufacturing plants, construction sites, warehouses, chemical plants, etc., an ideal 100% compliance rate can be challenging to achieve. However, it is imperative for employers to ensure that their workplaces are absolutely smoke-free.</p> <p>VisionAI makes it possible to avert workplace hazards and help employers maintain 100% compliance through smart AI solutions. Our next-gen real-time detection systems make sure a fire/smoke or any sign of vaping is detected instantly. These systems are also trained to generate alerts and notifications accordingly.</p>"},{"location":"scenarios/no-smoking-hazard/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to detect smoking/vaping events by providing real-time video feeds of the factory area. The cameras scan every frame to ensure there is no sign of smoking/vaping.</p>"},{"location":"scenarios/no-smoking-hazard/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-smoking-hazard/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world smoking/vaping events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/no-smoking-hazard/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   1785 v2 Both(Ceiling and Straight) 98% 95% 95%"},{"location":"scenarios/no-smoking-hazard/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to detect smoking/vaping events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people in the camera feed and we monitor whether the person is involved in any smoking/vaping activity.</li> <li>If the person is detected with this event, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-smoking-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-smoking-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: no-smoking-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/no-smoking-hazard/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy</p> <p>Trained and Tested to give the best results. Our systems are trained to detect Fire and Smoke at the earliest detection with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time</p> <p>Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives</p> <p>Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment </p> <p>Our models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations</p> <p>Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/no-smoking-hazard/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/no-smoking-hazard/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/no-smoking/","title":"No Smoking/No Vaping","text":"<p>No smoking &amp; No vaping zone enforcements with Vision AI.</p> <p> </p> Detection of Smoking event"},{"location":"scenarios/no-smoking/#overview","title":"Overview","text":"<p>Smoking and vaping are typically banned in workplaces like manufacturing plants, construction sites, warehouses, chemical plants, etc., an ideal 100% compliance rate can be challenging to achieve. However, it is imperative for employers to ensure that their workplaces are absolutely smoke-free.</p> <p>VisionAI makes it possible to avert workplace hazards and help employers maintain 100% compliance through smart AI solutions. Our next-gen real-time detection systems make sure a fire/smoke or any sign of vaping is detected instantly. These systems are also trained to generate alerts and notifications accordingly.</p>"},{"location":"scenarios/no-smoking/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to detect smoking/vaping events by providing real-time video feeds of the factory area. The cameras scan every frame to ensure there is no sign of smoking/vaping.</p>"},{"location":"scenarios/no-smoking/#model-details","title":"Model Details","text":""},{"location":"scenarios/no-smoking/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world smoking/vaping events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/no-smoking/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team.</p>"},{"location":"scenarios/no-smoking/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   1785 v2 Both(Ceiling and Straight) 98% 95% 95%"},{"location":"scenarios/no-smoking/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to detect smoking/vaping events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people in the camera feed and we monitor whether the person is involved in any smoking/vaping activity.</li> <li>If the person is detected with this event, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-smoking-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-smoking-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: no-smoking-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/no-smoking/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy</p> <p>Trained and Tested to give the best results. Our systems are trained to detect Fire and Smoke at the earliest detection with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time</p> <p>Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives</p> <p>Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment </p> <p>Our models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations</p> <p>Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/no-smoking/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/no-smoking/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/occupancy-metrics/","title":"Occupancy Policies","text":"<p>Ensuring security and controlled access at the workplace is vital for organizations, but conventional surveillance and crowd management techniques are complex, expensive, and heavily reliant on human intervention. Furthermore, these systems are often unable to provide desired, foolproof results due to limitations, inaccuracies, and the inability to provide multiple metrics. One of the significant challenges with current systems for workplace crowd management is the constantly evolving workplace dynamics.</p> <p>What\u2019s included under Occupancy policies:</p> <ul> <li>Max occupancy</li> <li>Restricted areas/times</li> <li>Dwell time</li> </ul>"},{"location":"scenarios/occupancy-metrics/#occupancy-metrics","title":"Occupancy Metrics","text":"<p>Track workplace Occupancy Metrics effortlessly.</p> <p>Occupancy metrics</p>"},{"location":"scenarios/occupancy-metrics/#overview","title":"Overview","text":"<p>Measuring occupancy metrics is crucial for businesses because of the valuable insights it offers. By accurately measuring and analyzing occupancy metrics, businesses can make data-driven decisions to improve other aspects of their business, like optimizing the physical layout and resource management.</p>"},{"location":"scenarios/occupancy-metrics/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Track workplace Occupancy Metrics effortlessly with Vision AI monitoring ready-to-deploy model that provides real-time insights and analysis with valuable insights such as:</p> <ul> <li> <p>Occupancy count: This measures the number of people or objects within a given space, such as a room or a parking lot.</p> </li> <li> <p>Dwell time: This measures the amount of time that people spend in a particular location, such as a store or a workspace.</p> </li> <li> <p>Occupancy behavior: This tracks how people move and interact within a space, providing insights into how spaces are being used and how they can be optimized.</p> </li> <li> <p>Occupancy trends: This analyzes occupancy data over time, identifying patterns and trends that can inform business decisions such as staffing, marketing, and capacity planning.</p> </li> <li> <p>Occupancy alerts: This triggers alerts when occupancy levels exceed predetermined thresholds, enabling businesses to take proactive measures to manage crowds and ensure safety.</p> </li> </ul>"},{"location":"scenarios/occupancy-metrics/#model-details","title":"Model Details","text":""},{"location":"scenarios/occupancy-metrics/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li>Images with different subjects</li> <li>Images with different lighting conditions</li> <li>Images captured at different times of the day</li> <li>Images with different camera angles</li> <li>Images with different camera </li> <li>Images with a variety of occlusions, such as people partially hidden behind obstacles or objects.</li> <li>Variation in subjects</li> <li>Images with different environments</li> </ul>"},{"location":"scenarios/occupancy-metrics/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   4726 v1 Ceiling 91.0%  89.6%  84.0%"},{"location":"scenarios/occupancy-metrics/#scenario-and-potential-deployment-area-details","title":"Scenario and Potential Deployment Area Details","text":"<ul> <li> <p>Retail stores: Retail stores can use occupancy metrics to optimize store layouts, improve customer flow, and reduce wait times.</p> </li> <li> <p>Office buildings: Office buildings can use occupancy metrics to optimize workspace layouts, improve traffic flow, and identify underutilized areas.</p> </li> <li> <p>Transportation hubs: Transportation hubs, such as airports and train stations, can use occupancy metrics to optimize passenger flow, reduce wait times, and improve safety.</p> </li> <li> <p>Stadiums and event venues: Stadiums and event venues can use occupancy metrics to optimize seating arrangements, improve traffic flow, and enhance the overall visitor experience.</p> </li> <li> <p>Healthcare facilities: Healthcare facilities can use occupancy metrics to optimize waiting areas, improve patient flow, and ensure compliance with social distancing guidelines.</p> </li> <li> <p>Public spaces: Public spaces, such as parks and city centers, can use occupancy metrics to optimize traffic flow, identify overcrowding, and improve safety.</p> </li> <li> <p>Manufacturing plants: Manufacturing plants can use occupancy metrics to optimize production lines, improve traffic flow, and identify bottlenecks.</p> </li> <li> <p>Parking lots: Parking lots can use occupancy metrics to optimize parking arrangements, reduce wait times, and improve safety.</p> </li> </ul>"},{"location":"scenarios/occupancy-metrics/#features","title":"Features","text":"<ul> <li> <p>Lightning Fast and Response Time: Ultra-fast Processing for real-time inference results and feedback (~30 frames per second processing) with customizable telemetry and inference results for your requirements.</p> </li> <li> <p>Scalability and Instant Deployment: Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. </p> </li> <li> <p>Custom Integrations: Our custom smart dashboards and real-time alert/notification systems can be tailored to fit your specific needs be it simple dashboards or complex ERP integrations.</p> </li> <li> <p>Multiple channels for notifications: Employee Role-based notifications and alerts through different omni channels like emails, messages, custom alert systems, etc.</p> </li> <li> <p>Pre-Processing and Privacy by design: Our Pre-processing enhances Image quality before further analysis  While  maintaining data privacy by blurring out faces and other sensitive information present in a frame.</p> </li> <li> <p>Intelligent Insights: Our Active Continuous Learning creates by-products in the form of intelligent insights, analytics and insightful data that helps you optimize processes and increase efficiency..</p> </li> <li> <p>Hassle-free Data Access: Clients can access and manage data/insights/analytics from anywhere using Cloud services. Further, we create role-based authentication systems for access to data.</p> </li> </ul>"},{"location":"scenarios/occupancy-metrics/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/occupancy-metrics/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/occupancy-metrics/#restricted-areastimes","title":"Restricted areas/times","text":"<p>Secure restricted areas with our powerful AI-based monitoring system that detects and prevents unauthorized access in real-time</p>"},{"location":"scenarios/occupancy-metrics/#overview_1","title":"Overview","text":"<p>Unauthorized access to restricted zones at the workplace can lead to theft, accidents, and other security breaches. Be it valuable assets, sensitive information or providing employee safety, maintaining high-security and controlled access for restricted zones at the workplace is essential for all organizations. </p> <p>Another major problem with existing systems cannot detect intrusion after an unauthorized access has been made.This renders biometric, sensors and security personnels ineffective after an unauthorized access has been already made.</p>"},{"location":"scenarios/occupancy-metrics/#vision-ai-based-monitoring_1","title":"Vision AI based monitoring","text":"<p>With our Vision AI monitoring you can authorize access as well as continuous monitor live feeds inside a restricted area for real-time detection of unauthorized personnel. Our fully automated detection models are not only more powerful and accurate than existing systems but also more affordable and easy to integrate into existing infrastructure allowing users to scale the power of i-based real-time detection with a few simple clicks.</p> <p> </p> Detection of unauthorized entry event"},{"location":"scenarios/occupancy-metrics/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Person detected in restricted area</li> <li>Movement detected in restricted area</li> <li>Person detected after hours</li> <li>Movement detected after hours\"</li> </ul> <p>It is recommended that any instance of unauthorized entry be reported to the appropriate authority. An event data for a unauthozrized entry in exclusion zones may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> <li>Image of the event</li> </ul>"},{"location":"scenarios/occupancy-metrics/#model-details_1","title":"Model Details","text":""},{"location":"scenarios/occupancy-metrics/#dataset_1","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li> <p>Different environments: Both indoor and outdoor with varying/contrasting surrounding and infrastructure details</p> </li> <li> <p>Different angles and perspectives: The dataset includes images captured from different angles and perspectives, such as from above, below, or from the side of subjects</p> </li> <li> <p>Different modes of unauthorized access: The dataset includes images of individuals attempting to gain unauthorized access in different ways, such as climbing over fences, breaking locks, using counterfeit credentials, or attempting to sneak past security personnel.</p> </li> <li> <p>Diversity of individuals: The dataset includes images of individuals from different genders, ages, and ethnicities, to ensure that the AI model is able to accurately detect unauthorized access attempts regardless of the individual's appearance.</p> </li> </ul>"},{"location":"scenarios/occupancy-metrics/#model-card_1","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   2326 v5 Ceiling 65%  71%  71%"},{"location":"scenarios/occupancy-metrics/#scenario-details","title":"Scenario details","text":"<p>Real-time detection and alerts for different kinds unauthorized access which includes but are not limited to:</p> <ul> <li>When an unauthorized person follows an authorized person through a secure area without proper authorization</li> <li>When an individual lingering around restricted areas without proper authorization</li> <li>Forceful entry </li> <li>Use of counterfeit access credentials</li> <li>Unauthorized access attempts during off-hours</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test exclusion-detection\n\nDownloading models for scenario: exclusion-detection\nModel: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: exclusion-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of unauthorized access or forceful entry within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/occupancy-metrics/#features_1","title":"Features","text":"<p>Some potential features of VisionAI for detecting missing fire extinguishers could include:</p> <ul> <li> <p>Lightning Fast and Response Time: Ultra-fast Processing for real-time inference results and feedback (~30 frames per second processing) with customizable telemetry and inference results for your requirements.</p> </li> <li> <p>Scalability and Instant Deployment: Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. </p> </li> <li> <p>Custom Integrations: Our custom smart dashboards and real-time alert/notification systems can be tailored to fit your specific needs be it simple dashboards or complex ERP integrations.</p> </li> <li> <p>Multiple channels for notifications: Employee Role-based notifications and alerts through different omni channels like emails, messages, custom alert systems, etc.</p> </li> <li> <p>Pre-Processing and Privacy by design: Our Pre-processing enhances Image quality before further analysis  While  maintaining data privacy by blurring out faces and other sensitive information present in a frame.</p> </li> </ul>"},{"location":"scenarios/occupancy-metrics/#training-with-custom-data_1","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/occupancy-metrics/#contact-us_1","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/occupancy-policies/","title":"Occupancies Policies","text":"<p>Ensuring security and controlled access at the workplace is vital for organizations, but conventional surveillance and crowd management techniques are complex, expensive, and heavily reliant on human intervention. Furthermore, these systems are often unable to provide desired, foolproof results due to limitations, inaccuracies, and the inability to provide multiple metrics.</p> <p>One of the significant challenges with current systems for workplace crowd management is the constantly evolving workplace dynamics. For instance, frequent changes in access requirements can make it difficult to reconfigure one-time installation measures and create a tedious and time-consuming task. As a result, these systems may be limited in terms of flexibility, adaptability, and scalability, making them less effective in certain situations and unable to adapt to changing needs or circumstances. This challenge highlights the need for more dynamic and adaptable solutions to accommodate the evolving nature of workplaces.</p>"},{"location":"scenarios/occupancy-policies/#visionifys-workplace-safety-suite-for-occupancy-policies","title":"Visionify\u2019s Workplace Safety Suite for Occupancy Policies","text":"<p>Enhance the safety and intelligence of your workplace with our cutting-edge VisionAI suite designed for effective crowd management. Our Crowd Management suite offers a complete solution set that helps you regulate access, enforce security policy adherence and deter intruders effectively. Our fully automated and ready-to-deploy models enable real-time monitoring for the detection of unauthorized access attempts, ensuring that your facility is guarded 24/7. Instant alerts are sent to prevent security breaches before they happen. Our system guarantees reliable detection and can be seamlessly integrated with your existing camera infrastructure, making it easy to scale your system with just a few clicks.</p> <p>What\u2019s included in this suite:</p> <ul> <li>Max occupancy</li> <li>Restricted areas/times</li> <li>Dwell time</li> <li>Authorized personnel</li> </ul>"},{"location":"scenarios/ppe-detection/","title":"PPE Detection","text":"<p>Prevent Workplace Injuries and Occupational Hazards with Vision AI</p>"},{"location":"scenarios/ppe-detection/#overview","title":"Overview","text":"<p>Personal protective equipment, or \"PPE,\" is the clothing worn to reduce exposure to risks that might result in significant workplace diseases and injuries. Contact with chemical, radioactive, physical, electrical, mechanical, or other job hazards may cause these wounds and illnesses. Items like gloves, safety goggles, shoes, earplugs or muffs, hard hats, respirators, coveralls, vests, and full-body suits are examples of personal protection equipment. Accidents and injuries due to employees not wearing PPE hold business owners/employers legally accountable. Apart from legal damages, accidents due to PPE negligence result in loss of time, reduced productivity, costly worker compensations, etc.</p> <ul> <li> <p>Workplace Fatalities are rising, and employers cannot afford to tolerate PPE negligence. According to the Bureau of Labor Statistics, there were 5,190 fatal work injuries recorded in the United States in 2021, an 8.9-percent increase from 4,764</p> </li> <li> <p>Speaking about one of the most injury-susceptible body parts - hands, OSHA reports that almost 70 percent of hand and arm injuries could be prevented with personal protective equipment, specifically safety gloves. Yet, 70 percent of workers don\u2019t wear hand protection, and 30 percent don\u2019t wear the right kind of glove for the task.</p> </li> <li> <p>If we consider one of the leading causes of workplace fatalities - Head Injuries a study by BLO found that 84% of head injuries at a worksite were due to the absence of safety helmets. A percentage which not small considering there were 68,170 head injuries in 2021</p> </li> </ul> <p>Therefore, complete compliance is necessary since even a brief lapse in usage can prove fatal. To monitor PPE compliance, cameras can be used.</p>"},{"location":"scenarios/ppe-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to monitor PPE compliance by providing real-time video feeds of the factory unit. The cameras scan every frame to ensure there's no PPE negligence, eliminating occupational hazards and serious injuries.</p> <p>To ensure accuracy and reliability, these camera-based monitoring services should be supplemented by strong compliance processes. Furthermore, workers working in different factory units should always be made aware of PPE compliance practices.</p>"},{"location":"scenarios/ppe-detection/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on PPE detection algorithms that are currently in practice.</p> <p>The dataset is made up of images and videos gathered from various sources. The dataset has been catalogued to ensure real-world intricacies. It has an even distribution of:</p> <ul> <li>Different(indoor/outdoor) environments</li> <li>Male vs Female</li> <li>Variations in PPE suits</li> <li>Variations in gloves, helmet, goggles, safety-vest design</li> <li>Different light settings</li> <li>Variations in camera orientations</li> <li>Using security camera feeds</li> </ul>"},{"location":"scenarios/ppe-detection/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team.</p>"},{"location":"scenarios/ppe-detection/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   1836 v8 Both(Ceiling and Straight) 88%  86%  91%  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/ppe-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to monitor the compliance of PPE in the workplace.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people in the camera feed and we monitor whether the person is wearing safety gloves, goggles, helmet, mask, safety-shoes and vest or not.</li> <li>If the person is detected without safety gloves, goggles, helmet, mask, safety-shoes and vest, an alert is raised.</li> </ul>"},{"location":"scenarios/ppe-detection/#events-supported","title":"Events Supported","text":"<p>This scenario supports the following event:</p> <ul> <li>PPE detected: This event is generated when a PPE including goggles, gloves, helmet, mask, safety-shoes, vest, etc. are detected in the camera feed.</li> </ul>"},{"location":"scenarios/ppe-detection/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The solution enables you to monitor PPE compliance events in real-time, using camera feeds from the premises.</p> </li> <li> <p>Alert system: The solution provides an alert system to notify the concerned authorities in case of PPE negligence.</p> </li> <li> <p>Customizable: The solution is customizable to suit your needs. You can customize the solution to monitor PPE compliance in your workplace.</p> </li> </ul>"},{"location":"scenarios/ppe-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/ppe-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/rust-and-corrosion-hazard/","title":"Equipment monitoring","text":""},{"location":"scenarios/rust-and-corrosion-hazard/#rust-and-corrosion-detection","title":"Rust and Corrosion Detection","text":"<p>Ensure the safety of employees by inspecting machine equipment for the presence of rust/corrosion. </p> <p></p>"},{"location":"scenarios/rust-and-corrosion-hazard/#overview","title":"Overview","text":"<p>Visual inspection of industrial environments is a common requirement across heavy industries, such as transportation, construction, and shipbuilding, and typically requires qualified experts to perform the inspection. Inspection locations can often be remote or in adverse environments that put humans at risk, such as bridges, skyscrapers, and offshore oil rigs. </p> <p>Many of these industries deal with huge metal surfaces and harsh environments. A common problem across these industries is metal corrosion and rust. Although corrosion and rust are used interchangeably across different industries (we also use the terms interchangeably in this post), these two phenomena are different. </p> <p>Visionify\u2019s AI Vision Model for Rust/Corrosion Detection is designed to detect instances of rust/corrosion if any in machine parts, manufacturing equipments etc. </p>"},{"location":"scenarios/rust-and-corrosion-hazard/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Rust or corrosion event detected</li> </ul>"},{"location":"scenarios/rust-and-corrosion-hazard/#model-details","title":"Model Details","text":""},{"location":"scenarios/rust-and-corrosion-hazard/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on rust/corrosion detection algorithms. The dataset is made up of images and videos gathered from various sources where instances of rust were found. The dataset has been catalogued to ensure real-world situations. It has an even distribution of:</p> <ul> <li>Variations of pieces of equipment</li> <li>Different(indoor/outdoor) environments</li> <li>Different rust severity</li> <li>Variations in camera orientations</li> <li>Using security camera feeds</li> </ul> <p>Total number of images used was 5572.</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team.</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   2326 v5 Straight 79% 49% 56% <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the equipments in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect instances of rust/corrosion if any in machine parts, manufacturing equipments.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test rust-detection\n\nDownloading models for scenario: rust-detection\nModel: rust-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: rust-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of smoking/vaping event within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/rust-and-corrosion-hazard/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/sexual-harassment/","title":"Sexual Harassment","text":"<p>Create a safer and more respectful workplace for all employees</p> <p> </p> Monitoring of Sexual Harassment event"},{"location":"scenarios/sexual-harassment/#overview","title":"Overview","text":"<p>Sexual harassment in the workplace is a serious issue that can have significant negative impacts on employees' mental health, job satisfaction, and overall well-being. In addition, it can also result in decreased productivity and increased turnover, leading to financial costs for the company.</p> <p>Implementing a sexual harassment detection model can also send a clear message to employees that the organization takes sexual harassment seriously and is committed to creating a safe and respectful workplace for all. This can help to foster a culture of respect and trust within the organization, which can have positive impacts on employee morale and overall job satisfaction.</p> <p>Overall, a sexual harassment detection model can help organizations to create a safer and more respectful workplace for all employees, which can lead to increased productivity, reduced turnover, and improved organizational outcomes.</p>"},{"location":"scenarios/sexual-harassment/#visionai-based-monitoring","title":"VisionAI Based Monitoring","text":"<p>Sexual harassment detection using VisionAI based solution can help to prevent inappropriate behavior and create safer environments. Our models can analyze video footage or images and identify patterns or actions that are indicative of sexual harassment.</p> <p>To detect sexual harassment our model is trained on a dataset of video footage or images with labeled instances of sexual harassment. The algorithm can learn to recognize patterns in the data that are associated with inappropriate behavior, such as physical contact, gestures, or facial expressions.</p> <p>Our trained model now, can be used to analyze live video footage or images in real-time which can further help in enabling alert to authorities or trigger an alarm when it detects suspicious behavior, allowing them to intervene and prevent further harassment.</p>"},{"location":"scenarios/sexual-harassment/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   4126 v5 Both(Ceiling and Straight) 84%  77%  74%"},{"location":"scenarios/sexual-harassment/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/shipping-activity/","title":"Shipping Activity Detection","text":"<p>Stay vigilant even after hours with our advanced suspicious shipping activity solution.</p> <p>Detection of suspicious shipping activity event</p>"},{"location":"scenarios/shipping-activity/#overview","title":"Overview","text":"<p>Shipping activity detection refers to the use of technology to identify and monitor shipping activity that may be indicative of illicit activity. Shipping activity detection technologies may include sensors, cameras, and other monitoring systems that can detect and track shipping activity. Some of these technologies can be integrated with machine learning and artificial intelligence (AI) algorithms to analyze data and identify patterns of behavior that may be indicative of suspicious shipping activity.</p> <p>Shipping activity detection technologies can be used in a variety of settings, including ports, harbors, and other areas where shipping activity may occur. These technologies can help identify potential security threats, such as vessels that may be carrying weapons or engaging in suspicious activities.</p> <p>The Suspicious shipping activity detected from non-designated area and during after-hours model is an important tool to identify potential threats and take appropriate action to mitigate risks.</p>"},{"location":"scenarios/shipping-activity/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI's shipping activity detection solutions can be used to for the detection of suspicious shipping activity events by providing real-time video feeds of the shipping area. The cameras scan every frame and raise an event when a suspicious entry detected from an usually closed location or during off-hours and/or for extended duration of time.</p> <p>Suspicious Shipping Activity Detection model is an important tool for helping to prevent fraudulent or criminal activity in the shipping industry, and it works in real time to help ensure that potentially suspicious activity is identified and addressed as quickly as possible.</p>"},{"location":"scenarios/shipping-activity/#model-details","title":"Model Details","text":""},{"location":"scenarios/shipping-activity/#dataset","title":"Dataset","text":"<p>The dataset of Suspicious shipping activity detected from non-designated area and during after-hours is a collection of data points that provide insights into potential illicit activities taking place in the shipping industry.  One key feature of this dataset is the inclusion of information on shipping activity outside of designated areas and during after-hours. These factors are often indicators of suspicious behavior, as they suggest that the vessel is attempting to avoid detection and operate outside of normal shipping patterns. By analyzing this data, security personnel can identify potential threats and take appropriate action to prevent harm.</p>"},{"location":"scenarios/shipping-activity/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   4126 v5 Both(Ceiling and Straight) 84.0% 87.5% 81.4%"},{"location":"scenarios/shipping-activity/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for Suspicious shipping activity detection works in different scenarios.</p> <ul> <li> <p>The model works by continuously monitoring shipping data from various sources and then analyzes the data to identify patterns and anomalies that may be indicative of suspicious activity.</p> </li> <li> <p>The model may flag a shipment as suspicious if it originates from a non-designated area or if it is being shipped during after-hours.</p> </li> <li> <p>Once the model identifies a potentially suspicious shipment, it can trigger an alert to notify relevant personnel or authorities, who can then  investigate further and take appropriate action as needed.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test shipping-activity-detection\n\nDownloading models for scenario: shipping-activity-detection\nModel: shipping-activity-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-shipping-activity-detection/yolov5s-shipping-activity-detection-0.0.1.zip\n\n\nStarting scenario: shipping-activity-detection..\n</code></pre> </li> <li> <p>You should be able to see the information generated on your console window with suspicious shipping activity detection events within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/shipping-activity/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The solution is designed to monitor shipping data in real-time, allowing for rapid detection and response to suspicious shipping activity. </p> </li> <li> <p>Alert system: The model is programmed to send alerts or notify the security personnel in case of any suspicious shipping activity.</p> </li> <li> <p>Easy to deploy: The solution can be deployed easily with minimal effort and can be integrated with the existing camera infrastructure.</p> </li> <li> <p>Customizable: The solution can be customized to meet the specific requirements of the organization.</p> </li> </ul>"},{"location":"scenarios/shipping-activity/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/shipping-activity/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/slip-and-fall-detection/","title":"Slip and Fall Detection","text":"<p>Ensure the safety of employees by inspecting slip and fall instances. Get real-time alerts when such kind of accidents occur at the workplace. </p>"},{"location":"scenarios/slip-and-fall-detection/#overview","title":"Overview","text":"<p>Slip and Falls are among the leading causes of occupational fatalities. Timely action in the event of a Fall/Slip accident can minimize damage and save lives. However, working alone or in a noisy environment hinders timely assistance.  </p> <p>Here are some situations where a slip-fall detection system would be useful. </p> <ul> <li> <p>Construction sites: Detect slips and falls among workers and alert supervisors to potential hazards. Potential deployment zones include ladders, ramps, and scaffolds.\u202f </p> </li> <li> <p>Working at heights: Falls from higher to lower levels are the most common cause of fatalities in Slip and Fall accidents, making Slip and Fall detection for workers at height vital. </p> </li> <li> <p>Working Alone: Slip and Fall detection is crucial for specific job settings where employees are required to work alone, particularly during off-hours. Deployment areas could be for the following category of workers. </p> </li> <li> <p>Construction Sites: Electricians, plumbers, and HVAC (Heating, ventilation, and air conditioning) technicians work alone on specific tasks where slip and fall would be useful. </p> </li> <li> <p>Oil and gas: The system will be useful for remote location workers such as drill operators, pipeline inspectors, and pump operators working in oil and gas mining regions. </p> </li> <li> <p>Telecommunications: Workers such as tower climbers and cable technicians often work alone at high-rise towers and other elevated locations and their safety can be ensured by slip and detection system. </p> </li> <li> <p>Mining Industry: Workers such as underground miners and drill operators often work alone in remote and confined spaces. </p> </li> <li> <p>Maintenance: Building engineers, window cleaners, painters, facility maintenance workers, etc., often work alone during the night shift and in isolated places at heights.</p> </li> <li> <p>Noisy Environments: Like working alone, a noisy environment can hinder the process of quick response. Deployment areas could be for the following category of workers.  </p> </li> <li> <p>Construction: Heavy equipment operators like jackhammer operators at construction sites work in extremely noisy environments. </p> </li> <li> <p>Manufacturing: Workers in factories and assembly lines are exposed to a lot of noise due to the machinery and equipment used. </p> </li> <li> <p>Airports: Workers at airports, such as ground crew and baggage handlers, are often exposed to high noise levels from aircraft engines and other airport equipment. </p> </li> <li> <p>Elderly care facilities and Smart Homes: Detect slip and fall hazards in elderly care facilities. Such systems can also be deployed in homes to monitor the elderly or disabled, alerting caregivers or family members. </p> </li> </ul>"},{"location":"scenarios/slip-and-fall-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect slip and fall with high accuracy. Additionally, our model trained on real-world images minimizes false-positives or false-negatives.  </p> <p>The cameras scan every frame to ensure there are no accidents related to slip and fall cases. </p> <p>To ensure accuracy and reliability for the model, these camera-based monitoring services should be supplemented by strong compliance processes. Furthermore, workers working in different factory units should always be made aware of these accidents and how to safeguard them. </p>"},{"location":"scenarios/slip-and-fall-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/slip-and-fall-detection/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Only person class is considered for model building. COCO is a  large-scale dataset that addresses three core research problems in scene understanding: detecting non-iconic views (or non-canonical perspectives of objects), contextual reasoning between objects and the precise 2D localization of objects. </p> <p>COCO dataset has an even distribution of: </p> <ul> <li> <p>Different(indoor/outdoor) environments </p> </li> <li> <p>Male vs Female  </p> </li> <li> <p>Different light settings </p> </li> <li> <p>Variations in camera orientations </p> </li> <li> <p>Using security camera feeds </p> </li> </ul>"},{"location":"scenarios/slip-and-fall-detection/#model","title":"Model","text":"<p>The model is built using Yolov5 pre-trained model for detecting a person followed by a media pipe library used to estimate the pose of the person. </p>"},{"location":"scenarios/slip-and-fall-detection/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   2326 v5 Ceiling 65%  71%  71%  <p>The model is adaptable enough to run on any edge computing device. </p>"},{"location":"scenarios/slip-and-fall-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor and detect occurrences of slip and fall incidents. </p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect human poses to identify slip and fall accidents in the camera feed. \u00a0</p> </li> <li>If either slip or fall is detected, an alert is raised.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test slip-and-fall-detection\n\nDownloading models for scenario: slip-and-fall-detection\nModel: slip-and-fall-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-slip-and-fall-detection/yolov5s-slip-and-fall-detection-0.0.1.zip\n\n\nStarting scenario: slip-and-fall-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with slip and fall being detected within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/slip-and-fall-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/slip-and-fall-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/","title":"Early Fire Signs Detection","text":"<p>An intelligent Early Fire Signs Detection system aimed at safeguarding people and facilities</p> <p> </p> Detection of Smoke and Fire event"},{"location":"scenarios/smoke-and-fire-detection/#overview","title":"Overview","text":"<p>Fire can be one of the most catastrophic disasters that can happen anywhere and cause major destruction. Fire hazards exist in all types of industrial buildings and business environments. Fire incident in the workplace not only accounts for a large number of injuries but also for downtime and running costs to repair the damage to the premises and machinery. </p> <p>Conventional heat detector-based fire warning systems cannot detect an early fire. Heat detectors only alert when the temperature of the surrounding environment reaches a certain level, and it doesn\u2019t happen until fire spreads considerably, not leaving many opportunities to avoid the incident. Therefore, it is important to have an early fire signs detection method that would allow authorities to detect and put out fires before it goes out of control.  </p> <p>To monitor and detect early signs of fire at workplaces, cameras can be used. </p>"},{"location":"scenarios/smoke-and-fire-detection/#vision-ai-based-monitoring","title":"Vision AI-based monitoring","text":"<p>Vision AI-based Model for Early Fire Signs Detection is designed to spot early signs of smoke and fire and helps save lives and mitigate damages caused by industry fires. We aim to create safe workplaces by offering innovative, reliable, flexible, and scalable solutions. </p> <p>To ensure accuracy and reliability, these camera-based monitoring services should be supplemented by effective practices to ensure and prevent fire hazards. Furthermore, workers working in different factory units should always be made aware of fire signs to look for. </p>"},{"location":"scenarios/smoke-and-fire-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/smoke-and-fire-detection/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on early fire detection algorithms that are currently in practice.  </p> <p>The dataset is made up of images and videos gathered from varied sources. The dataset has been designed to ensure real-world situations. It has an even distribution of: </p> <ul> <li>Different(indoor/outdoor) environments </li> <li>Variations in foregroung objects including persons, equipments etc </li> <li>Different lighting conditions</li> <li>Variations in weather conditions </li> <li>Using security camera feeds </li> <li>Multiple instances of fire and smoke </li> <li>Variations in camera orientations </li> <li>Classes considered for model building are smoke and fire</li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/#model","title":"Model","text":"<p>The model is based off of the YOLOv5 algorithm. The model is trained on a custom dataset of images and videos. The model is trained based on the above dataset compiled by our team. We intend to develop a model that generalizes well in real world situations.</p>"},{"location":"scenarios/smoke-and-fire-detection/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP  63,055 v7 Straight 98 95 95 <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/smoke-and-fire-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the early signs of fire in the workplace to ensure the safety of human lives in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect the presence of smoke and fire in the camera feed. </li> <li>An alarming system is inplace as part of early fire signs detection solution.</li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/#try-it-now","title":"Try it now","text":""},{"location":"scenarios/smoke-and-fire-detection/#quick-method-using-your-local-web-cam","title":"Quick method - using your local web-cam","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Test the scenario from your local web-cam</li> </ul> <pre><code>$ visionai scenario test smoke-and-fire-detection\n\nDownloading models for scenario: smoke-and-fire-detection\nModel: smoke-and-fire-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n\nStarting scenario: smoke-and-fire-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with smoke and fire being detected within the camera field of view.</li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/#in-an-actual-environment","title":"In an actual environment","text":"<p>To use this scenario in an actual environment, you can follow these steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Download the scenario</li> </ul> <pre><code>$ visionai scenario download smoke-and-fire-detection\n\nDownloading models for scenario: smoke-and-fire-detection\nModel: smoke-and-fire-detection\nhttps://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n</code></pre> <ul> <li>Add the camera feed to the scenario</li> </ul> <pre><code>$ visionai camera add OFFICE-01 --url rtsp://192.168.0.1/stream1\n$ visionai camera OFFICE-01 add-scenario smoke-and-fire-detection\n$ visionai run\n\nStarting scenario: smoke-and-fire-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with smoke and fire being detected within the camera field of view.</li> </ul> <p>For more details visit VisionAI web application.</p>"},{"location":"scenarios/smoke-and-fire-detection/#features","title":"Features:","text":"<ul> <li> <p>Unparalleled Accuracy and faster detection</p> <p>VisionAI's Fire Signs detection model is capable of detecting fire incidents at an outstanding accuracy of up to 98% and a detection speed of 36 FPS.</p> </li> <li> <p>Flexible and Scalable </p> <p>VisionAI's Fire Signs detection is an end-to-end solution that integrates seamlessly with your existing camera network and is ready to detect. It can fit any building size and is easily expandable by adding more IP cameras to the network. </p> </li> <li> <p>Integrated Solution </p> <p>It is an integrated system combining surveillance and early fire signs detection in one system.</p> </li> <li> <p>Deployment Ready </p> <p>Our pre-trained AI models are ready for immediate industrial deployments</p> </li> <li> <p>Versatile Framework </p> <p>We offer flexibility in deployment; the model can operate at the Edge, in the cloud, or any self-hosted environment </p> </li> <li> <p>Compatible </p> <p>VisionAI's Early Fire Signs detection model has a broad potential and can be efficiently used for indoor and outdoor applications. </p> </li> <li> <p>Privacy Protection</p> <p>We understand your concerns about data privacy and take a proactive approach to preserve it. Our models are privacy oriented by design.</p> </li> </ul>"},{"location":"scenarios/smoke-and-fire-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/smoke-and-fire-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/social-distance/","title":"Social Distancing","text":"<p>Creating Safe Workplaces: Companies Implement Measures to Ensure Social Distancing in the Workplace with Vision AI.</p> <p> </p> Detection of Social distancing event"},{"location":"scenarios/social-distance/#overview","title":"Overview","text":"<p>Maintaining social distancing in workplaces and industries is crucial to prevent the spread of diseases including COVID-19 and protect the health and safety of employees or workers. </p> <p>In workplaces and industries, where employees or workers are often in close proximity to each other for extended periods, social distancing can help to reduce the spread of the virus. By keeping a safe distance from each other, employees or workers can avoid coming into contact with respiratory droplets and reduce the risk of infection.</p> <p>Overall, maintaining social distancing in workplaces and industries is an important part of a comprehensive approach to controlling the spread of certain diseases. By implementing social distancing measures and other best practices, employers can help to protect the health and safety of their employees or workers and prevent the spread of the virus.</p>"},{"location":"scenarios/social-distance/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI based monitors can be used to push out events for different people observed in the frame and the distances between them by providing real-time video feeds of the factory area. The cameras scan every frame to ensure social distancing is well maintained.</p>"},{"location":"scenarios/social-distance/#events","title":"Events","text":"<p>VisionAI model's generated events would be: - Person distance events detected</p>"},{"location":"scenarios/social-distance/#camera-placement","title":"Camera Placement","text":"<ul> <li>Install cameras in high traffic areas, such as entrances, exits, and common areas.</li> <li>Place cameras in areas where social distancing violations are most likely to occur, such as checkout lines or waiting areas.</li> </ul>"},{"location":"scenarios/social-distance/#camera-height","title":"Camera Height","text":"<ul> <li> <p>Cameras should be installed at a height of 7-8 feet above the floor level.</p> </li> <li> <p>Place the camera 10-12 feet from the focal point.</p> </li> </ul>"},{"location":"scenarios/social-distance/#camera-angle-mounting-ranges","title":"Camera Angle Mounting Ranges","text":"<ul> <li>Place the camera at an angle that captures a wide area and any social distancing violations.</li> </ul> <p>Find more details about camera placement here.</p>"},{"location":"scenarios/social-distance/#model-details","title":"Model Details","text":""},{"location":"scenarios/social-distance/#dataset","title":"Dataset","text":"<p>The dataset for this scenario is based on real-world social distancing events. The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/social-distance/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   6580 v1 Both(Ceiling and Straight) 89.0%  91.6%  84.0%"},{"location":"scenarios/social-distance/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises and raise social distancing events.</li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</li> <li>We detect people and the distance between them form the camera feed and raise a alert if social distancing is not maintained.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test social-distancing\n\nDownloading models for scenario: social distancing\n\n\n\nStarting scenario: social distancing..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of social distancing within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/social-distance/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Unmatched accuracy</p> <p>Trained and Tested to give the best results. Our systems are trained to detect social distancing with an accuracy of 99%</p> </li> <li> <p>Lightning Fast and Response Time</p> <p>Our Ultra-fast Processing provides real-time inference results and feedback (~30 frames per second processing). </p> </li> <li> <p>Minimizing false-positives/negatives</p> <p>Our systems create a fail-proof system by ensuring there are no false-positives or false-negatives. </p> </li> <li> <p>Scalability and Deployment </p> <p>Our pre-trained/custom models can be deployed instantly and are camera independent which means they can be pre-installed with existing cameras on site. We also offer cameras, IoT sensors and edge devices with strategic placement that helps scale a large workplace area with minimum installations. </p> </li> <li> <p>Custom Integrations</p> <p>Our detection system can be integrated with other safety systems, such as building management systems or alarm systems, allowing for a coordinated response to emergencies.</p> </li> </ul>"},{"location":"scenarios/social-distance/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/social-distance/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/solicitation/","title":"Solicitation Detection","text":"<p>A smarter way to unveil solicitation</p> <p> </p> Detection of Solicitation detection event"},{"location":"scenarios/solicitation/#overview","title":"Overview","text":"<p>Solicitation is the act of requesting or offering something in return for a favor, service, or product. In industries, Solicitation can take various forms, for example, employees soliciting other employees for money in exchange for job-related favors, any outsiders approaching factory employees or workers for different purposes like obtaining confidential information, for employment etc. Solicitation can also occur in public places, including malls, hotels, casinos, public transportation, clubs, etc. However, solicitation is often prohibited in these areas due to some specific rules and regulations depending upon the location and jurisdiction. </p> <p>To maintain a safe work environment, sustain an organization\u2019s values and prevent unethical behaviors within the companies and in public spaces, it is important to detect solicitation, take appropriate measures to address such behaviors and prevent it from happening. Computer Vision solutions can help effectively detect acts of solicitation before they occur.</p>"},{"location":"scenarios/solicitation/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Implement VisionAI solution to address the problems associated with Solicitation by timely detection of any unethical behaviors in public spaces or companies. Our AI and deep learning-based solution can identify behavioral anomalies that may indicate solicitation.    </p> <p>Our smart solution seamlessly integrates with the existing camera infrastructure and analyzes the real-time video feed. It can help in different ways and offers a comprehensive solution for Solicitation;</p> <ul> <li>Facial recognition technology can help identify individuals involved in solicitation activities</li> <li>The algorithm can also analyze patterns of behaviors associated with solicitation, such as loitering or approaching strangers</li> <li>The algorithm also detects objects generally related to solicitation, like signs, posters, and flyers</li> <li>The algorithm can identify any unusual or suspicious behaviors in public spaces that indicate solicitation and help security and law enforcement personnel to respond more quickly to potential issues </li> </ul>"},{"location":"scenarios/solicitation/#model-details","title":"Model Details","text":""},{"location":"scenarios/solicitation/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos in large numbers collected from diverse sources and is designed to reflect real-world scenarios. The dataset is representative of all types of people from different ages, genders and backgrounds engaging in solicitation behavior. Also, it includes all settings, situations and different types of solicitation behavior, such as advertising or sales pitches. It is evenly distributed with;</p> <ul> <li> <p>Different locations - in public places, there are various locations where solicitation can take place, like walkways, public parks, parking lots, public transportation hubs, malls, clubs, hotels, casinos, and retail environments.</p> </li> <li> <p>Different angles and perspectives - The dataset includes images or videos captured from different angles and lighting conditions to ensure that the model can detect solicitation behavior in various real-world scenarios.</p> </li> <li> <p>Different versions - The dataset undertakes different types of solicitation to ensure the model is robust enough and can generalize well to any new situation.</p> </li> </ul>"},{"location":"scenarios/solicitation/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   4810 v1 Both(Ceiling and Straight) 92.0%  91.6%  88.0%"},{"location":"scenarios/solicitation/#scenario-details","title":"Scenario details","text":"<p>Our VisionAI solution for solicitation detection works in different scenarios to detect any unethical behavior indicating solicitation within an industrial setting or in public spaces. The model is equipped to detect the following;</p> <ul> <li>Identify through facial recognition: known solicitors</li> <li>Identify a single person going and talking to multiple people</li> <li>Identify and track scantily clad persons and whether they are talking to people</li> <li>Identify similar patterns like one person repeating the same type of behavior</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test solicitation-detection\n\nDownloading models for scenario: solicitation-detection\nModel: solicitation-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-solicitation-detection/yolov5s-solicitation-detection-0.0.1.zip\n\n\nStarting scenario: solicitation-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of light sensor monitoring within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/solicitation/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The model can be deployed in real-time to monitor the public spaces and industrial settings for any unethical behavior indicating solicitation.</p> </li> <li> <p>Easy to deploy: The solution can be deployed easily with minimal effort and can be integrated with the existing camera infrastructure.</p> </li> <li> <p>Customizable: The solution can be customized to meet the specific requirements of the organization.</p> </li> </ul>"},{"location":"scenarios/solicitation/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/solicitation/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/spills-and-leaks-hazard/","title":"Spills &amp; Leaks detection","text":"<p>Spills and Leaks detection through Vision AI.</p>"},{"location":"scenarios/spills-and-leaks-hazard/#overview","title":"Overview","text":"<p>Spills and leaks in industries can have significant health impacts on both humans and wildlife. The severity of the health impact depends on the type of substance that is spilled or leaked, the duration and extent of the exposure, and the vulnerability of the exposed population. Some potential health impacts of spills and leaks are Respiratory problems, skin irritation, Neurological effects, Cancer, Reproductive problems and Environmental impact.</p> <p>Preventing and mitigating spills and leaks is crucial for protecting the environment and human health. Existing solitions could be regular inspections and maintenance of equipment. Manual inspection is not foolproof and can be prone to errors and oversights. Human inspectors may miss small leaks or spills that may go undetected until they become larger and more severe.</p>"},{"location":"scenarios/spills-and-leaks-hazard/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Manual inspections can be time-consuming and labor-intensive, which can make them impractical for large or complex industrial facilities.</p> <p>Vision AI-based model is designed to detect spills and leaks including water puddles, water leaks and slippery surfaces. The model can analyze images and video footage to identify visual anomalies, such as the appearance of a spill or leak, which can be missed by human inspectors.</p> Oil leak Water leak in pipes"},{"location":"scenarios/spills-and-leaks-hazard/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Water puddle detected</li> <li>Water leak from equipment detected</li> <li>Spill event detected</li> <li>Slippery sign detected</li> </ul>"},{"location":"scenarios/spills-and-leaks-hazard/#model-details","title":"Model Details","text":""},{"location":"scenarios/spills-and-leaks-hazard/#dataset","title":"Dataset","text":"<p>Dataset for spills/leakages is properly curated and validated to ensure that the models are accurate and reliable. </p> <p>Some of the sources used to take images are:</p> <ul> <li>CAMEO Chemicals dataset</li> <li>The NOAA Hazardous Material Incident database</li> <li> <p>The Oil Spill Dataset</p> </li> <li> <p>The Pipeline and Hazardous Materials Safety Administration (PHMSA) dataset</p> </li> <li> <p>The Spill Impact Mitigation Assessment (SIMA) dataset</p> </li> </ul>"},{"location":"scenarios/spills-and-leaks-hazard/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   4170 v1 Both(Ceiling and Straight) 85.0%  91.6%  87.0%"},{"location":"scenarios/spills-and-leaks-hazard/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the signs of leakage, spills in the workplace to ensure the safety of human lives in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect any kind of leakage in the camera feed.</li> <li>An alarming system is inplace as part of solution.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-leak-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-leak-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: no-leak-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of spills and leak within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/spills-and-leaks-hazard/#features","title":"Features","text":"<p>VisionAI's Spill and leak detection  identifies and classifies spills and leaks in real-time. Here are some features of spill and leak detection:</p> <ul> <li> <p>Real-time monitoring: AI-based spill and leak detection systems can continuously monitor facilities and pipelines in real-time, allowing for quick detection and response times.</p> </li> <li> <p>Automated detection and alerts: AI-based systems can detect spills and leaks automatically and issue alerts to relevant personnel or systems, allowing for quick response and mitigation of the issue.</p> </li> <li> <p>Increased accuracy and reliability: VisionAI models can analyze large amounts of data quickly and accurately, allowing for the identification of even small leaks or spills that may be missed by human inspectors.</p> </li> <li> <p>Integration with other systems: VisionAI solution can be integrated with other systems such as alarm systems and spill response plans, allowing for a more comprehensive and effective response to spills and leaks.</p> </li> <li> <p>Predictive analytics: VisionAI models  can analyze historical data and patterns to identify potential risks and prevent future spills and leaks.</p> </li> <li> <p>Remote monitoring:  The system allows continuous monitoring of facilities and pipelines in remote or hard-to-reach areas.</p> </li> </ul> <p>Note</p> <p>Overall, spill and leak detection using our VisionAI's solution provides a powerful tool for industries to improve the accuracy, speed, and efficiency of spill and leak detection and response. The use of AI can also help to reduce the risk of human exposure to hazardous materials and prevent environmental damage caused by spills and leaks.</p>"},{"location":"scenarios/spills-and-leaks-hazard/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/spills-and-leaks-hazard/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/spills-and-leaks/","title":"Spills &amp; Leaks detection","text":"<p>Spills and Leaks detection through Vision AI.</p>"},{"location":"scenarios/spills-and-leaks/#overview","title":"Overview","text":"<p>Spills and leaks in industries can have significant health impacts on both humans and wildlife. The severity of the health impact depends on the type of substance that is spilled or leaked, the duration and extent of the exposure, and the vulnerability of the exposed population. Some potential health impacts of spills and leaks are Respiratory problems, skin irritation, Neurological effects, Cancer, Reproductive problems and Environmental impact.</p> <p>Preventing and mitigating spills and leaks is crucial for protecting the environment and human health. Existing solitions could be regular inspections and maintenance of equipment. Manual inspection is not foolproof and can be prone to errors and oversights. Human inspectors may miss small leaks or spills that may go undetected until they become larger and more severe.</p>"},{"location":"scenarios/spills-and-leaks/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Manual inspections can be time-consuming and labor-intensive, which can make them impractical for large or complex industrial facilities.</p> <p>Vision AI-based model is designed to detect spills and leaks including water puddles, water leaks and slippery surfaces. The model can analyze images and video footage to identify visual anomalies, such as the appearance of a spill or leak, which can be missed by human inspectors.</p> Oil leak Water leak in pipes"},{"location":"scenarios/spills-and-leaks/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Water puddle detected</li> <li>Water leak from equipment detected</li> <li>Spill event detected</li> <li>Slippery sign detected</li> </ul>"},{"location":"scenarios/spills-and-leaks/#model-details","title":"Model Details","text":""},{"location":"scenarios/spills-and-leaks/#dataset","title":"Dataset","text":"<p>Dataset for spills/leakages is properly curated and validated to ensure that the models are accurate and reliable. </p> <p>Some of the sources used to take images are:</p> <ul> <li>CAMEO Chemicals dataset</li> <li>The NOAA Hazardous Material Incident database</li> <li> <p>The Oil Spill Dataset</p> </li> <li> <p>The Pipeline and Hazardous Materials Safety Administration (PHMSA) dataset</p> </li> <li> <p>The Spill Impact Mitigation Assessment (SIMA) dataset</p> </li> </ul>"},{"location":"scenarios/spills-and-leaks/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   4170 v1 Both(Ceiling and Straight) 85.0%  91.6%  87.0%"},{"location":"scenarios/spills-and-leaks/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li>We use existing camera feeds from the premises to monitor the signs of leakage, spills in the workplace to ensure the safety of human lives in the workplace. </li> <li>VisionAI system is able to run on edge devices. It uses camera feeds for processing. </li> <li>We detect any kind of leakage in the camera feed.</li> <li>An alarming system is inplace as part of solution.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test no-leak-detection\n\nDownloading models for scenario: no-smoking-detection\nModel: no-leak-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n\n\nStarting scenario: no-leak-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of spills and leak within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/spills-and-leaks/#features","title":"Features","text":"<p>VisionAI's Spill and leak detection  identifies and classifies spills and leaks in real-time. Here are some features of spill and leak detection:</p> <ul> <li> <p>Real-time monitoring: AI-based spill and leak detection systems can continuously monitor facilities and pipelines in real-time, allowing for quick detection and response times.</p> </li> <li> <p>Automated detection and alerts: AI-based systems can detect spills and leaks automatically and issue alerts to relevant personnel or systems, allowing for quick response and mitigation of the issue.</p> </li> <li> <p>Increased accuracy and reliability: VisionAI models can analyze large amounts of data quickly and accurately, allowing for the identification of even small leaks or spills that may be missed by human inspectors.</p> </li> <li> <p>Integration with other systems: VisionAI solution can be integrated with other systems such as alarm systems and spill response plans, allowing for a more comprehensive and effective response to spills and leaks.</p> </li> <li> <p>Predictive analytics: VisionAI models  can analyze historical data and patterns to identify potential risks and prevent future spills and leaks.</p> </li> <li> <p>Remote monitoring:  The system allows continuous monitoring of facilities and pipelines in remote or hard-to-reach areas.</p> </li> </ul> <p>Note</p> <p>Overall, spill and leak detection using our VisionAI's solution provides a powerful tool for industries to improve the accuracy, speed, and efficiency of spill and leak detection and response. The use of AI can also help to reduce the risk of human exposure to hazardous materials and prevent environmental damage caused by spills and leaks.</p>"},{"location":"scenarios/spills-and-leaks/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/spills-and-leaks/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/station-occupancy/","title":"Station Occupancy","text":"<p>Revolutionize your workspace with our Smart Desk Occupancy Tracker.</p>"},{"location":"scenarios/station-occupancy/#overview","title":"Overview","text":"<p>Tracking Workplace Metrics is key for identifying problems and driving growth. One such metric that organizations need to keep tabs on is desk occupancy. Tracking Desk Occupancy provides multiple valuable insights like worker productivity, worker behavioral analysis, floor planning, and utilization of space, all of which are required for workspace optimization and efficient resource management.</p> <p>Despite the increasing adoption of desk occupancy measurement across industries, present systems utilized to measure desk occupancy are fraught with several limitations, exhibit limited accuracy, lack the ability to provide multiple metrics, and can incur substantial installation costs.</p>"},{"location":"scenarios/station-occupancy/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Introducing our fully automated Vision AI system for monitoring Desk Occupancy. Our next-gen AI models detect and count the presence of people within a specific area, whether they are performing a particular task or not, their dwell time, occupancy density and many more metrics. </p> <p>Our robust occupancy monitoring systems offer higher accuracy compared to current solutions, are cost-effective, and are capable of seamlessly integrating with existing cameras and infrastructure. With our system, there's no need to install multiple sensors or measurement devices, as a single camera can cover a wide area and enable users to easily leverage our AI-based real-time detection with minimal effort.</p> <p> </p> monitoring desk occupancy"},{"location":"scenarios/station-occupancy/#events","title":"Events","text":"<p>VisionAI model's generated events would be:</p> <ul> <li>Daily summary of occupancy metrics on a per desk/station basis</li> </ul> <p>It is recommended that any instance of an absence of a person from his/her desk be reported to the appropriate authority. An event data for desk occupancy scenario may include information such as:</p> <ul> <li>Date and time of the event</li> <li>Location of the event</li> </ul>"},{"location":"scenarios/station-occupancy/#model-details","title":"Model Details","text":""},{"location":"scenarios/station-occupancy/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from diverse sources and is designed to reflect real-world scenarios. It is evenly distributed with:</p> <ul> <li> <p>Positive images: The dataset includes images that contain people sitting at desks. These images should show a clear view of the desk and the person occupying it.</p> </li> <li> <p>Negative images: The dataset includes images that do not contain people sitting at desks. They could show empty desks or other objects in the workspace.</p> </li> <li> <p>Images with occlusions: The dataset includes images where the view of the person occupying the desk is partially obstructed, for example, by another object or person.</p> </li> <li> <p>Images with different lighting conditions: The dataset includes images that are taken under different lighting conditions, such as bright daylight, low-light, or artificial light.</p> </li> <li> <p>Images with different camera angles: The dataset includes images that are taken from different camera angles, such as top-down, side view, or angled view.</p> </li> <li> <p>Images with different desk layouts: The dataset includes images that show different types of desks, such as standing desks, shared desks, or cubicles.</p> </li> </ul>"},{"location":"scenarios/station-occupancy/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   1280 v1 Ceiling 95.0%  91.6%  88.0%"},{"location":"scenarios/station-occupancy/#scenario-details","title":"Scenario details","text":"<p>Real-time detection and alerts for different scenarios includes but are not limited to:</p> <ul> <li>When a person sits down at a desk that was previously unoccupied, the model can detect the change in occupancy.</li> <li>When a person gets up from a desk, the model can detect that the desk is now unoccupied.</li> <li>If the model detects an object on the desk that obstructs the view of the person occupying it, it may not be able to detect occupancy until the obstruction is removed.</li> <li>If the lighting conditions in the room change, the model may need to adjust its settings to continue accurately detecting occupancy.</li> <li>The model can also detect occupancy in real-time as people move around the workspace, allowing it to track changes in occupancy throughout the day.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test desk-occupancy\n</code></pre> <p>Downloading models for scenario: desk-occupancy Model: miss-fire-exting-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip</p> <p>Starting scenario: desk-occupancy..</p> <p>```</p> </li> <li> <p>You should be able to see the events generated on your console window with the detections of desk occupancy within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/station-occupancy/#features","title":"Features","text":"<p>Some potential features of VisionAI for monitoring desk occupancy could include:</p> <ul> <li> <p>Object Detection: This feature can help to monitor the occupancy of the desks and alert if a desk is occupied or not.</p> <ul> <li> <p>Heat Map: This feature can help to optimize the usage of the workspace and identify hotspots where there may be congestion.</p> </li> <li> <p>Occupancy Monitoring: This feature can help to optimize the usage of the workspace and ensure that all desks are being used efficiently.</p> </li> <li> <p>Desk Usage Patterns: This feature can help to optimize the usage of the workspace and identify areas that need improvement.</p> </li> <li> <p>Desk Reservation: This feature can help to optimize the usage of the workspace and ensure that all desks are being used efficiently.</p> </li> </ul> </li> </ul>"},{"location":"scenarios/station-occupancy/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/station-occupancy/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/suspicious-activity/","title":"Suspicious Activity","text":""},{"location":"scenarios/suspicious-activity/#overview","title":"Overview","text":"<p>Suspicious activity detection refers to the process of identifying behavior or actions that deviate from the norm or expected patterns, and may indicate potential threats or risks. </p> <p>In security, suspicious activity detection can help identify potential threats or breaches in systems, networks, or physical environments. This can involve monitoring of access logs, network traffic, user behavior, or physical activity using video surveillance or other sensors.</p> <p>What\u2019s included in this suite:</p> <ul> <li>Vandalism &amp; property destruction</li> <li>Firearms &amp; knives</li> </ul>"},{"location":"scenarios/suspicious-package-detection/","title":"Suspicious Package Detection","text":"<p>Reliable and accurate Suspicious package detection for a safe and secure workplace environment</p> <p> </p> Detection of Suspicious Package event"},{"location":"scenarios/suspicious-package-detection/#overview","title":"Overview","text":"<p>Manual inspection of every package or parcel is time-consuming and can lead to delays in delivering important items. An automated detection model can quickly screen packages and prioritize those that require additional inspection. By detecting suspicious packages early, it may be possible to prevent an incident from occurring. This can save lives and minimize damage to property.</p> <p>Suspicious packages could contain hazardous materials such as explosives or chemicals, which could pose a significant risk to the safety of employees and the public. A detection model can quickly identify potential threats and allow for timely evacuation or other appropriate actions.</p> <p>Implementing a suspicious package detection model can enhance workplace safety and security, improve operational efficiency, and ensure compliance with legal requirements.</p>"},{"location":"scenarios/suspicious-package-detection/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>VisionAI suspicious package detection model is trained on a large dataset of known suspicious packages, as well as non-suspicious packages, to learn to recognize the characteristics that are most indicative of a threat.  VisionAI based suspicious package monitoring can be used to analyze new packages and determine whether they are suspicious or not. If a package is flagged as suspicious, security personnel can be alerted to investigate further and take appropriate action.</p>"},{"location":"scenarios/suspicious-package-detection/#model-details","title":"Model Details","text":""},{"location":"scenarios/suspicious-package-detection/#dataset","title":"Dataset","text":"<p>The dataset for this type of model typically consists of a large number of images or videos, captured from a variety of angles and under different lighting conditions. The images or videos may be collected from surveillance cameras or from other sources, such as social media posts or news reports.</p> <p>To ensure that the model is able to generalize to new and unseen images or videos, the dataset should include a diverse range of packages, with different sizes, shapes, colors, and markings. The dataset should also include examples of packages that are not suspicious or abandoned, in order to provide a balanced training set.</p>"},{"location":"scenarios/suspicious-package-detection/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   7810 v1 Both(Ceiling and Straight) 95.0%  91.6%  88.0%"},{"location":"scenarios/suspicious-package-detection/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li>We use existing camera feeds from the premises to monitor suspicious packages within the camera field of view.</li> <li>The model is able to detect suspicious packages and an alert system is in place to notify the appropriate authorities in the event that a suspicious package is detected. It is designed to minimize false alarms and provide timely and accurate information.</li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test suspicious-package-detection\n\nDownloading models for scenario: suspicious-package-detection\nModel: suspicious-package-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-suspicious-package-detection/yolov5s-suspicious-package-detection-0.0.1.zip\n\n\nStarting scenario: suspicious-package-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with the detections of aggressive behavior within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/suspicious-package-detection/#features","title":"Features","text":"<ul> <li> <p>Real-time monitoring: The model is able to analyze data in real-time to detect potential suspicious packages.</p> </li> <li> <p>Integration with other systems: The model is able to integrate with other security systems, such as access control systems, to provide a comprehensive approach to package security.</p> </li> <li> <p>Alert system: The model is having an alert system that can notify the appropriate authorities in the event that a suspicious package is detected. It is designed to minimize false alarms and provide timely and accurate information.</p> </li> </ul>"},{"location":"scenarios/suspicious-package-detection/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/suspicious-package-detection/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/theft/","title":"Shoplifting or Theft Detection","text":"<p>Ensure prevention of Shoplifting, employee theft, minimize insurance loss and other related damages across the retail sector.</p> <p> </p> Detection of shoplifting or theft event"},{"location":"scenarios/theft/#overview","title":"Overview","text":"<p>Typically considered one of the most accessible and in many cases least-sophisticated types of crime, shoplifting persists as an undeniably damaging affliction across the retail sector. In fact, the National Retail Security Survey reported that loss of inventory cost U.S. retailers an estimated $49 billion USD in 2016, with 70 percent of the loss caused by employee theft and shoplifting.</p> <p>Theft or shoplifting detection models can provide businesses with a proactive approach to preventing losses due to theft or shoplifting, promoting employee safety, complying with legal requirements, and deterring potential offenders.</p> <p>There are several reasons why theft or shoplifting detection models are necessary at workplaces:</p> <ul> <li> <p>Loss prevention: Theft or shoplifting can result in significant financial losses for businesses. By implementing theft or shoplifting detection models, businesses can identify and prevent such losses.</p> </li> <li> <p>Employee safety: Theft or shoplifting incidents can also put employees at risk, especially if they attempt to intervene. Detection models can provide a safer way to monitor and prevent such incidents.</p> </li> <li> <p>Legal compliance: Some industries are required by law to implement security measures to prevent theft or shoplifting. Implementing a detection model can help businesses comply with these regulations.</p> </li> <li> <p>Deterrent effect: The presence of a theft or shoplifting detection model can act as a deterrent to potential offenders, reducing the likelihood of theft or shoplifting incidents.</p> </li> </ul>"},{"location":"scenarios/theft/#visionai-based-monitoring","title":"VisionAI Based Monitoring","text":"<p>Theft or shoplifting detection using our solution can prove to be an important application in retail settings, as it can help to prevent loss and increase security. </p> <p>To detect theft or shoplifting our model is trained on a dataset of video footage with labeled instances of theft or shoplifting. The state-of-the-art model is then learned, to recognize patterns in the video data that are associated with suspicious behavior, such as loitering near a display or concealing merchandise in a bag or pocket.</p> <p>Our trained model can be used to analyze live video footage from surveillance cameras in real-time. The system can alert security personnel or trigger an alarm when it detects suspicious behavior, allowing them to intervene and prevent the theft or shoplifting from occurring. Our systems for theft or shoplifting detection uses various techniques, such as object detection, tracking, and activity recognition. These techniques can be combined to create a more robust and accurate system.</p>"},{"location":"scenarios/theft/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   1280 v1 Both(Ceiling and Straight) 95.0%  91.6%  88.0%"},{"location":"scenarios/theft/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"scenarios/vandalism/","title":"Vandalism and property destruction","text":"<p>Safeguard your assets with our advanced vandalism detection model.</p> <p>Vandalism/Graffiti detection event</p>"},{"location":"scenarios/vandalism/#overview","title":"Overview","text":"<p>Vandalism and property destruction can have serious consequences, both for individuals and for society as a whole. For example, it can lead to physical harm, emotional distress, financial losses, and damage to public infrastructure. By developing a model that can accurately detect and predict incidents of vandalism and property destruction, we can take proactive measures to prevent them from occurring or minimize their impact if they do occur. This can include increasing surveillance, enhancing security measures, and improving emergency response protocols. Ultimately, a vandalism and property destruction model can help protect people and property, reduce costs associated with damage, and promote a safer and more secure society. We need a vandalism and property destruction model to help prevent and mitigate damage caused by these types of incidents. </p> <p>Technically, vandalism is described as a video event that is instantiated by a video object that inflicts temporally consistent static changes (such as damage) inside a preset restricted region that is purportedly left unaltered by normal (i.e., legal) interaction with video objects.</p>"},{"location":"scenarios/vandalism/#visionai-based-monitoring","title":"VisionAI Based Monitoring","text":""},{"location":"scenarios/vandalism/#vandalism","title":"Vandalism","text":"<p>VisionAI based Monitoring is an effective approach to investigate if the site has temporally  consistent and significant static changes, indicative of damage, when an object is detected departing such a place. A vandalism event is declared and the vandals are located if there are such changes and given that the site is typically unaltered following legal use. The proposed method has a 96% detection rate when applied to video clips of actual and simulated vandalism in action. It recognises several types of vandalism, including theft and graffiti, and it can deal with abrupt illumination changes, occlusions, and segmentation mistakes. The frame rate of the suggested approach is 13 frames per second.</p>"},{"location":"scenarios/vandalism/#constraints","title":"Constraints","text":"<p>The automatic detection of vandalism in video surveillance is a challenging task because of: - The complex and unpredictable nature of a vandalism act and the speed at which it may occur - The underlying difficulty of finding a unique definition for vandalism which may vary based on social contexts and applications - The difficulty in distinguishing between normal and vandal interaction between persons and vandalism-prone objects or sites and - The lack of real vandalism test video sequences publicly available for training or testing.</p>"},{"location":"scenarios/vandalism/#proposed-method-for-detection-of-vandalism","title":"Proposed Method for detection of vandalism","text":"<p>A video object refers to a temporally consistent region (over a short period)in a video sequence. Video objects have spatio-temporal features such as contour,area, motion, and trajectory. For example, a video object has a unique identifier (ID) maintained by the tracking algorithm during the life-time of an object in the videosequence. A video event is an interpreted spatio-temporal relationship associating one or multiple objects (e.g., moving, staying long and is inside). Video events have information associated with them such as the IDs of the video objects involved in the event, the time at which it is detected, and its duration which is the number of consecutive frames the event is detected.We only consider rigid vandalism-prone objects that do not change over time.This includes pay-phones, vending machines, and paying stations in parking lots.</p> <p>For example, vandalism of electronic street signs switching content periodically is not considered. Also, we expect that the vandalism act alters the normal appearance of objects. Meaning, after the site is vandalized, there is visible damage (i.e., change) to the site. We use video object segmentation and ID tracking.</p>"},{"location":"scenarios/vandalism/#graffiti","title":"Graffiti","text":"<p>Graffiti can have a negative effect on a community's property value and tourism. Moreover, it may cause a decline in retail sales and an increase in public dread, both of which might drain tax funds intended for prevention. </p> <p>The Graffiti Image classifier can help law enforcement more effectively recognise Graffiti Images on the streets in order to lessen damage.</p> <p>VisionAI based solution is focused on improving the performance of Graffiti \u201cclassifier\u201d using the ResNet50 neural network by tuning parameters like Learning Rate, Batch Size and identifying the best freezing layer.</p>"},{"location":"scenarios/vandalism/#model-details","title":"Model Details","text":""},{"location":"scenarios/vandalism/#dataset","title":"Dataset","text":"<p>The dataset consists of images and videos collected from various sources. </p>"},{"location":"scenarios/vandalism/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall mAP 4126 v1 Both(Ceiling and Straight) 95.0%  78.6%  91.0%"},{"location":"scenarios/vandalism/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows:</p> <ul> <li> <p>We use existing camera feeds from the premises to monitor an area or property in real-time, detecting any instances of vandalism or destruction as they occur.</p> </li> <li> <p>VisionAI system is able to run on edge devices. It uses camera feeds for processing.</p> </li> <li> <p>When instances of vandalism or destruction is detected, an alert will be raised.</p> </li> </ul> Test now with online Web-CamWith RTSP Camera - PipelinesWith Azure Setup <p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li> <p>Install the visionai package from PyPI</p> <pre><code>$ pip install visionai\n</code></pre> </li> <li> <p>Test the scenario from your local web-cam</p> <pre><code>$ visionai scenario test vandalism-graffiti-detection\n\nDownloading models for scenario: vandalism-graffiti-detection\nModel: vandalism-graffiti-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-vandalism-graffiti-detection/yolov5s-vandalism-graffiti-detection-0.0.1.zip\n\n\nStarting scenario: vandalism-graffiti-detection..\n</code></pre> </li> <li> <p>You should be able to see the events generated on your console window with graffiti vandalism being detected within the camera field of view.</p> </li> </ul> <p>[TODO]</p> <p>VisionAI app is available at a Azure Market place, one can download and use it by following steps mentioned here</p>"},{"location":"scenarios/vandalism/#features","title":"Features","text":"<p>The VisionAI solution is the most efficient way of implementing this scenario, as evidenced by the following features:</p> <ul> <li> <p>Prediction: Our vandalism graffiti detection model uses data and historical patterns to predict when and where vandalism and destruction might occur. For example, it can analyze patterns of past vandalism incidents to predict where future incidents might occur.</p> </li> <li> <p>Real-time monitoring: Vandalism graffiti detection model can continuously monitor an area or property in real-time, detecting any instances of vandalism or destruction as they occur. This allows for a rapid response and intervention.</p> </li> <li> <p>Automated alerts: Alerts can automatically be generated to authorities or property owners when incidents of vandalism or destruction are detected. This can help to improve response times and prevent further damage.</p> </li> </ul>"},{"location":"scenarios/vandalism/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please contact us.</p>"},{"location":"scenarios/vandalism/#contact-us","title":"Contact Us","text":"<ul> <li> <p>For technical issues, you can open a Github issue here</p> </li> <li> <p>For business inquiries, you can contact us through our website</p> </li> </ul>"},{"location":"scenarios/worker-health-and-safety/","title":"Worker Health and Safety","text":"<p>Workplace injuries are a growing concern for employers and employees. In 2021, the US recorded 5,190 fatal work injuries, with the private sector alone reporting 2.6 million non-fatal injuries. These numbers highlight the need for companies to prioritize workplace safety and implement measures to prevent injuries. Unsafe workplace environments not only result in injuries and fatalities but also account for downtime, reduced productivity, increased healthcare costs, and legal fines.</p> <p>Despite advancements in safety equipment and protective gear, injuries still occur due to non-compliance with protocols and Standard Operating Procedures (SOPs). Therefore, employers are responsible for providing up-to-date safety gear and equipment and ensuring their correct usage and compliance. This should involve live monitoring and enforcing adherence to established protocols and SOPs. By taking these measures, employers can effectively mitigate workplace hazards and prevent injuries from occurring.</p> <p>Establishing protocols and guidelines and providing safety gear might not seem difficult, but ensuring that every individual is consistently complying is challenging. A lapse in compliance and usage of protective equipment (PPE), even for a short duration, can result in a workplace accident. Therefore, you need a mechanism to ensure everyone complies and adheres to guidelines. But the question is, how do you do it?</p>"},{"location":"scenarios/worker-health-and-safety/#eliminate-occupational-hazards-with-visionifys-workplace-health-and-safety-suite","title":"Eliminate Occupational Hazards with Visionify\u2019s Workplace Health and Safety Suite","text":"<p>Leverage Fully Automated, Vision AI-based real-time Detection and Monitoring systems for different workplace scenarios and eliminate occupational hazards and injury risks. Stay on top of the situation with instant alerts and notifications that allow for quick response and resolution of any potential safety concerns, ensuring the well-being of your employees and promoting a culture of safety within your organization. Our next-gen Vision AI models Pre-trained can be deployed instantly to work with any existing camera infrastructure.</p> <p>What\u2019s included in this suite:</p> <ul> <li>PPE Detection </li> <li>Slip and Fall Detection </li> <li>Working at Heights </li> <li>Environment monitoring</li> <li>Slip, trip and fall detection</li> <li>Posture &amp; Ergonomics</li> <li>Empty pallets</li> <li>Spills &amp; Leaks detection</li> <li>Hand-wash</li> <li>Confined spaces monitoring</li> </ul>"},{"location":"scenarios/working-at-heights/","title":"Working at Heights","text":"<p>Ensure the safety of employees at workplace.</p> <p> </p> Events: Working at heights <p>Working at heights is a hazardous activity and has the potential to cause serious injuries or fatalities. It is important for employers to ensure that the work place is set up to prevent employees from falling off of elevated surfaces. Employers must provide proper training and equipment to employees who work at heights and must ensure that safety regulations are followed. Employers should also provide periodic reviews to ensure that employees are following safety protocols and that the work environment is safe and secure.</p> <p>Working at heights, such as on a roof or in a tall building, requires specialized safety equipment and training to ensure the safety of the workers. Depending on the job, you may need to wear a safety harness or other protective gear.</p> <p>Falling from heights is a serious hazard, and can result in serious injury or even death. Timely action in the event of a Fall/Slip accident can minimize damage and save lives.</p>"},{"location":"scenarios/working-at-heights/#vision-ai-based-monitoring","title":"Vision AI based monitoring","text":"<p>Vision AI-based system can be used to detect slip and fall with high accuracy. Additionally, our model trained on real-world images minimizes false-positives or false-negatives.  </p> <p>The cameras scan every frame to ensure there are no accidents related to slip and fall cases. </p> <p>To ensure accuracy and reliability for the model, these camera-based monitoring services should be supplemented by strong compliance processes. Furthermore, workers working in different factory units should always be made aware of these accidents and how to safeguard them. </p>"},{"location":"scenarios/working-at-heights/#model-details","title":"Model Details","text":""},{"location":"scenarios/working-at-heights/#dataset","title":"Dataset","text":"<p>Model training is carried out with Microsoft COCO: Common Objects in Context dataset. Only person class is considered for model building. COCO is a  large-scale dataset that addresses three core research problems in scene understanding: detecting non-iconic views (or non-canonical perspectives of objects), contextual reasoning between objects and the precise 2D localization of objects. </p> <p>COCO dataset has an even distribution of: </p> <ul> <li> <p>Different(indoor/outdoor) environments </p> </li> <li> <p>Male vs Female  </p> </li> <li> <p>Different light settings </p> </li> <li> <p>Variations in camera orientations </p> </li> <li> <p>Using security camera feeds </p> </li> </ul>"},{"location":"scenarios/working-at-heights/#model","title":"Model","text":"<p>The model is built using Yolov5 pre-trained model for detecting a person followed by a media pipe library used to estimate the pose of the person.</p>"},{"location":"scenarios/working-at-heights/#model-card","title":"Model card","text":"Dataset size Version Camera support Precision Recall  mAP   3220 v1 Both(Ceiling and Straight) 65.0%  71.6%  71.0%  <p>The model is adaptable enough to run on any edge computing device.</p>"},{"location":"scenarios/working-at-heights/#scenario-details","title":"Scenario details","text":"<p>The business logic for this scenario is as follows: </p> <ul> <li> <p>We use existing camera feeds from the premises to monitor and detect occurrences of slip and fall incidents. </p> </li> <li> <p>VisionAI s able to run on edge devices. It uses camera feeds for processing. </p> </li> <li> <p>We detect human poses to identify slip and fall accidents in the camera feed. \u00a0</p> </li> <li>If either slip or fall is detected, an alert is raised.</li> </ul>"},{"location":"scenarios/working-at-heights/#try-it-now","title":"Try it now","text":""},{"location":"scenarios/working-at-heights/#quick-method-using-your-local-web-cam","title":"Quick method - using your local web-cam","text":"<p>To test this model &amp; scenario, you can use the following steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Test the scenario from your local web-cam</li> </ul> <pre><code>$ visionai scenario test slip-and-fall-detection\n\nDownloading models for scenario: slip-and-fall-detection\nModel: slip-and-fall-detection: https://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n\nStarting scenario: slip-and-fall-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with slip and fall being detected within the camera field of view.</li> </ul>"},{"location":"scenarios/working-at-heights/#in-an-actual-environment","title":"In an actual environment","text":"<p>To use this scenario in an actual environment, you can follow these steps:</p> <ul> <li>Install the visionai package from PyPI</li> </ul> <pre><code>$ pip install visionai\n---&gt; 100%\n</code></pre> <ul> <li>Download the scenario</li> </ul> <pre><code>$ visionai scenario download slip-and-fall-detection\n\nDownloading models for scenario: slip-and-fall-detection\nModel: slip-and-fall-detection\nhttps://workplaceos.blob.core.windows.net/models/yolov5s-people/yolov5s-people-0.0.4.zip\n---&gt; 100%\n</code></pre> <ul> <li>Add the camera feed to the scenario</li> </ul> <pre><code>$ visionai camera add OFFICE-01 --url rtsp://192.168.0.1/stream1\n$ visionai camera OFFICE-01 add-scenario slip-and-fall-detection\n$ visionai run\n\nStarting scenario: slip-and-fall-detection..\n</code></pre> <ul> <li>You should be able to see the events generated on your console window with slip and fall being detected within the camera field of view.</li> </ul> <p>For more details visit VisionAI web application.</p>"},{"location":"scenarios/working-at-heights/#training-with-custom-data","title":"Training with custom data","text":"<p>The scenario is provided as part of our GPL-v3 package for VisionAI. If you wish to train this with custom datasets, please contact us and we can provide you with the training code. You can do custom training with your own datasets for free, as long as it complies with GPLv3 license (you give back the code to the community). If you are interested in a custom license, please (contact us)[contact.md].</p>"},{"location":"scenarios/working-at-heights/#contact-us","title":"Contact Us","text":"<ul> <li>For technical issues, you can open a Github issue here.</li> <li>For business inquiries, you can contact us through our website.</li> </ul>"},{"location":"tutorials/","title":"Tutorials","text":""},{"location":"tutorials/#set-up-a-pipeline","title":"Set up a Pipeline","text":"<p>You can specify a single scenario or multiple scenarios along with pre-processing steps under pipeline creation.</p> <p>In other words, pipeline is a list of scenarios to be run for specific cameras. The flow is as follows.</p> <p>You can create a pipeline by using the following command:</p> <pre><code>$ visionai pipeline create --name test_pipe\n</code></pre> <p>Scenarios can be added as follows:</p> <p>For example let's add Smoke-and-Fire dection and PPE detection to our pipeline.</p> <pre><code>$ visionai pipeline add-scenario --pipeline test_pipe  --name smoke-and-fire\n</code></pre> <pre><code>$ visionai pipeline add-scenario --pipeline test_pipe  --name ppe-detection\n</code></pre> <p>You can get the details of the pipeline:</p> <pre><code>$ visionai pipeline show --pipeline test_pipe\n</code></pre>"}]}